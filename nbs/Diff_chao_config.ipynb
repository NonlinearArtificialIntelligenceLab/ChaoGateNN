{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: JAX_PLATFORM_NAME=cpu\n",
      "The jaxtyping extension is already loaded. To reload it, use:\n",
      "  %reload_ext jaxtyping\n"
     ]
    }
   ],
   "source": [
    "%env JAX_PLATFORM_NAME=cpu\n",
    "\n",
    "import jaxtyping  # noqa: F401\n",
    "\n",
    "%load_ext jaxtyping\n",
    "# %jaxtyping.typechecker beartype.beartype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import equinox as eqx\n",
    "import optax\n",
    "\n",
    "from tqdm.notebook import trange\n",
    "\n",
    "from jaxtyping import Array, Bool, Float\n",
    "\n",
    "from chaogatenn.chaogate import ChaoGate\n",
    "from chaogatenn.maps import LogisticMap, DuffingMap, LorenzMap, RosslerMap\n",
    "from chaogatenn.utils import grad_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data for the AND gate\n",
    "X = jnp.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=bool)  # Input combinations\n",
    "AND_Y = jnp.array([0, 0, 0, 1], dtype=bool)  # AND gate output\n",
    "OR_Y = jnp.array([0, 1, 1, 1], dtype=bool)  # OR gate output\n",
    "XOR_Y = jnp.array([0, 1, 1, 0], dtype=bool)  # XOR\n",
    "NAND_Y = jnp.array([1, 1, 1, 0], dtype=bool)  # NAND\n",
    "NOR_Y = jnp.array([1, 0, 0, 0], dtype=bool)  # NOR\n",
    "XNOR_Y = jnp.array([1, 0, 0, 1], dtype=bool)  # XNOR\n",
    "Y = XNOR_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map = LogisticMap(a=4.0)\n",
    "# Map = LorenzMap()\n",
    "# Map = DuffingMap(\n",
    "#     alpha=1.0, beta=1.0, delta=0.02, gamma=8.0, omega=0.5, dt=0.01, steps=1000\n",
    "# )\n",
    "Map = RosslerMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(-1.1745434, dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Map(2)  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "DELTA, X0, X_THRESHOLD = jax.random.normal(jax.random.PRNGKey(42), (3,))\n",
    "chao_gate = ChaoGate(DELTA=DELTA, X0=X0, X_THRESHOLD=X_THRESHOLD, Map=Map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Array(0.9000974, dtype=float32),\n",
       " Array(0.89128643, dtype=float32),\n",
       " Array(0.89128643, dtype=float32),\n",
       " Array(0.88171566, dtype=float32)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[chao_gate(x) for x in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "@eqx.filter_value_and_grad()\n",
    "def compute_loss(\n",
    "    chao_gate: ChaoGate, x: Bool[Array, \"batch 2\"], y: Bool[Array, \"batch\"]\n",
    ") -> Float[Array, \"\"]:  # noqa: F821\n",
    "    pred = jax.vmap(chao_gate)(x)\n",
    "    # binary cross entropy\n",
    "    return -jnp.mean(y * jnp.log(pred + 1e-15) + (1 - y) * jnp.log(1 - pred + 1e-15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "@eqx.filter_jit\n",
    "def make_step(\n",
    "    model: ChaoGate,\n",
    "    x: Bool[Array, \"dim 2\"],\n",
    "    y: Bool[Array, \"dim\"],  # noqa: F821\n",
    "    optim: optax.GradientTransformation,\n",
    "    opt_state: optax.OptState,\n",
    ") -> (Float[Array, \"dim\"], ChaoGate, optax.OptState):  # type: ignore  # noqa: F821\n",
    "    loss, grads = compute_loss(model, x, y)\n",
    "    updates, opt_state = optim.update(grads, opt_state)\n",
    "    # jax.debug.print(f\"{grads, updates}\")\n",
    "    model = eqx.apply_updates(model, updates)\n",
    "    return loss, model, opt_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = optax.adabelief(3e-3)\n",
    "opt_state = optim.init(eqx.filter(chao_gate, eqx.is_inexact_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ee50fc6375c4bbe92e46018212df3ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 1.1673038005828857, Grad Norm: 0.4794231057167053\n",
      "Epoch 10, Loss: 1.1385128498077393, Grad Norm: 0.4699413776397705\n",
      "Epoch 20, Loss: 1.0989536046981812, Grad Norm: 0.4556359052658081\n",
      "Epoch 30, Loss: 1.0516053438186646, Grad Norm: 0.4361053705215454\n",
      "Epoch 40, Loss: 1.000169038772583, Grad Norm: 0.41114988923072815\n",
      "Epoch 50, Loss: 0.948222279548645, Grad Norm: 0.3808229863643646\n",
      "Epoch 60, Loss: 0.8989492654800415, Grad Norm: 0.34564855694770813\n",
      "Epoch 70, Loss: 0.8549132347106934, Grad Norm: 0.30678901076316833\n",
      "Epoch 80, Loss: 0.8178040981292725, Grad Norm: 0.26603856682777405\n",
      "Epoch 90, Loss: 0.7882799506187439, Grad Norm: 0.22556018829345703\n",
      "Epoch 100, Loss: 0.7660256624221802, Grad Norm: 0.18747439980506897\n",
      "Epoch 110, Loss: 0.7500277757644653, Grad Norm: 0.15346908569335938\n",
      "Epoch 120, Loss: 0.7389442920684814, Grad Norm: 0.12458822876214981\n",
      "Epoch 130, Loss: 0.7314277291297913, Grad Norm: 0.10121803730726242\n",
      "Epoch 140, Loss: 0.7263311147689819, Grad Norm: 0.0831783339381218\n",
      "Epoch 150, Loss: 0.7227809429168701, Grad Norm: 0.06986827403306961\n",
      "Epoch 160, Loss: 0.7201708555221558, Grad Norm: 0.06041260436177254\n",
      "Epoch 170, Loss: 0.7181121110916138, Grad Norm: 0.05383764207363129\n",
      "Epoch 180, Loss: 0.7163714170455933, Grad Norm: 0.04924257472157478\n",
      "Epoch 190, Loss: 0.7148181200027466, Grad Norm: 0.04591558128595352\n",
      "Epoch 200, Loss: 0.7133839130401611, Grad Norm: 0.04336203634738922\n",
      "Epoch 210, Loss: 0.7120344042778015, Grad Norm: 0.0412694551050663\n",
      "Epoch 220, Loss: 0.7107552289962769, Grad Norm: 0.03945436328649521\n",
      "Epoch 230, Loss: 0.7095390558242798, Grad Norm: 0.0378091037273407\n",
      "Epoch 240, Loss: 0.7083836197853088, Grad Norm: 0.03627420589327812\n",
      "Epoch 250, Loss: 0.7072874903678894, Grad Norm: 0.03481494262814522\n",
      "Epoch 260, Loss: 0.7062500715255737, Grad Norm: 0.033412519842386246\n",
      "Epoch 270, Loss: 0.7052702307701111, Grad Norm: 0.03205570578575134\n",
      "Epoch 280, Loss: 0.7043465375900269, Grad Norm: 0.030737441033124924\n",
      "Epoch 290, Loss: 0.703478217124939, Grad Norm: 0.029455609619617462\n",
      "Epoch 300, Loss: 0.7026630640029907, Grad Norm: 0.02820907160639763\n",
      "Epoch 310, Loss: 0.7018998861312866, Grad Norm: 0.0269966758787632\n",
      "Epoch 320, Loss: 0.7011860609054565, Grad Norm: 0.025819556787610054\n",
      "Epoch 330, Loss: 0.7005201578140259, Grad Norm: 0.0246763713657856\n",
      "Epoch 340, Loss: 0.6999001502990723, Grad Norm: 0.02356870099902153\n",
      "Epoch 350, Loss: 0.6993240714073181, Grad Norm: 0.02249598130583763\n",
      "Epoch 360, Loss: 0.6987890601158142, Grad Norm: 0.021458128467202187\n",
      "Epoch 370, Loss: 0.6982935667037964, Grad Norm: 0.020455772057175636\n",
      "Epoch 380, Loss: 0.6978351473808289, Grad Norm: 0.019487790763378143\n",
      "Epoch 390, Loss: 0.6974121928215027, Grad Norm: 0.01855401135981083\n",
      "Epoch 400, Loss: 0.6970219612121582, Grad Norm: 0.017654355615377426\n",
      "Epoch 410, Loss: 0.6966630220413208, Grad Norm: 0.01678830198943615\n",
      "Epoch 420, Loss: 0.6963331699371338, Grad Norm: 0.015954893082380295\n",
      "Epoch 430, Loss: 0.6960304975509644, Grad Norm: 0.015153838321566582\n",
      "Epoch 440, Loss: 0.6957530379295349, Grad Norm: 0.0143844373524189\n",
      "Epoch 450, Loss: 0.6954993009567261, Grad Norm: 0.013645990751683712\n",
      "Epoch 460, Loss: 0.6952676177024841, Grad Norm: 0.012937095016241074\n",
      "Epoch 470, Loss: 0.6950562596321106, Grad Norm: 0.012258623726665974\n",
      "Epoch 480, Loss: 0.6948636770248413, Grad Norm: 0.011608273722231388\n",
      "Epoch 490, Loss: 0.6946887373924255, Grad Norm: 0.010986069217324257\n",
      "Epoch 500, Loss: 0.69452965259552, Grad Norm: 0.01039145141839981\n",
      "Epoch 510, Loss: 0.6943854689598083, Grad Norm: 0.009822322987020016\n",
      "Epoch 520, Loss: 0.694254994392395, Grad Norm: 0.009279673919081688\n",
      "Epoch 530, Loss: 0.6941368579864502, Grad Norm: 0.008761422708630562\n",
      "Epoch 540, Loss: 0.6940301656723022, Grad Norm: 0.008266893215477467\n",
      "Epoch 550, Loss: 0.6939340233802795, Grad Norm: 0.007796093821525574\n",
      "Epoch 560, Loss: 0.6938473582267761, Grad Norm: 0.007347382605075836\n",
      "Epoch 570, Loss: 0.6937695741653442, Grad Norm: 0.006920237559825182\n",
      "Epoch 580, Loss: 0.6936997175216675, Grad Norm: 0.00651396531611681\n",
      "Epoch 590, Loss: 0.6936368942260742, Grad Norm: 0.006128019653260708\n",
      "Epoch 600, Loss: 0.6935807466506958, Grad Norm: 0.00576105248183012\n",
      "Epoch 610, Loss: 0.6935306191444397, Grad Norm: 0.005412951111793518\n",
      "Epoch 620, Loss: 0.6934855580329895, Grad Norm: 0.005082792602479458\n",
      "Epoch 630, Loss: 0.6934458017349243, Grad Norm: 0.004769717343151569\n",
      "Epoch 640, Loss: 0.6934100985527039, Grad Norm: 0.004473167471587658\n",
      "Epoch 650, Loss: 0.6933785080909729, Grad Norm: 0.00419225450605154\n",
      "Epoch 660, Loss: 0.6933503150939941, Grad Norm: 0.003926530480384827\n",
      "Epoch 670, Loss: 0.693325400352478, Grad Norm: 0.0036754037719219923\n",
      "Epoch 680, Loss: 0.6933032870292664, Grad Norm: 0.003438252490013838\n",
      "Epoch 690, Loss: 0.6932836771011353, Grad Norm: 0.003214118303731084\n",
      "Epoch 700, Loss: 0.6932665109634399, Grad Norm: 0.003002664539963007\n",
      "Epoch 710, Loss: 0.6932512521743774, Grad Norm: 0.0028033542912453413\n",
      "Epoch 720, Loss: 0.693237841129303, Grad Norm: 0.002615593373775482\n",
      "Epoch 730, Loss: 0.6932260990142822, Grad Norm: 0.002438719617202878\n",
      "Epoch 740, Loss: 0.6932157278060913, Grad Norm: 0.002272123470902443\n",
      "Epoch 750, Loss: 0.6932066679000854, Grad Norm: 0.002115931361913681\n",
      "Epoch 760, Loss: 0.6931986808776855, Grad Norm: 0.001968780066817999\n",
      "Epoch 770, Loss: 0.693191647529602, Grad Norm: 0.001830770750530064\n",
      "Epoch 780, Loss: 0.6931856870651245, Grad Norm: 0.0017013262258842587\n",
      "Epoch 790, Loss: 0.6931804418563843, Grad Norm: 0.0015797968953847885\n",
      "Epoch 800, Loss: 0.6931758522987366, Grad Norm: 0.00146606529597193\n",
      "Epoch 810, Loss: 0.6931718587875366, Grad Norm: 0.0013598081422969699\n",
      "Epoch 820, Loss: 0.6931684017181396, Grad Norm: 0.0012599463807418942\n",
      "Epoch 830, Loss: 0.6931653022766113, Grad Norm: 0.0011665148194879293\n",
      "Epoch 840, Loss: 0.6931627988815308, Grad Norm: 0.00107953988481313\n",
      "Epoch 850, Loss: 0.6931605339050293, Grad Norm: 0.0009982339106500149\n",
      "Epoch 860, Loss: 0.6931586265563965, Grad Norm: 0.0009226339752785861\n",
      "Epoch 870, Loss: 0.6931569576263428, Grad Norm: 0.000852061144541949\n",
      "Epoch 880, Loss: 0.6931554675102234, Grad Norm: 0.0007859984179958701\n",
      "Epoch 890, Loss: 0.6931542158126831, Grad Norm: 0.000724869198165834\n",
      "Epoch 900, Loss: 0.6931531429290771, Grad Norm: 0.0006680534570477903\n",
      "Epoch 910, Loss: 0.6931521892547607, Grad Norm: 0.0006151801208034158\n",
      "Epoch 920, Loss: 0.6931514739990234, Grad Norm: 0.0005659058806486428\n",
      "Epoch 930, Loss: 0.6931507587432861, Grad Norm: 0.0005202997708693147\n",
      "Epoch 940, Loss: 0.6931502223014832, Grad Norm: 0.00047800576430745423\n",
      "Epoch 950, Loss: 0.693149745464325, Grad Norm: 0.0004386665241327137\n",
      "Epoch 960, Loss: 0.6931495070457458, Grad Norm: 0.00040242879185825586\n",
      "Epoch 970, Loss: 0.6931489706039429, Grad Norm: 0.00036898834514431655\n",
      "Epoch 980, Loss: 0.6931487917900085, Grad Norm: 0.000338110257871449\n",
      "Epoch 990, Loss: 0.6931484341621399, Grad Norm: 0.00030951944063417614\n",
      "Epoch 1000, Loss: 0.6931482553482056, Grad Norm: 0.000283141213003546\n",
      "Epoch 1010, Loss: 0.693148136138916, Grad Norm: 0.0002587483322713524\n",
      "Epoch 1020, Loss: 0.6931479573249817, Grad Norm: 0.00023611493816133589\n",
      "Epoch 1030, Loss: 0.6931478381156921, Grad Norm: 0.0002154192770831287\n",
      "Epoch 1040, Loss: 0.693147599697113, Grad Norm: 0.00019636994693428278\n",
      "Epoch 1050, Loss: 0.6931476593017578, Grad Norm: 0.00017903605476021767\n",
      "Epoch 1060, Loss: 0.6931475400924683, Grad Norm: 0.00016293163935188204\n",
      "Epoch 1070, Loss: 0.6931474208831787, Grad Norm: 0.00014819251373410225\n",
      "Epoch 1080, Loss: 0.6931475400924683, Grad Norm: 0.000134600093588233\n",
      "Epoch 1090, Loss: 0.6931474208831787, Grad Norm: 0.00012226162652950734\n",
      "Epoch 1100, Loss: 0.6931473612785339, Grad Norm: 0.00011095609079347923\n",
      "Epoch 1110, Loss: 0.6931473612785339, Grad Norm: 0.00010052740253740922\n",
      "Epoch 1120, Loss: 0.6931473016738892, Grad Norm: 9.1158552095294e-05\n",
      "Epoch 1130, Loss: 0.6931473016738892, Grad Norm: 8.257276931544766e-05\n",
      "Epoch 1140, Loss: 0.6931471824645996, Grad Norm: 7.455197192030028e-05\n",
      "Epoch 1150, Loss: 0.6931473016738892, Grad Norm: 6.742969708284363e-05\n",
      "Epoch 1160, Loss: 0.6931472420692444, Grad Norm: 6.071297684684396e-05\n",
      "Epoch 1170, Loss: 0.6931471824645996, Grad Norm: 5.495057121152058e-05\n",
      "Epoch 1180, Loss: 0.6931473016738892, Grad Norm: 4.935249671689235e-05\n",
      "Epoch 1190, Loss: 0.6931471824645996, Grad Norm: 4.461669232114218e-05\n",
      "Epoch 1200, Loss: 0.6931473016738892, Grad Norm: 4.030462878290564e-05\n",
      "Epoch 1210, Loss: 0.6931472420692444, Grad Norm: 3.613497756305151e-05\n",
      "Epoch 1220, Loss: 0.6931471824645996, Grad Norm: 3.2226529583567753e-05\n",
      "Epoch 1230, Loss: 0.6931471824645996, Grad Norm: 2.9168959372327663e-05\n",
      "Epoch 1240, Loss: 0.6931471824645996, Grad Norm: 2.6319583412259817e-05\n",
      "Epoch 1250, Loss: 0.6931471228599548, Grad Norm: 2.358662459300831e-05\n",
      "Epoch 1260, Loss: 0.6931471824645996, Grad Norm: 2.0950146790710278e-05\n",
      "Epoch 1270, Loss: 0.6931471824645996, Grad Norm: 1.876136775535997e-05\n",
      "Epoch 1280, Loss: 0.6931472420692444, Grad Norm: 1.672687903919723e-05\n",
      "Epoch 1290, Loss: 0.6931471824645996, Grad Norm: 1.4996175195847172e-05\n",
      "Epoch 1300, Loss: 0.6931471824645996, Grad Norm: 1.3256892088975292e-05\n",
      "Epoch 1310, Loss: 0.6931471824645996, Grad Norm: 1.1811097465397324e-05\n",
      "Epoch 1320, Loss: 0.6931471824645996, Grad Norm: 1.0581506103335414e-05\n",
      "Epoch 1330, Loss: 0.6931471824645996, Grad Norm: 9.578294339007698e-06\n",
      "Epoch 1340, Loss: 0.6931471824645996, Grad Norm: 8.402061212109402e-06\n",
      "Epoch 1350, Loss: 0.6931471824645996, Grad Norm: 7.4571639743226115e-06\n",
      "Epoch 1360, Loss: 0.6931472420692444, Grad Norm: 6.571158792212373e-06\n",
      "Epoch 1370, Loss: 0.6931471824645996, Grad Norm: 5.727320967707783e-06\n",
      "Epoch 1380, Loss: 0.6931471824645996, Grad Norm: 5.092497758596437e-06\n",
      "Epoch 1390, Loss: 0.6931471824645996, Grad Norm: 4.565066319628386e-06\n",
      "Epoch 1400, Loss: 0.6931471824645996, Grad Norm: 4.07510651712073e-06\n",
      "Epoch 1410, Loss: 0.6931471824645996, Grad Norm: 3.69804070032842e-06\n",
      "Epoch 1420, Loss: 0.6931471824645996, Grad Norm: 3.2485984320373973e-06\n",
      "Epoch 1430, Loss: 0.6931471824645996, Grad Norm: 2.894471435865853e-06\n",
      "Epoch 1440, Loss: 0.6931471824645996, Grad Norm: 2.5572428512532497e-06\n",
      "Epoch 1450, Loss: 0.6931471824645996, Grad Norm: 2.2914016426511807e-06\n",
      "Epoch 1460, Loss: 0.6931471824645996, Grad Norm: 1.937208253366407e-06\n",
      "Epoch 1470, Loss: 0.6931471824645996, Grad Norm: 1.679936531218118e-06\n",
      "Epoch 1480, Loss: 0.6931472420692444, Grad Norm: 1.5272054270099034e-06\n",
      "Epoch 1490, Loss: 0.6931471824645996, Grad Norm: 1.3384527619564324e-06\n",
      "Epoch 1500, Loss: 0.6931471824645996, Grad Norm: 1.1558008736756165e-06\n",
      "Epoch 1510, Loss: 0.6931471824645996, Grad Norm: 9.867449080047663e-07\n",
      "Epoch 1520, Loss: 0.6931471824645996, Grad Norm: 8.239210274041397e-07\n",
      "Epoch 1530, Loss: 0.6931472420692444, Grad Norm: 7.362515930253721e-07\n",
      "Epoch 1540, Loss: 0.6931471824645996, Grad Norm: 6.154738798613835e-07\n",
      "Epoch 1550, Loss: 0.6931471824645996, Grad Norm: 5.327034955371346e-07\n",
      "Epoch 1560, Loss: 0.6931471824645996, Grad Norm: 4.651096219276951e-07\n",
      "Epoch 1570, Loss: 0.6931471824645996, Grad Norm: 4.124150336792809e-07\n",
      "Epoch 1580, Loss: 0.6931472420692444, Grad Norm: 3.514431909934501e-07\n",
      "Epoch 1590, Loss: 0.6931471824645996, Grad Norm: 3.199414777554921e-07\n",
      "Epoch 1600, Loss: 0.6931471824645996, Grad Norm: 2.4978896817628993e-07\n",
      "Epoch 1610, Loss: 0.6931471824645996, Grad Norm: 2.337155819986947e-07\n",
      "Epoch 1620, Loss: 0.6931471824645996, Grad Norm: 2.240135614783867e-07\n",
      "Epoch 1630, Loss: 0.6931471228599548, Grad Norm: 1.752074894056932e-07\n",
      "Epoch 1640, Loss: 0.6931471228599548, Grad Norm: 1.284009698565569e-07\n",
      "Epoch 1650, Loss: 0.6931471824645996, Grad Norm: 1.095007249318769e-07\n",
      "Epoch 1660, Loss: 0.6931471824645996, Grad Norm: 9.570443637585413e-08\n",
      "Epoch 1670, Loss: 0.6931471824645996, Grad Norm: 6.828569354411229e-08\n",
      "Epoch 1680, Loss: 0.6931471824645996, Grad Norm: 6.828569354411229e-08\n",
      "Epoch 1690, Loss: 0.6931471824645996, Grad Norm: 5.424104543294561e-08\n",
      "Epoch 1700, Loss: 0.6931471824645996, Grad Norm: 4.280032683823265e-08\n",
      "Epoch 1710, Loss: 0.6931471824645996, Grad Norm: 4.998001301714794e-08\n",
      "Epoch 1720, Loss: 0.6931471824645996, Grad Norm: 3.4142846772056146e-08\n",
      "Epoch 1730, Loss: 0.6931471824645996, Grad Norm: 3.0719530030864917e-08\n",
      "Epoch 1740, Loss: 0.6931471824645996, Grad Norm: 3.0719530030864917e-08\n",
      "Epoch 1750, Loss: 0.6931472420692444, Grad Norm: 3.725290298461914e-08\n",
      "Epoch 1760, Loss: 0.6931471824645996, Grad Norm: 3.0719530030864917e-08\n",
      "Epoch 1770, Loss: 0.6931471824645996, Grad Norm: 3.725290298461914e-08\n",
      "Epoch 1780, Loss: 0.6931472420692444, Grad Norm: 3.725290298461914e-08\n",
      "Epoch 1790, Loss: 0.6931471824645996, Grad Norm: 4.998001301714794e-08\n",
      "Epoch 1800, Loss: 0.6931471824645996, Grad Norm: 3.332000630962284e-08\n",
      "Epoch 1810, Loss: 0.6931472420692444, Grad Norm: 2.107342389479072e-08\n",
      "Epoch 1820, Loss: 0.6931471824645996, Grad Norm: 5.5755037919880124e-08\n",
      "Epoch 1830, Loss: 0.6931471824645996, Grad Norm: 7.337970941989624e-08\n",
      "Epoch 1840, Loss: 0.6931471824645996, Grad Norm: 5.21540641784668e-08\n",
      "Epoch 1850, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 1860, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 1870, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 1880, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 1890, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 1900, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 1910, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 1920, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 1930, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 1940, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 1950, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 1960, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 1970, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 1980, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 1990, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 2000, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 2010, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 2020, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 2030, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 2040, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 2050, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 2060, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 2070, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 2080, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 2090, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 2100, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 2110, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 2120, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 2130, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 2140, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 2150, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 2160, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 2170, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 2180, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 2190, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 2200, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 2210, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 2220, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 2230, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 2240, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 2250, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 2260, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 2270, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 2280, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 2290, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 2300, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 2310, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 2320, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 2330, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 2340, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 2350, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 2360, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 2370, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 2380, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 2390, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 2400, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 2410, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 2420, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 2430, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 2440, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 2450, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 2460, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 2470, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 2480, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 2490, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 2500, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 2510, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 2520, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 2530, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 2540, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 2550, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 2560, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 2570, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 2580, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 2590, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 2600, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 2610, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 2620, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 2630, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 2640, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 2650, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 2660, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 2670, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 2680, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 2690, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 2700, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 2710, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 2720, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 2730, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 2740, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 2750, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 2760, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 2770, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 2780, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 2790, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 2800, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 2810, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 2820, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 2830, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 2840, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 2850, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 2860, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 2870, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 2880, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 2890, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 2900, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 2910, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 2920, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 2930, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 2940, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 2950, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 2960, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 2970, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 2980, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 2990, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 3000, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 3010, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 3020, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 3030, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 3040, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 3050, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 3060, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 3070, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 3080, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 3090, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 3100, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 3110, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 3120, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 3130, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 3140, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 3150, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 3160, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 3170, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 3180, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 3190, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 3200, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 3210, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 3220, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 3230, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 3240, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 3250, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 3260, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 3270, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 3280, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 3290, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 3300, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 3310, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 3320, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 3330, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 3340, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 3350, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 3360, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 3370, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 3380, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 3390, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 3400, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 3410, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 3420, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 3430, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 3440, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 3450, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 3460, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 3470, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 3480, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 3490, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 3500, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 3510, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 3520, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 3530, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 3540, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 3550, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 3560, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 3570, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 3580, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 3590, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 3600, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 3610, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 3620, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 3630, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 3640, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 3650, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 3660, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 3670, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 3680, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 3690, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 3700, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 3710, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 3720, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 3730, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 3740, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 3750, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 3760, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 3770, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 3780, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 3790, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 3800, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 3810, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 3820, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 3830, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 3840, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 3850, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 3860, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 3870, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 3880, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 3890, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 3900, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 3910, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 3920, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 3930, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 3940, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 3950, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 3960, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 3970, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 3980, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 3990, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 4000, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 4010, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 4020, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 4030, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 4040, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 4050, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 4060, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 4070, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 4080, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 4090, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 4100, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 4110, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 4120, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 4130, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 4140, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 4150, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 4160, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 4170, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 4180, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 4190, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 4200, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 4210, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 4220, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 4230, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 4240, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 4250, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 4260, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 4270, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 4280, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 4290, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 4300, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 4310, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 4320, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 4330, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 4340, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 4350, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 4360, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 4370, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 4380, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 4390, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 4400, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 4410, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 4420, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 4430, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 4440, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 4450, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 4460, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 4470, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 4480, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 4490, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 4500, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 4510, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 4520, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 4530, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 4540, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 4550, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 4560, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 4570, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 4580, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 4590, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 4600, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 4610, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 4620, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 4630, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 4640, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 4650, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 4660, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 4670, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 4680, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 4690, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 4700, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 4710, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 4720, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 4730, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 4740, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 4750, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 4760, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 4770, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 4780, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 4790, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 4800, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 4810, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 4820, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 4830, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 4840, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 4850, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 4860, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 4870, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 4880, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 4890, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 4900, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 4910, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 4920, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 4930, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 4940, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 4950, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 4960, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 4970, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 4980, Loss: 0.6931471824645996, Grad Norm: 0.0\n",
      "Epoch 4990, Loss: 0.6931471824645996, Grad Norm: 0.0\n"
     ]
    }
   ],
   "source": [
    "epochs = 5_000\n",
    "\n",
    "for epoch in trange(epochs):\n",
    "    loss, chao_gate, opt_state = make_step(chao_gate, X, Y, optim, opt_state)  # type: ignore\n",
    "    _, grads = compute_loss(chao_gate, X, Y)\n",
    "    grad_norm_value = grad_norm(grads)\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss}, Grad Norm: {grad_norm_value}\")\n",
    "\n",
    "    if loss < 1e-3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Trained ChaoGate Parameters:\n",
      "DELTA: 1.6840472483181657e-07, X0: 0.17210151255130768, X_THRESHOLD: -0.11809306591749191\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTrained ChaoGate Parameters:\")\n",
    "print(\n",
    "    f\"DELTA: {chao_gate.DELTA}, X0: {chao_gate.X0}, X_THRESHOLD: {chao_gate.X_THRESHOLD}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(False, False, True),\n",
       " (False, True, True),\n",
       " (True, False, True),\n",
       " (True, True, True)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[\n",
    "    (\n",
    "        bool(x1.item()),\n",
    "        bool(x2.item()),\n",
    "        (\n",
    "            chao_gate.Map(chao_gate.X0 + x1 * chao_gate.DELTA + x2 * chao_gate.DELTA)\n",
    "            > chao_gate.X_THRESHOLD\n",
    "        ).item(),\n",
    "    )\n",
    "    for x1, x2 in X\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final_accuracy=0.5\n"
     ]
    }
   ],
   "source": [
    "pred_ys = jax.vmap(chao_gate)(X)\n",
    "num_correct = jnp.sum((pred_ys > 0.5) == Y)\n",
    "final_accuracy = (num_correct / len(X)).item()\n",
    "print(f\"final_accuracy={final_accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
