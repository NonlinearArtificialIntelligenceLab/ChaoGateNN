{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: JAX_PLATFORM_NAME=cpu\n",
      "The jaxtyping extension is already loaded. To reload it, use:\n",
      "  %reload_ext jaxtyping\n"
     ]
    }
   ],
   "source": [
    "%env JAX_PLATFORM_NAME=cpu\n",
    "\n",
    "import jaxtyping  # noqa: F401\n",
    "\n",
    "%load_ext jaxtyping\n",
    "# %jaxtyping.typechecker beartype.beartype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import equinox as eqx\n",
    "import optax\n",
    "\n",
    "from tqdm.notebook import trange\n",
    "\n",
    "from jaxtyping import Array, Bool, Float\n",
    "\n",
    "from chaogatenn.chaogate import NewChaoGate\n",
    "from chaogatenn.maps import (\n",
    "    LogisticMap,\n",
    "    DuffingMap,\n",
    "    LorenzMap,\n",
    "    RosslerMap,\n",
    "    ChenMap,\n",
    "    RosslerHyperchaosMap,\n",
    ")\n",
    "from chaogatenn.utils import grad_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data for the AND gate\n",
    "X = jnp.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=bool)  # Input combinations\n",
    "AND_Y = jnp.array([0, 0, 0, 1], dtype=bool)  # AND gate output\n",
    "OR_Y = jnp.array([0, 1, 1, 1], dtype=bool)  # OR gate output\n",
    "XOR_Y = jnp.array([0, 1, 1, 0], dtype=bool)  # XOR\n",
    "NAND_Y = jnp.array([1, 1, 1, 0], dtype=bool)  # NAND\n",
    "NOR_Y = jnp.array([1, 0, 0, 0], dtype=bool)  # NOR\n",
    "XNOR_Y = jnp.array([1, 0, 0, 1], dtype=bool)  # XNOR\n",
    "Y = XNOR_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "Map = LogisticMap(a=4.0)\n",
    "# Map = LorenzMap()\n",
    "# Map = DuffingMap(\n",
    "#     alpha=1.0, beta=1.0, delta=0.02, gamma=8.0, omega=0.5, dt=0.01, steps=1000\n",
    "# )\n",
    "# Map = ChenMap(dt=1e-5, steps=100)\n",
    "# Map = RosslerHyperchaosMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-8.0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Map(2)  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "DELTA_X, DELTA_Y, X0, X_THRESHOLD = jax.random.normal(jax.random.PRNGKey(0), (4,))\n",
    "chao_gate = NewChaoGate(\n",
    "    DELTA_X=DELTA_X, DELTA_Y=DELTA_Y, X0=X0, X_THRESHOLD=X_THRESHOLD, Map=Map\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Array(0.8072584, dtype=float32),\n",
       " Array(0.1401511, dtype=float32),\n",
       " Array(7.993785e-05, dtype=float32),\n",
       " Array(0.15277262, dtype=float32)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[chao_gate(x) for x in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "@eqx.filter_value_and_grad()\n",
    "def compute_loss(\n",
    "    chao_gate: NewChaoGate, x: Bool[Array, \"batch 2\"], y: Bool[Array, \"batch\"]\n",
    ") -> Float[Array, \"\"]:  # noqa: F821\n",
    "    pred = jax.vmap(chao_gate)(x)\n",
    "    # binary cross entropy\n",
    "    return -jnp.mean(y * jnp.log(pred + 1e-15) + (1 - y) * jnp.log(1 - pred + 1e-15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "@eqx.filter_jit\n",
    "def make_step(\n",
    "    model: NewChaoGate,\n",
    "    x: Bool[Array, \"dim 2\"],\n",
    "    y: Bool[Array, \"dim\"],  # noqa: F821\n",
    "    optim: optax.GradientTransformation,\n",
    "    opt_state: optax.OptState,\n",
    ") -> (Float[Array, \"dim\"], NewChaoGate, optax.OptState):  # type: ignore  # noqa: F821\n",
    "    loss, grads = compute_loss(model, x, y)\n",
    "    updates, opt_state = optim.update(grads, opt_state)\n",
    "    # jax.debug.print(f\"{grads, updates}\")\n",
    "    model = eqx.apply_updates(model, updates)\n",
    "    return loss, model, opt_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = optax.adabelief(3e-4)\n",
    "opt_state = optim.init(eqx.filter(chao_gate, eqx.is_inexact_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bc210c7794f4c5b81ad673c58c5839e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.5609986186027527, Grad Norm: 2.913334846496582\n",
      "Epoch 10, Loss: 0.5418010950088501, Grad Norm: 2.8317952156066895\n",
      "Epoch 20, Loss: 0.5157095193862915, Grad Norm: 2.7182648181915283\n",
      "Epoch 30, Loss: 0.4849158227443695, Grad Norm: 2.5788655281066895\n",
      "Epoch 40, Loss: 0.4518531858921051, Grad Norm: 2.4215004444122314\n",
      "Epoch 50, Loss: 0.41851258277893066, Grad Norm: 2.2537543773651123\n",
      "Epoch 60, Loss: 0.3863244354724884, Grad Norm: 2.082369327545166\n",
      "Epoch 70, Loss: 0.3562135696411133, Grad Norm: 1.9130005836486816\n",
      "Epoch 80, Loss: 0.32868966460227966, Grad Norm: 1.7500680685043335\n",
      "Epoch 90, Loss: 0.3039478659629822, Grad Norm: 1.5966875553131104\n",
      "Epoch 100, Loss: 0.281963586807251, Grad Norm: 1.4547477960586548\n",
      "Epoch 110, Loss: 0.26257404685020447, Grad Norm: 1.3250977993011475\n",
      "Epoch 120, Loss: 0.24554315209388733, Grad Norm: 1.2078001499176025\n",
      "Epoch 130, Loss: 0.23060619831085205, Grad Norm: 1.1023706197738647\n",
      "Epoch 140, Loss: 0.2174987494945526, Grad Norm: 1.0080028772354126\n",
      "Epoch 150, Loss: 0.2059737890958786, Grad Norm: 0.9237261414527893\n",
      "Epoch 160, Loss: 0.19580897688865662, Grad Norm: 0.8485197424888611\n",
      "Epoch 170, Loss: 0.18680982291698456, Grad Norm: 0.7813901901245117\n",
      "Epoch 180, Loss: 0.17880867421627045, Grad Norm: 0.7214049100875854\n",
      "Epoch 190, Loss: 0.17166230082511902, Grad Norm: 0.6677179932594299\n",
      "Epoch 200, Loss: 0.16524936258792877, Grad Norm: 0.6195716857910156\n",
      "Epoch 210, Loss: 0.15946757793426514, Grad Norm: 0.5763020515441895\n",
      "Epoch 220, Loss: 0.1542305052280426, Grad Norm: 0.5373263955116272\n",
      "Epoch 230, Loss: 0.1494649052619934, Grad Norm: 0.5021352767944336\n",
      "Epoch 240, Loss: 0.14510901272296906, Grad Norm: 0.4702889621257782\n",
      "Epoch 250, Loss: 0.14111049473285675, Grad Norm: 0.44140341877937317\n",
      "Epoch 260, Loss: 0.13742461800575256, Grad Norm: 0.4151461124420166\n",
      "Epoch 270, Loss: 0.13401354849338531, Grad Norm: 0.39122840762138367\n",
      "Epoch 280, Loss: 0.1308446228504181, Grad Norm: 0.36939796805381775\n",
      "Epoch 290, Loss: 0.12789015471935272, Grad Norm: 0.3494362533092499\n",
      "Epoch 300, Loss: 0.12512601912021637, Grad Norm: 0.33114922046661377\n",
      "Epoch 310, Loss: 0.12253149598836899, Grad Norm: 0.31436973810195923\n",
      "Epoch 320, Loss: 0.12008863687515259, Grad Norm: 0.29894939064979553\n",
      "Epoch 330, Loss: 0.11778172850608826, Grad Norm: 0.2847566604614258\n",
      "Epoch 340, Loss: 0.11559724807739258, Grad Norm: 0.2716761827468872\n",
      "Epoch 350, Loss: 0.11352317780256271, Grad Norm: 0.25960490107536316\n",
      "Epoch 360, Loss: 0.11154911667108536, Grad Norm: 0.24845145642757416\n",
      "Epoch 370, Loss: 0.10966580361127853, Grad Norm: 0.2381342351436615\n",
      "Epoch 380, Loss: 0.10786527395248413, Grad Norm: 0.22858066856861115\n",
      "Epoch 390, Loss: 0.10614021122455597, Grad Norm: 0.21972380578517914\n",
      "Epoch 400, Loss: 0.10448427498340607, Grad Norm: 0.21150533854961395\n",
      "Epoch 410, Loss: 0.10289184749126434, Grad Norm: 0.20387142896652222\n",
      "Epoch 420, Loss: 0.10135789960622787, Grad Norm: 0.19677428901195526\n",
      "Epoch 430, Loss: 0.09987790882587433, Grad Norm: 0.19016967713832855\n",
      "Epoch 440, Loss: 0.09844784438610077, Grad Norm: 0.18401767313480377\n",
      "Epoch 450, Loss: 0.09706415235996246, Grad Norm: 0.17828208208084106\n",
      "Epoch 460, Loss: 0.09572356194257736, Grad Norm: 0.17292971909046173\n",
      "Epoch 470, Loss: 0.09442313015460968, Grad Norm: 0.16793036460876465\n",
      "Epoch 480, Loss: 0.09316032379865646, Grad Norm: 0.16325615346431732\n",
      "Epoch 490, Loss: 0.09193271398544312, Grad Norm: 0.1588817834854126\n",
      "Epoch 500, Loss: 0.09073812514543533, Grad Norm: 0.1547837108373642\n",
      "Epoch 510, Loss: 0.08957454562187195, Grad Norm: 0.15094022452831268\n",
      "Epoch 520, Loss: 0.08844032883644104, Grad Norm: 0.1473323106765747\n",
      "Epoch 530, Loss: 0.08733372390270233, Grad Norm: 0.14394116401672363\n",
      "Epoch 540, Loss: 0.08625340461730957, Grad Norm: 0.14075034856796265\n",
      "Epoch 550, Loss: 0.08519791811704636, Grad Norm: 0.13774444162845612\n",
      "Epoch 560, Loss: 0.08416608721017838, Grad Norm: 0.13490916788578033\n",
      "Epoch 570, Loss: 0.08315671980381012, Grad Norm: 0.13223136961460114\n",
      "Epoch 580, Loss: 0.08216880261898041, Grad Norm: 0.12969936430454254\n",
      "Epoch 590, Loss: 0.08120141178369522, Grad Norm: 0.127301886677742\n",
      "Epoch 600, Loss: 0.08025354892015457, Grad Norm: 0.1250285655260086\n",
      "Epoch 610, Loss: 0.07932455092668533, Grad Norm: 0.12287016212940216\n",
      "Epoch 620, Loss: 0.07841350138187408, Grad Norm: 0.12081783264875412\n",
      "Epoch 630, Loss: 0.07751983404159546, Grad Norm: 0.11886380612850189\n",
      "Epoch 640, Loss: 0.07664281129837036, Grad Norm: 0.11700067669153214\n",
      "Epoch 650, Loss: 0.075781911611557, Grad Norm: 0.11522169411182404\n",
      "Epoch 660, Loss: 0.07493647187948227, Grad Norm: 0.11352035403251648\n",
      "Epoch 670, Loss: 0.07410598546266556, Grad Norm: 0.11189129203557968\n",
      "Epoch 680, Loss: 0.07328996062278748, Grad Norm: 0.1103290393948555\n",
      "Epoch 690, Loss: 0.0724879652261734, Grad Norm: 0.10882888734340668\n",
      "Epoch 700, Loss: 0.07169951498508453, Grad Norm: 0.10738640278577805\n",
      "Epoch 710, Loss: 0.07092423737049103, Grad Norm: 0.10599751770496368\n",
      "Epoch 720, Loss: 0.07016174495220184, Grad Norm: 0.10465842485427856\n",
      "Epoch 730, Loss: 0.06941162049770355, Grad Norm: 0.10336574167013168\n",
      "Epoch 740, Loss: 0.06867354363203049, Grad Norm: 0.1021161749958992\n",
      "Epoch 750, Loss: 0.06794728338718414, Grad Norm: 0.10090702772140503\n",
      "Epoch 760, Loss: 0.06723233312368393, Grad Norm: 0.0997355505824089\n",
      "Epoch 770, Loss: 0.06652854382991791, Grad Norm: 0.09859925508499146\n",
      "Epoch 780, Loss: 0.06583553552627563, Grad Norm: 0.09749596565961838\n",
      "Epoch 790, Loss: 0.06515312939882278, Grad Norm: 0.09642356634140015\n",
      "Epoch 800, Loss: 0.06448103487491608, Grad Norm: 0.09538023173809052\n",
      "Epoch 810, Loss: 0.06381897628307343, Grad Norm: 0.09436424821615219\n",
      "Epoch 820, Loss: 0.06316675245761871, Grad Norm: 0.09337399154901505\n",
      "Epoch 830, Loss: 0.06252411007881165, Grad Norm: 0.09240785986185074\n",
      "Epoch 840, Loss: 0.06189081817865372, Grad Norm: 0.09146478027105331\n",
      "Epoch 850, Loss: 0.061266738921403885, Grad Norm: 0.09054328501224518\n",
      "Epoch 860, Loss: 0.060651566833257675, Grad Norm: 0.08964230865240097\n",
      "Epoch 870, Loss: 0.06004521995782852, Grad Norm: 0.0887608751654625\n",
      "Epoch 880, Loss: 0.05944739282131195, Grad Norm: 0.08789782226085663\n",
      "Epoch 890, Loss: 0.058857984840869904, Grad Norm: 0.08705247193574905\n",
      "Epoch 900, Loss: 0.05827684700489044, Grad Norm: 0.08622381091117859\n",
      "Epoch 910, Loss: 0.05770369991660118, Grad Norm: 0.08541121333837509\n",
      "Epoch 920, Loss: 0.057138510048389435, Grad Norm: 0.08461394906044006\n",
      "Epoch 930, Loss: 0.056580930948257446, Grad Norm: 0.08383119851350784\n",
      "Epoch 940, Loss: 0.05603102967143059, Grad Norm: 0.08306257426738739\n",
      "Epoch 950, Loss: 0.055488500744104385, Grad Norm: 0.08230733871459961\n",
      "Epoch 960, Loss: 0.054953187704086304, Grad Norm: 0.08156495541334152\n",
      "Epoch 970, Loss: 0.05442504584789276, Grad Norm: 0.08083499222993851\n",
      "Epoch 980, Loss: 0.05390390008687973, Grad Norm: 0.08011697977781296\n",
      "Epoch 990, Loss: 0.05338960886001587, Grad Norm: 0.07941054552793503\n",
      "Epoch 1000, Loss: 0.05288204923272133, Grad Norm: 0.0787152498960495\n",
      "Epoch 1010, Loss: 0.05238111689686775, Grad Norm: 0.07803071290254593\n",
      "Epoch 1020, Loss: 0.05188663676381111, Grad Norm: 0.07735635340213776\n",
      "Epoch 1030, Loss: 0.05139850825071335, Grad Norm: 0.0766921415925026\n",
      "Epoch 1040, Loss: 0.050916656851768494, Grad Norm: 0.07603760063648224\n",
      "Epoch 1050, Loss: 0.05044092610478401, Grad Norm: 0.0753924548625946\n",
      "Epoch 1060, Loss: 0.04997113347053528, Grad Norm: 0.07475636899471283\n",
      "Epoch 1070, Loss: 0.04950730502605438, Grad Norm: 0.07412917912006378\n",
      "Epoch 1080, Loss: 0.04904923588037491, Grad Norm: 0.07351041585206985\n",
      "Epoch 1090, Loss: 0.04859688878059387, Grad Norm: 0.07290004938840866\n",
      "Epoch 1100, Loss: 0.04815012589097023, Grad Norm: 0.07229769974946976\n",
      "Epoch 1110, Loss: 0.04770885780453682, Grad Norm: 0.0717032179236412\n",
      "Epoch 1120, Loss: 0.04727298766374588, Grad Norm: 0.07111639529466629\n",
      "Epoch 1130, Loss: 0.04684238135814667, Grad Norm: 0.07053690403699875\n",
      "Epoch 1140, Loss: 0.046417005360126495, Grad Norm: 0.06996463984251022\n",
      "Epoch 1150, Loss: 0.0459967777132988, Grad Norm: 0.06939934194087982\n",
      "Epoch 1160, Loss: 0.045581500977277756, Grad Norm: 0.06884092092514038\n",
      "Epoch 1170, Loss: 0.04517120495438576, Grad Norm: 0.06828917562961578\n",
      "Epoch 1180, Loss: 0.044765837490558624, Grad Norm: 0.06774384528398514\n",
      "Epoch 1190, Loss: 0.04436514526605606, Grad Norm: 0.06720484793186188\n",
      "Epoch 1200, Loss: 0.04396921396255493, Grad Norm: 0.06667204946279526\n",
      "Epoch 1210, Loss: 0.04357791692018509, Grad Norm: 0.06614526361227036\n",
      "Epoch 1220, Loss: 0.04319111257791519, Grad Norm: 0.06562428921461105\n",
      "Epoch 1230, Loss: 0.042808808386325836, Grad Norm: 0.06510905921459198\n",
      "Epoch 1240, Loss: 0.04243086278438568, Grad Norm: 0.06459935009479523\n",
      "Epoch 1250, Loss: 0.04205729812383652, Grad Norm: 0.06409513205289841\n",
      "Epoch 1260, Loss: 0.041687965393066406, Grad Norm: 0.0635962188243866\n",
      "Epoch 1270, Loss: 0.041322771459817886, Grad Norm: 0.06310250610113144\n",
      "Epoch 1280, Loss: 0.040961794555187225, Grad Norm: 0.06261390447616577\n",
      "Epoch 1290, Loss: 0.04060480743646622, Grad Norm: 0.06213019788265228\n",
      "Epoch 1300, Loss: 0.04025183990597725, Grad Norm: 0.06165134161710739\n",
      "Epoch 1310, Loss: 0.03990277647972107, Grad Norm: 0.061177339404821396\n",
      "Epoch 1320, Loss: 0.039557620882987976, Grad Norm: 0.0607079453766346\n",
      "Epoch 1330, Loss: 0.039216235280036926, Grad Norm: 0.06024312600493431\n",
      "Epoch 1340, Loss: 0.0388786643743515, Grad Norm: 0.05978278070688248\n",
      "Epoch 1350, Loss: 0.03854475915431976, Grad Norm: 0.05932679772377014\n",
      "Epoch 1360, Loss: 0.03821451961994171, Grad Norm: 0.05887513607740402\n",
      "Epoch 1370, Loss: 0.03788784146308899, Grad Norm: 0.058427631855010986\n",
      "Epoch 1380, Loss: 0.03756466507911682, Grad Norm: 0.05798420310020447\n",
      "Epoch 1390, Loss: 0.037244997918605804, Grad Norm: 0.05754480138421059\n",
      "Epoch 1400, Loss: 0.03692873939871788, Grad Norm: 0.057109273970127106\n",
      "Epoch 1410, Loss: 0.03661588579416275, Grad Norm: 0.056677643209695816\n",
      "Epoch 1420, Loss: 0.03630633279681206, Grad Norm: 0.05624990537762642\n",
      "Epoch 1430, Loss: 0.036000076681375504, Grad Norm: 0.05582587420940399\n",
      "Epoch 1440, Loss: 0.0356970876455307, Grad Norm: 0.055405475199222565\n",
      "Epoch 1450, Loss: 0.0353972427546978, Grad Norm: 0.05498873442411423\n",
      "Epoch 1460, Loss: 0.0351005420088768, Grad Norm: 0.05457555130124092\n",
      "Epoch 1470, Loss: 0.034806959331035614, Grad Norm: 0.05416586995124817\n",
      "Epoch 1480, Loss: 0.034516431391239166, Grad Norm: 0.053759653121232986\n",
      "Epoch 1490, Loss: 0.03422892466187477, Grad Norm: 0.05335673689842224\n",
      "Epoch 1500, Loss: 0.033944372087717056, Grad Norm: 0.05295716971158981\n",
      "Epoch 1510, Loss: 0.03366276994347572, Grad Norm: 0.05256093665957451\n",
      "Epoch 1520, Loss: 0.033384062349796295, Grad Norm: 0.05216791853308678\n",
      "Epoch 1530, Loss: 0.03310822695493698, Grad Norm: 0.05177809298038483\n",
      "Epoch 1540, Loss: 0.032835155725479126, Grad Norm: 0.05139137804508209\n",
      "Epoch 1550, Loss: 0.032564930617809296, Grad Norm: 0.05100780352950096\n",
      "Epoch 1560, Loss: 0.03229741007089615, Grad Norm: 0.05062726512551308\n",
      "Epoch 1570, Loss: 0.03203257918357849, Grad Norm: 0.05024976283311844\n",
      "Epoch 1580, Loss: 0.031770482659339905, Grad Norm: 0.049875225871801376\n",
      "Epoch 1590, Loss: 0.03151094168424606, Grad Norm: 0.0495036318898201\n",
      "Epoch 1600, Loss: 0.03125406801700592, Grad Norm: 0.0491349920630455\n",
      "Epoch 1610, Loss: 0.03099975362420082, Grad Norm: 0.04876920208334923\n",
      "Epoch 1620, Loss: 0.030747976154088974, Grad Norm: 0.04840623214840889\n",
      "Epoch 1630, Loss: 0.03049868904054165, Grad Norm: 0.04804608225822449\n",
      "Epoch 1640, Loss: 0.030251897871494293, Grad Norm: 0.047688741236925125\n",
      "Epoch 1650, Loss: 0.03000752627849579, Grad Norm: 0.04733412340283394\n",
      "Epoch 1660, Loss: 0.029765550047159195, Grad Norm: 0.04698225110769272\n",
      "Epoch 1670, Loss: 0.02952597290277481, Grad Norm: 0.046633053570985794\n",
      "Epoch 1680, Loss: 0.029288752004504204, Grad Norm: 0.04628654196858406\n",
      "Epoch 1690, Loss: 0.02905384637415409, Grad Norm: 0.04594263434410095\n",
      "Epoch 1700, Loss: 0.028821228072047234, Grad Norm: 0.045601386576890945\n",
      "Epoch 1710, Loss: 0.02859090082347393, Grad Norm: 0.04526268318295479\n",
      "Epoch 1720, Loss: 0.028362827375531197, Grad Norm: 0.044926583766937256\n",
      "Epoch 1730, Loss: 0.02813691273331642, Grad Norm: 0.04459305480122566\n",
      "Epoch 1740, Loss: 0.027913212776184082, Grad Norm: 0.04426197707653046\n",
      "Epoch 1750, Loss: 0.027691669762134552, Grad Norm: 0.043933458626270294\n",
      "Epoch 1760, Loss: 0.027472276240587234, Grad Norm: 0.04360741004347801\n",
      "Epoch 1770, Loss: 0.027254965156316757, Grad Norm: 0.04328382387757301\n",
      "Epoch 1780, Loss: 0.027039753273129463, Grad Norm: 0.042962681502103806\n",
      "Epoch 1790, Loss: 0.02682662382721901, Grad Norm: 0.04264391213655472\n",
      "Epoch 1800, Loss: 0.026615489274263382, Grad Norm: 0.04232756048440933\n",
      "Epoch 1810, Loss: 0.02640632539987564, Grad Norm: 0.042013585567474365\n",
      "Epoch 1820, Loss: 0.026199232786893845, Grad Norm: 0.04170200228691101\n",
      "Epoch 1830, Loss: 0.025994056835770607, Grad Norm: 0.04139275103807449\n",
      "Epoch 1840, Loss: 0.025790823623538017, Grad Norm: 0.04108582064509392\n",
      "Epoch 1850, Loss: 0.02558951824903488, Grad Norm: 0.04078124091029167\n",
      "Epoch 1860, Loss: 0.025390122085809708, Grad Norm: 0.040478888899087906\n",
      "Epoch 1870, Loss: 0.025192586705088615, Grad Norm: 0.04017888754606247\n",
      "Epoch 1880, Loss: 0.024996910244226456, Grad Norm: 0.03988116234540939\n",
      "Epoch 1890, Loss: 0.024803057312965393, Grad Norm: 0.0395856536924839\n",
      "Epoch 1900, Loss: 0.02461104467511177, Grad Norm: 0.039292335510253906\n",
      "Epoch 1910, Loss: 0.024420805275440216, Grad Norm: 0.03900127112865448\n",
      "Epoch 1920, Loss: 0.024232329800724983, Grad Norm: 0.038712386041879654\n",
      "Epoch 1930, Loss: 0.024045638740062714, Grad Norm: 0.038425713777542114\n",
      "Epoch 1940, Loss: 0.023860609158873558, Grad Norm: 0.03814118728041649\n",
      "Epoch 1950, Loss: 0.023677334189414978, Grad Norm: 0.03785887360572815\n",
      "Epoch 1960, Loss: 0.023495778441429138, Grad Norm: 0.03757862001657486\n",
      "Epoch 1970, Loss: 0.023315850645303726, Grad Norm: 0.03730052337050438\n",
      "Epoch 1980, Loss: 0.023137623444199562, Grad Norm: 0.037024594843387604\n",
      "Epoch 1990, Loss: 0.022961005568504333, Grad Norm: 0.03675070405006409\n",
      "Epoch 2000, Loss: 0.022785991430282593, Grad Norm: 0.036478862166404724\n",
      "Epoch 2010, Loss: 0.022612614557147026, Grad Norm: 0.03620915487408638\n",
      "Epoch 2020, Loss: 0.022440779954195023, Grad Norm: 0.03594148904085159\n",
      "Epoch 2030, Loss: 0.022270532324910164, Grad Norm: 0.035675834864377975\n",
      "Epoch 2040, Loss: 0.022101886570453644, Grad Norm: 0.035412274301052094\n",
      "Epoch 2050, Loss: 0.02193472906947136, Grad Norm: 0.0351506844162941\n",
      "Epoch 2060, Loss: 0.021769048646092415, Grad Norm: 0.03489110991358757\n",
      "Epoch 2070, Loss: 0.02160491980612278, Grad Norm: 0.03463353216648102\n",
      "Epoch 2080, Loss: 0.02144227735698223, Grad Norm: 0.03437790647149086\n",
      "Epoch 2090, Loss: 0.021281106397509575, Grad Norm: 0.03412429988384247\n",
      "Epoch 2100, Loss: 0.021121419966220856, Grad Norm: 0.03387260437011719\n",
      "Epoch 2110, Loss: 0.020963096991181374, Grad Norm: 0.033622853457927704\n",
      "Epoch 2120, Loss: 0.02080621011555195, Grad Norm: 0.03337505832314491\n",
      "Epoch 2130, Loss: 0.020650740712881088, Grad Norm: 0.033129122108221054\n",
      "Epoch 2140, Loss: 0.020496681332588196, Grad Norm: 0.0328851155936718\n",
      "Epoch 2150, Loss: 0.020343992859125137, Grad Norm: 0.03264302387833595\n",
      "Epoch 2160, Loss: 0.02019261196255684, Grad Norm: 0.03240276500582695\n",
      "Epoch 2170, Loss: 0.020042646676301956, Grad Norm: 0.03216436132788658\n",
      "Epoch 2180, Loss: 0.019893990829586983, Grad Norm: 0.03192783519625664\n",
      "Epoch 2190, Loss: 0.019746657460927963, Grad Norm: 0.031693123281002045\n",
      "Epoch 2200, Loss: 0.01960061676800251, Grad Norm: 0.03146025165915489\n",
      "Epoch 2210, Loss: 0.01945587806403637, Grad Norm: 0.031229214742779732\n",
      "Epoch 2220, Loss: 0.019312415271997452, Grad Norm: 0.030999936163425446\n",
      "Epoch 2230, Loss: 0.019170206040143967, Grad Norm: 0.030772464349865913\n",
      "Epoch 2240, Loss: 0.019029242917895317, Grad Norm: 0.03054676204919815\n",
      "Epoch 2250, Loss: 0.018889546394348145, Grad Norm: 0.030322832986712456\n",
      "Epoch 2260, Loss: 0.018751047551631927, Grad Norm: 0.030100615695118904\n",
      "Epoch 2270, Loss: 0.018613753840327263, Grad Norm: 0.029880177229642868\n",
      "Epoch 2280, Loss: 0.018477637320756912, Grad Norm: 0.02966146171092987\n",
      "Epoch 2290, Loss: 0.018342789262533188, Grad Norm: 0.02944442443549633\n",
      "Epoch 2300, Loss: 0.01820908486843109, Grad Norm: 0.029229138046503067\n",
      "Epoch 2310, Loss: 0.01807650923728943, Grad Norm: 0.029015492647886276\n",
      "Epoch 2320, Loss: 0.017945121973752975, Grad Norm: 0.028803545981645584\n",
      "Epoch 2330, Loss: 0.01781487837433815, Grad Norm: 0.02859327383339405\n",
      "Epoch 2340, Loss: 0.017685730010271072, Grad Norm: 0.028384681791067123\n",
      "Epoch 2350, Loss: 0.017557751387357712, Grad Norm: 0.028177663683891296\n",
      "Epoch 2360, Loss: 0.017430808395147324, Grad Norm: 0.02797229215502739\n",
      "Epoch 2370, Loss: 0.01730502024292946, Grad Norm: 0.027768559753894806\n",
      "Epoch 2380, Loss: 0.017180263996124268, Grad Norm: 0.02756645157933235\n",
      "Epoch 2390, Loss: 0.017056599259376526, Grad Norm: 0.027365922927856445\n",
      "Epoch 2400, Loss: 0.016933996230363846, Grad Norm: 0.027166912332177162\n",
      "Epoch 2410, Loss: 0.016812408342957497, Grad Norm: 0.02696952410042286\n",
      "Epoch 2420, Loss: 0.016691910102963448, Grad Norm: 0.02677365392446518\n",
      "Epoch 2430, Loss: 0.01657242700457573, Grad Norm: 0.026579393073916435\n",
      "Epoch 2440, Loss: 0.016453955322504044, Grad Norm: 0.026386648416519165\n",
      "Epoch 2450, Loss: 0.016336482018232346, Grad Norm: 0.026195449754595757\n",
      "Epoch 2460, Loss: 0.016220036894083023, Grad Norm: 0.026005765423178673\n",
      "Epoch 2470, Loss: 0.016104556620121002, Grad Norm: 0.025817591696977615\n",
      "Epoch 2480, Loss: 0.015990041196346283, Grad Norm: 0.02563089318573475\n",
      "Epoch 2490, Loss: 0.015876539051532745, Grad Norm: 0.025445694103837013\n",
      "Epoch 2500, Loss: 0.015763921663165092, Grad Norm: 0.025261947885155678\n",
      "Epoch 2510, Loss: 0.015652330592274666, Grad Norm: 0.025079669430851936\n",
      "Epoch 2520, Loss: 0.015541626140475273, Grad Norm: 0.024898827075958252\n",
      "Epoch 2530, Loss: 0.015431884676218033, Grad Norm: 0.02471942827105522\n",
      "Epoch 2540, Loss: 0.01532304473221302, Grad Norm: 0.02454140968620777\n",
      "Epoch 2550, Loss: 0.01521512120962143, Grad Norm: 0.02436482347548008\n",
      "Epoch 2560, Loss: 0.015108082443475723, Grad Norm: 0.024189641699194908\n",
      "Epoch 2570, Loss: 0.01500195823609829, Grad Norm: 0.024015823379158974\n",
      "Epoch 2580, Loss: 0.014896704815328121, Grad Norm: 0.023843366652727127\n",
      "Epoch 2590, Loss: 0.0147923044860363, Grad Norm: 0.023672308772802353\n",
      "Epoch 2600, Loss: 0.014688802883028984, Grad Norm: 0.02350255288183689\n",
      "Epoch 2610, Loss: 0.014586154371500015, Grad Norm: 0.023334112018346786\n",
      "Epoch 2620, Loss: 0.014484327286481857, Grad Norm: 0.023167071864008904\n",
      "Epoch 2630, Loss: 0.014383383095264435, Grad Norm: 0.02300136908888817\n",
      "Epoch 2640, Loss: 0.014283260330557823, Grad Norm: 0.022836962714791298\n",
      "Epoch 2650, Loss: 0.01418394222855568, Grad Norm: 0.022673873230814934\n",
      "Epoch 2660, Loss: 0.014085475355386734, Grad Norm: 0.022512046620249748\n",
      "Epoch 2670, Loss: 0.013987753540277481, Grad Norm: 0.02235148847103119\n",
      "Epoch 2680, Loss: 0.013890925794839859, Grad Norm: 0.022192226722836494\n",
      "Epoch 2690, Loss: 0.013794842176139355, Grad Norm: 0.022034283727407455\n",
      "Epoch 2700, Loss: 0.0136994868516922, Grad Norm: 0.02187751606106758\n",
      "Epoch 2710, Loss: 0.013605009764432907, Grad Norm: 0.02172200381755829\n",
      "Epoch 2720, Loss: 0.013511247001588345, Grad Norm: 0.021567724645137787\n",
      "Epoch 2730, Loss: 0.013418254442512989, Grad Norm: 0.021414663642644882\n",
      "Epoch 2740, Loss: 0.013326035812497139, Grad Norm: 0.02126285620033741\n",
      "Epoch 2750, Loss: 0.01323454175144434, Grad Norm: 0.021112246438860893\n",
      "Epoch 2760, Loss: 0.013143820688128471, Grad Norm: 0.020962756127119064\n",
      "Epoch 2770, Loss: 0.013053778558969498, Grad Norm: 0.020814500749111176\n",
      "Epoch 2780, Loss: 0.0129645224660635, Grad Norm: 0.0206674262881279\n",
      "Epoch 2790, Loss: 0.012875962071120739, Grad Norm: 0.02052147313952446\n",
      "Epoch 2800, Loss: 0.01278812624514103, Grad Norm: 0.02037670649588108\n",
      "Epoch 2810, Loss: 0.012700939550995827, Grad Norm: 0.02023312635719776\n",
      "Epoch 2820, Loss: 0.012614507228136063, Grad Norm: 0.020090606063604355\n",
      "Epoch 2830, Loss: 0.01252873707562685, Grad Norm: 0.019949184730648994\n",
      "Epoch 2840, Loss: 0.012443691492080688, Grad Norm: 0.01980895921587944\n",
      "Epoch 2850, Loss: 0.01235929410904646, Grad Norm: 0.019669827073812485\n",
      "Epoch 2860, Loss: 0.012275603599846363, Grad Norm: 0.01953176036477089\n",
      "Epoch 2870, Loss: 0.012192530557513237, Grad Norm: 0.01939479261636734\n",
      "Epoch 2880, Loss: 0.012110165320336819, Grad Norm: 0.019258907064795494\n",
      "Epoch 2890, Loss: 0.012028416618704796, Grad Norm: 0.019124064594507217\n",
      "Epoch 2900, Loss: 0.01194734312593937, Grad Norm: 0.018990324810147285\n",
      "Epoch 2910, Loss: 0.011866872198879719, Grad Norm: 0.018857616931200027\n",
      "Epoch 2920, Loss: 0.01178707741200924, Grad Norm: 0.0187259241938591\n",
      "Epoch 2930, Loss: 0.011707882396876812, Grad Norm: 0.018595317378640175\n",
      "Epoch 2940, Loss: 0.011629333719611168, Grad Norm: 0.018465708941221237\n",
      "Epoch 2950, Loss: 0.011551398783922195, Grad Norm: 0.01833709515631199\n",
      "Epoch 2960, Loss: 0.011474063619971275, Grad Norm: 0.01820957474410534\n",
      "Epoch 2970, Loss: 0.011397359892725945, Grad Norm: 0.0180830005556345\n",
      "Epoch 2980, Loss: 0.01132122427225113, Grad Norm: 0.017957396805286407\n",
      "Epoch 2990, Loss: 0.011245733126997948, Grad Norm: 0.017832793295383453\n",
      "Epoch 3000, Loss: 0.011170809157192707, Grad Norm: 0.017709147185087204\n",
      "Epoch 3010, Loss: 0.011096440255641937, Grad Norm: 0.01758650504052639\n",
      "Epoch 3020, Loss: 0.011022653430700302, Grad Norm: 0.017464803531765938\n",
      "Epoch 3030, Loss: 0.010949481278657913, Grad Norm: 0.0173440370708704\n",
      "Epoch 3040, Loss: 0.010876846499741077, Grad Norm: 0.01722424104809761\n",
      "Epoch 3050, Loss: 0.010804778896272182, Grad Norm: 0.01710536703467369\n",
      "Epoch 3060, Loss: 0.010733263567090034, Grad Norm: 0.016987420618534088\n",
      "Epoch 3070, Loss: 0.010662315413355827, Grad Norm: 0.01687038689851761\n",
      "Epoch 3080, Loss: 0.010591905564069748, Grad Norm: 0.016754237934947014\n",
      "Epoch 3090, Loss: 0.010522046126425266, Grad Norm: 0.016639046370983124\n",
      "Epoch 3100, Loss: 0.010452723130583763, Grad Norm: 0.016524704173207283\n",
      "Epoch 3110, Loss: 0.010383937507867813, Grad Norm: 0.01641129143536091\n",
      "Epoch 3120, Loss: 0.01031565759330988, Grad Norm: 0.016298724338412285\n",
      "Epoch 3130, Loss: 0.01024791318923235, Grad Norm: 0.01618705503642559\n",
      "Epoch 3140, Loss: 0.010180690325796604, Grad Norm: 0.01607625186443329\n",
      "Epoch 3150, Loss: 0.010113988071680069, Grad Norm: 0.015966271981596947\n",
      "Epoch 3160, Loss: 0.010047760792076588, Grad Norm: 0.015857147052884102\n",
      "Epoch 3170, Loss: 0.009982084855437279, Grad Norm: 0.015748893842101097\n",
      "Epoch 3180, Loss: 0.009916852228343487, Grad Norm: 0.015641463920474052\n",
      "Epoch 3190, Loss: 0.009852155111730099, Grad Norm: 0.015534861013293266\n",
      "Epoch 3200, Loss: 0.009787948802113533, Grad Norm: 0.015429060906171799\n",
      "Epoch 3210, Loss: 0.00972423143684864, Grad Norm: 0.01532411016523838\n",
      "Epoch 3220, Loss: 0.009660972282290459, Grad Norm: 0.015219954773783684\n",
      "Epoch 3230, Loss: 0.009598219767212868, Grad Norm: 0.015116621740162373\n",
      "Epoch 3240, Loss: 0.009535923600196838, Grad Norm: 0.015014063566923141\n",
      "Epoch 3250, Loss: 0.009474104270339012, Grad Norm: 0.014912279322743416\n",
      "Epoch 3260, Loss: 0.009412741288542747, Grad Norm: 0.014811282977461815\n",
      "Epoch 3270, Loss: 0.009351822547614574, Grad Norm: 0.014711065217852592\n",
      "Epoch 3280, Loss: 0.009291376918554306, Grad Norm: 0.014611613936722279\n",
      "Epoch 3290, Loss: 0.009231390431523323, Grad Norm: 0.014512899331748486\n",
      "Epoch 3300, Loss: 0.009171847254037857, Grad Norm: 0.014414981007575989\n",
      "Epoch 3310, Loss: 0.009112762287259102, Grad Norm: 0.014317800290882587\n",
      "Epoch 3320, Loss: 0.009054059162735939, Grad Norm: 0.014221340417861938\n",
      "Epoch 3330, Loss: 0.008995888754725456, Grad Norm: 0.014125634916126728\n",
      "Epoch 3340, Loss: 0.008938087150454521, Grad Norm: 0.014030640944838524\n",
      "Epoch 3350, Loss: 0.00888071209192276, Grad Norm: 0.013936394825577736\n",
      "Epoch 3360, Loss: 0.008823780342936516, Grad Norm: 0.013842822052538395\n",
      "Epoch 3370, Loss: 0.008767275139689445, Grad Norm: 0.013750028796494007\n",
      "Epoch 3380, Loss: 0.008711181581020355, Grad Norm: 0.013657893985509872\n",
      "Epoch 3390, Loss: 0.008655501529574394, Grad Norm: 0.01356645580381155\n",
      "Epoch 3400, Loss: 0.008600217290222645, Grad Norm: 0.013475717045366764\n",
      "Epoch 3410, Loss: 0.008545374497771263, Grad Norm: 0.013385665602982044\n",
      "Epoch 3420, Loss: 0.008490944281220436, Grad Norm: 0.013296310789883137\n",
      "Epoch 3430, Loss: 0.00843685120344162, Grad Norm: 0.01320762187242508\n",
      "Epoch 3440, Loss: 0.008383182808756828, Grad Norm: 0.013119582086801529\n",
      "Epoch 3450, Loss: 0.008329912088811398, Grad Norm: 0.013032198883593082\n",
      "Epoch 3460, Loss: 0.00827702134847641, Grad Norm: 0.012945516966283321\n",
      "Epoch 3470, Loss: 0.008224528282880783, Grad Norm: 0.01285949070006609\n",
      "Epoch 3480, Loss: 0.0081724151968956, Grad Norm: 0.012774077244102955\n",
      "Epoch 3490, Loss: 0.008120683021843433, Grad Norm: 0.012689310126006603\n",
      "Epoch 3500, Loss: 0.008069331757724285, Grad Norm: 0.012605187483131886\n",
      "Epoch 3510, Loss: 0.008018315769731998, Grad Norm: 0.012521703727543354\n",
      "Epoch 3520, Loss: 0.007967711426317692, Grad Norm: 0.01243882067501545\n",
      "Epoch 3530, Loss: 0.007917501032352448, Grad Norm: 0.012356587685644627\n",
      "Epoch 3540, Loss: 0.007867597974836826, Grad Norm: 0.01227495539933443\n",
      "Epoch 3550, Loss: 0.007818073965609074, Grad Norm: 0.012193931266665459\n",
      "Epoch 3560, Loss: 0.007768900599330664, Grad Norm: 0.012113498523831367\n",
      "Epoch 3570, Loss: 0.007720107212662697, Grad Norm: 0.012033674865961075\n",
      "Epoch 3580, Loss: 0.007671649567782879, Grad Norm: 0.011954452842473984\n",
      "Epoch 3590, Loss: 0.007623526267707348, Grad Norm: 0.011875836178660393\n",
      "Epoch 3600, Loss: 0.007575753144919872, Grad Norm: 0.011797796934843063\n",
      "Epoch 3610, Loss: 0.007528344169259071, Grad Norm: 0.011720300652086735\n",
      "Epoch 3620, Loss: 0.007481240201741457, Grad Norm: 0.011643389239907265\n",
      "Epoch 3630, Loss: 0.007434485945850611, Grad Norm: 0.011567067354917526\n",
      "Epoch 3640, Loss: 0.007388066500425339, Grad Norm: 0.011491304263472557\n",
      "Epoch 3650, Loss: 0.007341996766626835, Grad Norm: 0.011416089721024036\n",
      "Epoch 3660, Loss: 0.0072962455451488495, Grad Norm: 0.01134145725518465\n",
      "Epoch 3670, Loss: 0.007250814698636532, Grad Norm: 0.011267337016761303\n",
      "Epoch 3680, Loss: 0.007205702364444733, Grad Norm: 0.011193808168172836\n",
      "Epoch 3690, Loss: 0.007160909473896027, Grad Norm: 0.011120801791548729\n",
      "Epoch 3700, Loss: 0.007116436026990414, Grad Norm: 0.011048302054405212\n",
      "Epoch 3710, Loss: 0.0070722815580666065, Grad Norm: 0.010976382531225681\n",
      "Epoch 3720, Loss: 0.007028445601463318, Grad Norm: 0.010904964059591293\n",
      "Epoch 3730, Loss: 0.006984898820519447, Grad Norm: 0.010834075510501862\n",
      "Epoch 3740, Loss: 0.006941670551896095, Grad Norm: 0.010763735510408878\n",
      "Epoch 3750, Loss: 0.006898761726915836, Grad Norm: 0.010693872347474098\n",
      "Epoch 3760, Loss: 0.006856126245111227, Grad Norm: 0.010624537244439125\n",
      "Epoch 3770, Loss: 0.006813809275627136, Grad Norm: 0.010555680841207504\n",
      "Epoch 3780, Loss: 0.006771796382963657, Grad Norm: 0.010487351566553116\n",
      "Epoch 3790, Loss: 0.006730056367814541, Grad Norm: 0.0104195112362504\n",
      "Epoch 3800, Loss: 0.006688620895147324, Grad Norm: 0.010352188721299171\n",
      "Epoch 3810, Loss: 0.006647457834333181, Grad Norm: 0.0102853337302804\n",
      "Epoch 3820, Loss: 0.006606613285839558, Grad Norm: 0.010218960233032703\n",
      "Epoch 3830, Loss: 0.006566042546182871, Grad Norm: 0.010153084993362427\n",
      "Epoch 3840, Loss: 0.006525729782879353, Grad Norm: 0.010087687522172928\n",
      "Epoch 3850, Loss: 0.006485704332590103, Grad Norm: 0.010022779926657677\n",
      "Epoch 3860, Loss: 0.006445998325943947, Grad Norm: 0.00995828676968813\n",
      "Epoch 3870, Loss: 0.0064065344631671906, Grad Norm: 0.009894301183521748\n",
      "Epoch 3880, Loss: 0.006367344409227371, Grad Norm: 0.009830798022449017\n",
      "Epoch 3890, Loss: 0.006328427232801914, Grad Norm: 0.009767717681825161\n",
      "Epoch 3900, Loss: 0.0062897829338908195, Grad Norm: 0.009705101139843464\n",
      "Epoch 3910, Loss: 0.006251411512494087, Grad Norm: 0.009642927907407284\n",
      "Epoch 3920, Loss: 0.006213282234966755, Grad Norm: 0.009581214748322964\n",
      "Epoch 3930, Loss: 0.006175426300615072, Grad Norm: 0.009519923478364944\n",
      "Epoch 3940, Loss: 0.006137827876955271, Grad Norm: 0.009459113702178001\n",
      "Epoch 3950, Loss: 0.006100502330809832, Grad Norm: 0.009398691356182098\n",
      "Epoch 3960, Loss: 0.006063449662178755, Grad Norm: 0.009338732808828354\n",
      "Epoch 3970, Loss: 0.006026608869433403, Grad Norm: 0.00927920825779438\n",
      "Epoch 3980, Loss: 0.0059900106862187386, Grad Norm: 0.009220079518854618\n",
      "Epoch 3990, Loss: 0.005953730549663305, Grad Norm: 0.00916139967739582\n",
      "Epoch 4000, Loss: 0.005917632952332497, Grad Norm: 0.00910312682390213\n",
      "Epoch 4010, Loss: 0.005881821736693382, Grad Norm: 0.009045294485986233\n",
      "Epoch 4020, Loss: 0.005846239160746336, Grad Norm: 0.00898782815784216\n",
      "Epoch 4030, Loss: 0.005810883361846209, Grad Norm: 0.008930797688663006\n",
      "Epoch 4040, Loss: 0.005775799974799156, Grad Norm: 0.008874178864061832\n",
      "Epoch 4050, Loss: 0.005740913562476635, Grad Norm: 0.00881794560700655\n",
      "Epoch 4060, Loss: 0.005706299562007189, Grad Norm: 0.00876213051378727\n",
      "Epoch 4070, Loss: 0.005671897903084755, Grad Norm: 0.00870670285075903\n",
      "Epoch 4080, Loss: 0.005637737922370434, Grad Norm: 0.008651670999825\n",
      "Epoch 4090, Loss: 0.005603790283203125, Grad Norm: 0.008597008883953094\n",
      "Epoch 4100, Loss: 0.005570144392549992, Grad Norm: 0.008542737923562527\n",
      "Epoch 4110, Loss: 0.005536636337637901, Grad Norm: 0.008488877676427364\n",
      "Epoch 4120, Loss: 0.005503429565578699, Grad Norm: 0.008435377851128578\n",
      "Epoch 4130, Loss: 0.0054703750647604465, Grad Norm: 0.008382258005440235\n",
      "Epoch 4140, Loss: 0.005437592510133982, Grad Norm: 0.00832950510084629\n",
      "Epoch 4150, Loss: 0.005405021831393242, Grad Norm: 0.008277129381895065\n",
      "Epoch 4160, Loss: 0.0053726183250546455, Grad Norm: 0.008225143887102604\n",
      "Epoch 4170, Loss: 0.005340485833585262, Grad Norm: 0.008173493668437004\n",
      "Epoch 4180, Loss: 0.005308565683662891, Grad Norm: 0.008122206665575504\n",
      "Epoch 4190, Loss: 0.005276842042803764, Grad Norm: 0.008071276359260082\n",
      "Epoch 4200, Loss: 0.005245360545814037, Grad Norm: 0.008020693436264992\n",
      "Epoch 4210, Loss: 0.005214059725403786, Grad Norm: 0.007970479317009449\n",
      "Epoch 4220, Loss: 0.005182941909879446, Grad Norm: 0.00792061910033226\n",
      "Epoch 4230, Loss: 0.005152080208063126, Grad Norm: 0.007871122099459171\n",
      "Epoch 4240, Loss: 0.005121414549648762, Grad Norm: 0.007821951992809772\n",
      "Epoch 4250, Loss: 0.00509096123278141, Grad Norm: 0.007773090619593859\n",
      "Epoch 4260, Loss: 0.0050607044249773026, Grad Norm: 0.00772462785243988\n",
      "Epoch 4270, Loss: 0.0050306133925914764, Grad Norm: 0.007676477078348398\n",
      "Epoch 4280, Loss: 0.0050007798708975315, Grad Norm: 0.007628661580383778\n",
      "Epoch 4290, Loss: 0.0049710823222994804, Grad Norm: 0.007581192068755627\n",
      "Epoch 4300, Loss: 0.004941611085087061, Grad Norm: 0.007534021977335215\n",
      "Epoch 4310, Loss: 0.004912366159260273, Grad Norm: 0.007487183436751366\n",
      "Epoch 4320, Loss: 0.004883242771029472, Grad Norm: 0.007440690416842699\n",
      "Epoch 4330, Loss: 0.004854375962167978, Grad Norm: 0.00739450566470623\n",
      "Epoch 4340, Loss: 0.004825660493224859, Grad Norm: 0.007348631042987108\n",
      "Epoch 4350, Loss: 0.004797141067683697, Grad Norm: 0.007303063292056322\n",
      "Epoch 4360, Loss: 0.004768787883222103, Grad Norm: 0.007257853634655476\n",
      "Epoch 4370, Loss: 0.004740631673485041, Grad Norm: 0.007212938275188208\n",
      "Epoch 4380, Loss: 0.004712686408311129, Grad Norm: 0.007168317213654518\n",
      "Epoch 4390, Loss: 0.004684937186539173, Grad Norm: 0.007124011870473623\n",
      "Epoch 4400, Loss: 0.004657338839024305, Grad Norm: 0.0070800138637423515\n",
      "Epoch 4410, Loss: 0.004629922099411488, Grad Norm: 0.007036310620605946\n",
      "Epoch 4420, Loss: 0.00460268696770072, Grad Norm: 0.006992891430854797\n",
      "Epoch 4430, Loss: 0.004575647413730621, Grad Norm: 0.006949801463633776\n",
      "Epoch 4440, Loss: 0.0045487587340176105, Grad Norm: 0.00690696993842721\n",
      "Epoch 4450, Loss: 0.00452205166220665, Grad Norm: 0.006864449009299278\n",
      "Epoch 4460, Loss: 0.004495494998991489, Grad Norm: 0.006822236813604832\n",
      "Epoch 4470, Loss: 0.004469149746000767, Grad Norm: 0.006780282594263554\n",
      "Epoch 4480, Loss: 0.004442986100912094, Grad Norm: 0.006738634314388037\n",
      "Epoch 4490, Loss: 0.004416957497596741, Grad Norm: 0.006697256583720446\n",
      "Epoch 4500, Loss: 0.0043910956010222435, Grad Norm: 0.00665615638718009\n",
      "Epoch 4510, Loss: 0.004365414381027222, Grad Norm: 0.006615364458411932\n",
      "Epoch 4520, Loss: 0.004339929204434156, Grad Norm: 0.006574820727109909\n",
      "Epoch 4530, Loss: 0.004314580000936985, Grad Norm: 0.006534578278660774\n",
      "Epoch 4540, Loss: 0.004289381671696901, Grad Norm: 0.006494556553661823\n",
      "Epoch 4550, Loss: 0.004264349117875099, Grad Norm: 0.006454860791563988\n",
      "Epoch 4560, Loss: 0.004239482805132866, Grad Norm: 0.006415386218577623\n",
      "Epoch 4570, Loss: 0.004214797168970108, Grad Norm: 0.006376200821250677\n",
      "Epoch 4580, Loss: 0.00419023260474205, Grad Norm: 0.006337284576147795\n",
      "Epoch 4590, Loss: 0.004165833815932274, Grad Norm: 0.006298644933849573\n",
      "Epoch 4600, Loss: 0.004141615238040686, Grad Norm: 0.006260246969759464\n",
      "Epoch 4610, Loss: 0.004117532633244991, Grad Norm: 0.006222114898264408\n",
      "Epoch 4620, Loss: 0.004093601368367672, Grad Norm: 0.00618422357365489\n",
      "Epoch 4630, Loss: 0.004069835878908634, Grad Norm: 0.006146622821688652\n",
      "Epoch 4640, Loss: 0.004046205431222916, Grad Norm: 0.006109226495027542\n",
      "Epoch 4650, Loss: 0.004022741690278053, Grad Norm: 0.006072118412703276\n",
      "Epoch 4660, Loss: 0.00399941299110651, Grad Norm: 0.006035210564732552\n",
      "Epoch 4670, Loss: 0.003976279869675636, Grad Norm: 0.005998606793582439\n",
      "Epoch 4680, Loss: 0.003953252919018269, Grad Norm: 0.005962217226624489\n",
      "Epoch 4690, Loss: 0.003930361475795507, Grad Norm: 0.005926094949245453\n",
      "Epoch 4700, Loss: 0.003907620906829834, Grad Norm: 0.005890178959816694\n",
      "Epoch 4710, Loss: 0.0038850458804517984, Grad Norm: 0.005854525603353977\n",
      "Epoch 4720, Loss: 0.0038626063615083694, Grad Norm: 0.005819100420922041\n",
      "Epoch 4730, Loss: 0.003840287448838353, Grad Norm: 0.0057839215733110905\n",
      "Epoch 4740, Loss: 0.003818164113909006, Grad Norm: 0.005748954601585865\n",
      "Epoch 4750, Loss: 0.0037961318157613277, Grad Norm: 0.00571424700319767\n",
      "Epoch 4760, Loss: 0.0037742347922176123, Grad Norm: 0.005679741036146879\n",
      "Epoch 4770, Loss: 0.0037524886429309845, Grad Norm: 0.0056455000303685665\n",
      "Epoch 4780, Loss: 0.0037309075705707073, Grad Norm: 0.005611439235508442\n",
      "Epoch 4790, Loss: 0.003709432203322649, Grad Norm: 0.0055776480585336685\n",
      "Epoch 4800, Loss: 0.0036881077103316784, Grad Norm: 0.0055440557189285755\n",
      "Epoch 4810, Loss: 0.003666918259114027, Grad Norm: 0.005510687828063965\n",
      "Epoch 4820, Loss: 0.003645834978669882, Grad Norm: 0.005477553233504295\n",
      "Epoch 4830, Loss: 0.0036249016411602497, Grad Norm: 0.005444638896733522\n",
      "Epoch 4840, Loss: 0.0036041191779077053, Grad Norm: 0.005411918740719557\n",
      "Epoch 4850, Loss: 0.0035834270529448986, Grad Norm: 0.005379428155720234\n",
      "Epoch 4860, Loss: 0.003562870202586055, Grad Norm: 0.005347161088138819\n",
      "Epoch 4870, Loss: 0.0035424642264842987, Grad Norm: 0.0053150849416852\n",
      "Epoch 4880, Loss: 0.003522193292155862, Grad Norm: 0.005283252336084843\n",
      "Epoch 4890, Loss: 0.0035020126961171627, Grad Norm: 0.0052515980787575245\n",
      "Epoch 4900, Loss: 0.0034819981083273888, Grad Norm: 0.0052201589569449425\n",
      "Epoch 4910, Loss: 0.0034620738588273525, Grad Norm: 0.005188921932131052\n",
      "Epoch 4920, Loss: 0.0034422995522618294, Grad Norm: 0.005157890263944864\n",
      "Epoch 4930, Loss: 0.0034226160496473312, Grad Norm: 0.005127076059579849\n",
      "Epoch 4940, Loss: 0.0034030978567898273, Grad Norm: 0.005096445791423321\n",
      "Epoch 4950, Loss: 0.003383670235052705, Grad Norm: 0.0050660246051847935\n",
      "Epoch 4960, Loss: 0.0033643627539277077, Grad Norm: 0.005035793874412775\n",
      "Epoch 4970, Loss: 0.0033452208153903484, Grad Norm: 0.005005769897252321\n",
      "Epoch 4980, Loss: 0.0033261540811508894, Grad Norm: 0.0049759396351873875\n",
      "Epoch 4990, Loss: 0.0033072074875235558, Grad Norm: 0.0049463072791695595\n",
      "Epoch 5000, Loss: 0.0032883668318390846, Grad Norm: 0.004916852340102196\n",
      "Epoch 5010, Loss: 0.0032696761190891266, Grad Norm: 0.004887601360678673\n",
      "Epoch 5020, Loss: 0.0032510911114513874, Grad Norm: 0.004858535714447498\n",
      "Epoch 5030, Loss: 0.0032326108776032925, Grad Norm: 0.004829653073102236\n",
      "Epoch 5040, Loss: 0.0032142363488674164, Grad Norm: 0.004800968337804079\n",
      "Epoch 5050, Loss: 0.003196011995896697, Grad Norm: 0.004772475454956293\n",
      "Epoch 5060, Loss: 0.003177907783538103, Grad Norm: 0.004744154866784811\n",
      "Epoch 5070, Loss: 0.0031598792411386967, Grad Norm: 0.004716009367257357\n",
      "Epoch 5080, Loss: 0.003141910769045353, Grad Norm: 0.00468808738514781\n",
      "Epoch 5090, Loss: 0.0031241225078701973, Grad Norm: 0.004660302307456732\n",
      "Epoch 5100, Loss: 0.0031064245849847794, Grad Norm: 0.004632714670151472\n",
      "Epoch 5110, Loss: 0.0030888619367033243, Grad Norm: 0.0046052830293774605\n",
      "Epoch 5120, Loss: 0.0030713595915585756, Grad Norm: 0.004578059539198875\n",
      "Epoch 5130, Loss: 0.0030539624858647585, Grad Norm: 0.004550980869680643\n",
      "Epoch 5140, Loss: 0.003036745358258486, Grad Norm: 0.004524086136370897\n",
      "Epoch 5150, Loss: 0.0030195587314665318, Grad Norm: 0.004497377201914787\n",
      "Epoch 5160, Loss: 0.0030025076121091843, Grad Norm: 0.004470841493457556\n",
      "Epoch 5170, Loss: 0.002985546365380287, Grad Norm: 0.004444464109838009\n",
      "Epoch 5180, Loss: 0.002968735061585903, Grad Norm: 0.004418269265443087\n",
      "Epoch 5190, Loss: 0.0029519847594201565, Grad Norm: 0.0043922229669988155\n",
      "Epoch 5200, Loss: 0.0029353387653827667, Grad Norm: 0.004366342443972826\n",
      "Epoch 5210, Loss: 0.002918798476457596, Grad Norm: 0.0043406253680586815\n",
      "Epoch 5220, Loss: 0.0029023634269833565, Grad Norm: 0.004315112717449665\n",
      "Epoch 5230, Loss: 0.0028860033489763737, Grad Norm: 0.004289738368242979\n",
      "Epoch 5240, Loss: 0.0028697485104203224, Grad Norm: 0.004264509305357933\n",
      "Epoch 5250, Loss: 0.002853628946468234, Grad Norm: 0.004239462781697512\n",
      "Epoch 5260, Loss: 0.00283761415630579, Grad Norm: 0.004214568063616753\n",
      "Epoch 5270, Loss: 0.002821645000949502, Grad Norm: 0.004189822357147932\n",
      "Epoch 5280, Loss: 0.0028057959862053394, Grad Norm: 0.004165229387581348\n",
      "Epoch 5290, Loss: 0.0027900519780814648, Grad Norm: 0.004140820354223251\n",
      "Epoch 5300, Loss: 0.002774398075416684, Grad Norm: 0.004116538446396589\n",
      "Epoch 5310, Loss: 0.0027588193770498037, Grad Norm: 0.004092428367584944\n",
      "Epoch 5320, Loss: 0.0027433757204562426, Grad Norm: 0.004068462643772364\n",
      "Epoch 5330, Loss: 0.0027279923669993877, Grad Norm: 0.004044664092361927\n",
      "Epoch 5340, Loss: 0.002712684217840433, Grad Norm: 0.004021005239337683\n",
      "Epoch 5350, Loss: 0.0026974959764629602, Grad Norm: 0.003997471649199724\n",
      "Epoch 5360, Loss: 0.002682412974536419, Grad Norm: 0.0039741285145282745\n",
      "Epoch 5370, Loss: 0.002667405176907778, Grad Norm: 0.003950920887291431\n",
      "Epoch 5380, Loss: 0.0026524721179157495, Grad Norm: 0.003927850164473057\n",
      "Epoch 5390, Loss: 0.002637659665197134, Grad Norm: 0.003904927521944046\n",
      "Epoch 5400, Loss: 0.002622906817123294, Grad Norm: 0.003882158314809203\n",
      "Epoch 5410, Loss: 0.002608259441331029, Grad Norm: 0.0038595094811171293\n",
      "Epoch 5420, Loss: 0.002593686804175377, Grad Norm: 0.0038370280526578426\n",
      "Epoch 5430, Loss: 0.002579204738140106, Grad Norm: 0.0038146749138832092\n",
      "Epoch 5440, Loss: 0.0025648274458944798, Grad Norm: 0.0037924749776721\n",
      "Epoch 5450, Loss: 0.00255052512511611, Grad Norm: 0.003770410781726241\n",
      "Epoch 5460, Loss: 0.002536298241466284, Grad Norm: 0.0037484762724488974\n",
      "Epoch 5470, Loss: 0.002522191032767296, Grad Norm: 0.0037266879808157682\n",
      "Epoch 5480, Loss: 0.0025081292260438204, Grad Norm: 0.003705034963786602\n",
      "Epoch 5490, Loss: 0.0024941423907876015, Grad Norm: 0.0036835065111517906\n",
      "Epoch 5500, Loss: 0.0024802605621516705, Grad Norm: 0.0036621035542339087\n",
      "Epoch 5510, Loss: 0.002466453704982996, Grad Norm: 0.0036408561281859875\n",
      "Epoch 5520, Loss: 0.0024527371861040592, Grad Norm: 0.003619751427322626\n",
      "Epoch 5530, Loss: 0.002439080737531185, Grad Norm: 0.0035987566225230694\n",
      "Epoch 5540, Loss: 0.002425573766231537, Grad Norm: 0.003577913623303175\n",
      "Epoch 5550, Loss: 0.0024120972957462072, Grad Norm: 0.0035571735352277756\n",
      "Epoch 5560, Loss: 0.002398710697889328, Grad Norm: 0.0035365938674658537\n",
      "Epoch 5570, Loss: 0.002385384403169155, Grad Norm: 0.003516116412356496\n",
      "Epoch 5580, Loss: 0.002372133079916239, Grad Norm: 0.0034957730676978827\n",
      "Epoch 5590, Loss: 0.002358971629291773, Grad Norm: 0.0034755547530949116\n",
      "Epoch 5600, Loss: 0.002345915185287595, Grad Norm: 0.0034554607700556517\n",
      "Epoch 5610, Loss: 0.0023329188115894794, Grad Norm: 0.0034355055540800095\n",
      "Epoch 5620, Loss: 0.002319997176527977, Grad Norm: 0.003415652085095644\n",
      "Epoch 5630, Loss: 0.002307166112586856, Grad Norm: 0.003395954379811883\n",
      "Epoch 5640, Loss: 0.002294365083798766, Grad Norm: 0.003376364940777421\n",
      "Epoch 5650, Loss: 0.002281668595969677, Grad Norm: 0.0033568921498954296\n",
      "Epoch 5660, Loss: 0.002269047312438488, Grad Norm: 0.0033375283237546682\n",
      "Epoch 5670, Loss: 0.0022565010003745556, Grad Norm: 0.003318284172564745\n",
      "Epoch 5680, Loss: 0.0022440599277615547, Grad Norm: 0.0032991759944707155\n",
      "Epoch 5690, Loss: 0.0022316486574709415, Grad Norm: 0.003280174918472767\n",
      "Epoch 5700, Loss: 0.002219312358647585, Grad Norm: 0.003261298406869173\n",
      "Epoch 5710, Loss: 0.00220708129927516, Grad Norm: 0.0032425543759018183\n",
      "Epoch 5720, Loss: 0.002194864908233285, Grad Norm: 0.0032238943967968225\n",
      "Epoch 5730, Loss: 0.0021827537566423416, Grad Norm: 0.003205355955287814\n",
      "Epoch 5740, Loss: 0.002170732244849205, Grad Norm: 0.0031869392842054367\n",
      "Epoch 5750, Loss: 0.002158741233870387, Grad Norm: 0.0031686420552432537\n",
      "Epoch 5760, Loss: 0.002146884799003601, Grad Norm: 0.003150451695546508\n",
      "Epoch 5770, Loss: 0.0021350285969674587, Grad Norm: 0.0031323800794780254\n",
      "Epoch 5780, Loss: 0.0021232771687209606, Grad Norm: 0.0031144104432314634\n",
      "Epoch 5790, Loss: 0.0021115709096193314, Grad Norm: 0.0030965337064117193\n",
      "Epoch 5800, Loss: 0.00209996965713799, Grad Norm: 0.003078791778534651\n",
      "Epoch 5810, Loss: 0.002088413340970874, Grad Norm: 0.0030611606780439615\n",
      "Epoch 5820, Loss: 0.0020769317634403706, Grad Norm: 0.003043615724891424\n",
      "Epoch 5830, Loss: 0.002065480686724186, Grad Norm: 0.0030262055806815624\n",
      "Epoch 5840, Loss: 0.002054134150967002, Grad Norm: 0.0030088908970355988\n",
      "Epoch 5850, Loss: 0.0020428327843546867, Grad Norm: 0.0029916660860180855\n",
      "Epoch 5860, Loss: 0.002031650859862566, Grad Norm: 0.0029745553620159626\n",
      "Epoch 5870, Loss: 0.0020204996690154076, Grad Norm: 0.0029575503431260586\n",
      "Epoch 5880, Loss: 0.002009407849982381, Grad Norm: 0.0029406503308564425\n",
      "Epoch 5890, Loss: 0.001998361200094223, Grad Norm: 0.002923857420682907\n",
      "Epoch 5900, Loss: 0.0019874046556651592, Grad Norm: 0.0029071790631860495\n",
      "Epoch 5910, Loss: 0.0019765228498727083, Grad Norm: 0.0028905866201967\n",
      "Epoch 5920, Loss: 0.00196570111438632, Grad Norm: 0.002874089637771249\n",
      "Epoch 5930, Loss: 0.0019548945128917694, Grad Norm: 0.0028576990589499474\n",
      "Epoch 5940, Loss: 0.0019442223710939288, Grad Norm: 0.002841399749740958\n",
      "Epoch 5950, Loss: 0.0019335655961185694, Grad Norm: 0.002825200092047453\n",
      "Epoch 5960, Loss: 0.001922953873872757, Grad Norm: 0.0028091317508369684\n",
      "Epoch 5970, Loss: 0.0019124466925859451, Grad Norm: 0.0027931327931582928\n",
      "Epoch 5980, Loss: 0.0019019846804440022, Grad Norm: 0.0027772258035838604\n",
      "Epoch 5990, Loss: 0.0018915676046162844, Grad Norm: 0.0027614273130893707\n",
      "Epoch 6000, Loss: 0.001881240401417017, Grad Norm: 0.0027457282412797213\n",
      "Epoch 6010, Loss: 0.0018709581345319748, Grad Norm: 0.0027301041409373283\n",
      "Epoch 6020, Loss: 0.0018607209203764796, Grad Norm: 0.002714592032134533\n",
      "Epoch 6030, Loss: 0.0018506033811718225, Grad Norm: 0.0026991756167262793\n",
      "Epoch 6040, Loss: 0.0018404710572212934, Grad Norm: 0.0026838413905352354\n",
      "Epoch 6050, Loss: 0.0018304286058992147, Grad Norm: 0.0026686026249080896\n",
      "Epoch 6060, Loss: 0.001820445992052555, Grad Norm: 0.0026534502394497395\n",
      "Epoch 6070, Loss: 0.0018105085473507643, Grad Norm: 0.002638417761772871\n",
      "Epoch 6080, Loss: 0.0018006160389631987, Grad Norm: 0.0026234493125230074\n",
      "Epoch 6090, Loss: 0.0017908131703734398, Grad Norm: 0.0026085644494742155\n",
      "Epoch 6100, Loss: 0.001781040453352034, Grad Norm: 0.00259379087947309\n",
      "Epoch 6110, Loss: 0.001771357492543757, Grad Norm: 0.0025791083462536335\n",
      "Epoch 6120, Loss: 0.001761704683303833, Grad Norm: 0.0025645007845014334\n",
      "Epoch 6130, Loss: 0.0017520966939628124, Grad Norm: 0.002549976808950305\n",
      "Epoch 6140, Loss: 0.0017425634432584047, Grad Norm: 0.0025355422403663397\n",
      "Epoch 6150, Loss: 0.0017330902628600597, Grad Norm: 0.002521186601370573\n",
      "Epoch 6160, Loss: 0.001723647117614746, Grad Norm: 0.002506944350898266\n",
      "Epoch 6170, Loss: 0.0017142788274213672, Grad Norm: 0.002492752391844988\n",
      "Epoch 6180, Loss: 0.0017049703747034073, Grad Norm: 0.0024786903522908688\n",
      "Epoch 6190, Loss: 0.001695677055977285, Grad Norm: 0.0024646848905831575\n",
      "Epoch 6200, Loss: 0.001686488394625485, Grad Norm: 0.002450765110552311\n",
      "Epoch 6210, Loss: 0.0016773296520113945, Grad Norm: 0.0024369447492063046\n",
      "Epoch 6220, Loss: 0.001668216078542173, Grad Norm: 0.0024231793358922005\n",
      "Epoch 6230, Loss: 0.0016591474413871765, Grad Norm: 0.002409510314464569\n",
      "Epoch 6240, Loss: 0.0016501384088769555, Grad Norm: 0.0023959032259881496\n",
      "Epoch 6250, Loss: 0.0016412194818258286, Grad Norm: 0.0023824104573577642\n",
      "Epoch 6260, Loss: 0.001632285537198186, Grad Norm: 0.0023689833469688892\n",
      "Epoch 6270, Loss: 0.0016234711511060596, Grad Norm: 0.0023556493688374758\n",
      "Epoch 6280, Loss: 0.001614642096683383, Grad Norm: 0.0023423663806170225\n",
      "Epoch 6290, Loss: 0.001605902798473835, Grad Norm: 0.002329194452613592\n",
      "Epoch 6300, Loss: 0.001597163500264287, Grad Norm: 0.00231609377078712\n",
      "Epoch 6310, Loss: 0.001588543993420899, Grad Norm: 0.002303043147549033\n",
      "Epoch 6320, Loss: 0.0015799393877387047, Grad Norm: 0.002290091710165143\n",
      "Epoch 6330, Loss: 0.0015713947359472513, Grad Norm: 0.002277216874063015\n",
      "Epoch 6340, Loss: 0.0015628801193088293, Grad Norm: 0.0022644305136054754\n",
      "Epoch 6350, Loss: 0.0015543954214081168, Grad Norm: 0.0022516963072121143\n",
      "Epoch 6360, Loss: 0.001546015264466405, Grad Norm: 0.0022390394005924463\n",
      "Epoch 6370, Loss: 0.0015376503579318523, Grad Norm: 0.0022264623548835516\n",
      "Epoch 6380, Loss: 0.0015293601900339127, Grad Norm: 0.0022139823995530605\n",
      "Epoch 6390, Loss: 0.001521070022135973, Grad Norm: 0.0022015466820448637\n",
      "Epoch 6400, Loss: 0.0015128697268664837, Grad Norm: 0.0021891954820603132\n",
      "Epoch 6410, Loss: 0.0015046843327581882, Grad Norm: 0.002176922746002674\n",
      "Epoch 6420, Loss: 0.0014965439913794398, Grad Norm: 0.002164727309718728\n",
      "Epoch 6430, Loss: 0.0014884783886373043, Grad Norm: 0.002152593806385994\n",
      "Epoch 6440, Loss: 0.0014804576057940722, Grad Norm: 0.0021405203733593225\n",
      "Epoch 6450, Loss: 0.0014724519569426775, Grad Norm: 0.002128532389178872\n",
      "Epoch 6460, Loss: 0.001464520930312574, Grad Norm: 0.0021166226360946894\n",
      "Epoch 6470, Loss: 0.0014566051540896297, Grad Norm: 0.0021047729533165693\n",
      "Epoch 6480, Loss: 0.0014487490989267826, Grad Norm: 0.002092991955578327\n",
      "Epoch 6490, Loss: 0.0014409379800781608, Grad Norm: 0.0020812745206058025\n",
      "Epoch 6500, Loss: 0.001433186698704958, Grad Norm: 0.002069648588076234\n",
      "Epoch 6510, Loss: 0.0014254502020776272, Grad Norm: 0.0020580783020704985\n",
      "Epoch 6520, Loss: 0.0014177889097481966, Grad Norm: 0.0020465680863708258\n",
      "Epoch 6530, Loss: 0.0014101422857493162, Grad Norm: 0.0020351384300738573\n",
      "Epoch 6540, Loss: 0.0014025705168023705, Grad Norm: 0.0020237716380506754\n",
      "Epoch 6550, Loss: 0.0013949840795248747, Grad Norm: 0.0020124572329223156\n",
      "Epoch 6560, Loss: 0.0013874722644686699, Grad Norm: 0.0020012210588902235\n",
      "Epoch 6570, Loss: 0.001380035188049078, Grad Norm: 0.001990070566534996\n",
      "Epoch 6580, Loss: 0.00137261301279068, Grad Norm: 0.0019789596553891897\n",
      "Epoch 6590, Loss: 0.0013652360066771507, Grad Norm: 0.0019679199904203415\n",
      "Epoch 6600, Loss: 0.0013578737853094935, Grad Norm: 0.0019569701980799437\n",
      "Epoch 6610, Loss: 0.001350586418993771, Grad Norm: 0.0019460615003481507\n",
      "Epoch 6620, Loss: 0.001343343872576952, Grad Norm: 0.0019351912196725607\n",
      "Epoch 6630, Loss: 0.0013361163437366486, Grad Norm: 0.0019244346767663956\n",
      "Epoch 6640, Loss: 0.0013289486523717642, Grad Norm: 0.0019137175986543298\n",
      "Epoch 6650, Loss: 0.001321825897321105, Grad Norm: 0.0019030473195016384\n",
      "Epoch 6660, Loss: 0.0013147329445928335, Grad Norm: 0.0018924586474895477\n",
      "Epoch 6670, Loss: 0.0013076700270175934, Grad Norm: 0.0018819239921867847\n",
      "Epoch 6680, Loss: 0.0013006520457565784, Grad Norm: 0.0018714664038270712\n",
      "Epoch 6690, Loss: 0.001293678767979145, Grad Norm: 0.0018610487459227443\n",
      "Epoch 6700, Loss: 0.0012867057230323553, Grad Norm: 0.0018506953492760658\n",
      "Epoch 6710, Loss: 0.0012797923991456628, Grad Norm: 0.0018403944559395313\n",
      "Epoch 6720, Loss: 0.0012729837326332927, Grad Norm: 0.0018301750533282757\n",
      "Epoch 6730, Loss: 0.0012661600485444069, Grad Norm: 0.001820026314817369\n",
      "Epoch 6740, Loss: 0.0012593516148626804, Grad Norm: 0.0018099016742780805\n",
      "Epoch 6750, Loss: 0.0012526027858257294, Grad Norm: 0.0017998465336859226\n",
      "Epoch 6760, Loss: 0.0012459137942641973, Grad Norm: 0.001789874630048871\n",
      "Epoch 6770, Loss: 0.001239239820279181, Grad Norm: 0.0017799428896978498\n",
      "Epoch 6780, Loss: 0.0012325956486165524, Grad Norm: 0.0017700634198263288\n",
      "Epoch 6790, Loss: 0.0012259816285222769, Grad Norm: 0.0017602478619664907\n",
      "Epoch 6800, Loss: 0.0012194272130727768, Grad Norm: 0.0017504679271951318\n",
      "Epoch 6810, Loss: 0.001212902832776308, Grad Norm: 0.0017407925333827734\n",
      "Epoch 6820, Loss: 0.0012064232723787427, Grad Norm: 0.0017311403062194586\n",
      "Epoch 6830, Loss: 0.0011999885318800807, Grad Norm: 0.0017215615371242166\n",
      "Epoch 6840, Loss: 0.00119358382653445, Grad Norm: 0.0017120065167546272\n",
      "Epoch 6850, Loss: 0.0011871641036123037, Grad Norm: 0.0017025494016706944\n",
      "Epoch 6860, Loss: 0.001180834136903286, Grad Norm: 0.001693120808340609\n",
      "Epoch 6870, Loss: 0.0011745488736778498, Grad Norm: 0.0016837381990626454\n",
      "Epoch 6880, Loss: 0.0011682788608595729, Grad Norm: 0.0016744344029575586\n",
      "Epoch 6890, Loss: 0.0011620235163718462, Grad Norm: 0.001665174844674766\n",
      "Epoch 6900, Loss: 0.0011558132246136665, Grad Norm: 0.0016559588257223368\n",
      "Epoch 6910, Loss: 0.0011496477527543902, Grad Norm: 0.0016467912355437875\n",
      "Epoch 6920, Loss: 0.0011435271007940173, Grad Norm: 0.001637710607610643\n",
      "Epoch 6930, Loss: 0.0011374065652489662, Grad Norm: 0.0016286439495161176\n",
      "Epoch 6940, Loss: 0.0011313308496028185, Grad Norm: 0.001619650167413056\n",
      "Epoch 6950, Loss: 0.0011253147386014462, Grad Norm: 0.0016107134288176894\n",
      "Epoch 6960, Loss: 0.0011193137615919113, Grad Norm: 0.0016018212772905827\n",
      "Epoch 6970, Loss: 0.0011133278021588922, Grad Norm: 0.0015929671935737133\n",
      "Epoch 6980, Loss: 0.001107416464947164, Grad Norm: 0.0015841760905459523\n",
      "Epoch 6990, Loss: 0.0011015201453119516, Grad Norm: 0.001575449132360518\n",
      "Epoch 7000, Loss: 0.0010956089245155454, Grad Norm: 0.0015667661791667342\n",
      "Epoch 7010, Loss: 0.0010898171458393335, Grad Norm: 0.0015581162879243493\n",
      "Epoch 7020, Loss: 0.0010839956812560558, Grad Norm: 0.0015495384577661753\n",
      "Epoch 7030, Loss: 0.0010782189201563597, Grad Norm: 0.0015410050982609391\n",
      "Epoch 7040, Loss: 0.0010725019965320826, Grad Norm: 0.001532481168396771\n",
      "Epoch 7050, Loss: 0.0010667850729078054, Grad Norm: 0.0015240522334352136\n",
      "Epoch 7060, Loss: 0.0010610980680212379, Grad Norm: 0.001515681971795857\n",
      "Epoch 7070, Loss: 0.0010554707841947675, Grad Norm: 0.0015073159011080861\n",
      "Epoch 7080, Loss: 0.001049843616783619, Grad Norm: 0.0014990168856456876\n",
      "Epoch 7090, Loss: 0.0010442612692713737, Grad Norm: 0.0014907801523804665\n",
      "Epoch 7100, Loss: 0.0010386938229203224, Grad Norm: 0.0014825662365183234\n",
      "Epoch 7110, Loss: 0.0010331862140446901, Grad Norm: 0.001474417163990438\n",
      "Epoch 7120, Loss: 0.0010277086403220892, Grad Norm: 0.001466317567974329\n",
      "Epoch 7130, Loss: 0.0010222308337688446, Grad Norm: 0.0014582453295588493\n",
      "Epoch 7140, Loss: 0.0010167979635298252, Grad Norm: 0.0014502230333164334\n",
      "Epoch 7150, Loss: 0.0010113799944519997, Grad Norm: 0.0014422631356865168\n",
      "Epoch 7160, Loss: 0.0010060218628495932, Grad Norm: 0.0014343226794153452\n",
      "Epoch 7170, Loss: 0.0010007086675614119, Grad Norm: 0.0014264637138694525\n",
      "Epoch 7180, Loss: 0.000995380338281393, Grad Norm: 0.001418616622686386\n",
      "Epoch 7190, Loss: 0.0009900671429932117, Grad Norm: 0.0014108166797086596\n",
      "Epoch 7200, Loss: 0.0009848285699263215, Grad Norm: 0.0014030801830813289\n",
      "Epoch 7210, Loss: 0.0009796347003430128, Grad Norm: 0.00139539351221174\n",
      "Epoch 7220, Loss: 0.0009743962436914444, Grad Norm: 0.0013877281453460455\n",
      "Epoch 7230, Loss: 0.0009692623279988766, Grad Norm: 0.0013801169116050005\n",
      "Epoch 7240, Loss: 0.0009640984935685992, Grad Norm: 0.0013725514290854335\n",
      "Epoch 7250, Loss: 0.0009589646360836923, Grad Norm: 0.0013650351902469993\n",
      "Epoch 7260, Loss: 0.0009538904996588826, Grad Norm: 0.0013575430493801832\n",
      "Epoch 7270, Loss: 0.0009488461655564606, Grad Norm: 0.0013500992208719254\n",
      "Epoch 7280, Loss: 0.00094383186660707, Grad Norm: 0.0013426983496174216\n",
      "Epoch 7290, Loss: 0.0009388175094500184, Grad Norm: 0.0013353541726246476\n",
      "Epoch 7300, Loss: 0.0009338180534541607, Grad Norm: 0.0013280384009703994\n",
      "Epoch 7310, Loss: 0.0009288933360949159, Grad Norm: 0.001320753595791757\n",
      "Epoch 7320, Loss: 0.0009239985374733806, Grad Norm: 0.0013135176850482821\n",
      "Epoch 7330, Loss: 0.0009190738783217967, Grad Norm: 0.0013063537189736962\n",
      "Epoch 7340, Loss: 0.0009141939808614552, Grad Norm: 0.0012991955736652017\n",
      "Epoch 7350, Loss: 0.0009093738626688719, Grad Norm: 0.0012921029701828957\n",
      "Epoch 7360, Loss: 0.0009045836050063372, Grad Norm: 0.0012850476196035743\n",
      "Epoch 7370, Loss: 0.0008997934637591243, Grad Norm: 0.0012780193937942386\n",
      "Epoch 7380, Loss: 0.0008950331248342991, Grad Norm: 0.0012710362207144499\n",
      "Epoch 7390, Loss: 0.0008902877452783287, Grad Norm: 0.001264075399376452\n",
      "Epoch 7400, Loss: 0.0008855722844600677, Grad Norm: 0.001257161027751863\n",
      "Epoch 7410, Loss: 0.0008809015853330493, Grad Norm: 0.0012502975296229124\n",
      "Epoch 7420, Loss: 0.0008762309444136918, Grad Norm: 0.0012434599921107292\n",
      "Epoch 7430, Loss: 0.0008716051233932376, Grad Norm: 0.0012366807786747813\n",
      "Epoch 7440, Loss: 0.0008670240640640259, Grad Norm: 0.0012299178633838892\n",
      "Epoch 7450, Loss: 0.0008624430047348142, Grad Norm: 0.0012232012813910842\n",
      "Epoch 7460, Loss: 0.0008579067653045058, Grad Norm: 0.0012165363878011703\n",
      "Epoch 7470, Loss: 0.0008533557411283255, Grad Norm: 0.0012099096784368157\n",
      "Epoch 7480, Loss: 0.0008488344610668719, Grad Norm: 0.0012033001985400915\n",
      "Epoch 7490, Loss: 0.0008443879196420312, Grad Norm: 0.0011967408936470747\n",
      "Epoch 7500, Loss: 0.0008399115176871419, Grad Norm: 0.0011902078986167908\n",
      "Epoch 7510, Loss: 0.0008354798192158341, Grad Norm: 0.001183749409392476\n",
      "Epoch 7520, Loss: 0.0008310632547363639, Grad Norm: 0.0011772768339142203\n",
      "Epoch 7530, Loss: 0.0008266763761639595, Grad Norm: 0.0011708536185324192\n",
      "Epoch 7540, Loss: 0.0008222896140068769, Grad Norm: 0.0011644832557067275\n",
      "Epoch 7550, Loss: 0.0008179775904864073, Grad Norm: 0.001158134313300252\n",
      "Epoch 7560, Loss: 0.0008136356482282281, Grad Norm: 0.0011518184328451753\n",
      "Epoch 7570, Loss: 0.0008093832293525338, Grad Norm: 0.0011455611092969775\n",
      "Epoch 7580, Loss: 0.0008051160257309675, Grad Norm: 0.0011392943561077118\n",
      "Epoch 7590, Loss: 0.0008008487056940794, Grad Norm: 0.0011331181740388274\n",
      "Epoch 7600, Loss: 0.0007966561242938042, Grad Norm: 0.0011269475799053907\n",
      "Epoch 7610, Loss: 0.0007924336823634803, Grad Norm: 0.0011208228534087539\n",
      "Epoch 7620, Loss: 0.0007882709614932537, Grad Norm: 0.0011147316545248032\n",
      "Epoch 7630, Loss: 0.0007841381011530757, Grad Norm: 0.0011086793383583426\n",
      "Epoch 7640, Loss: 0.0007799903396517038, Grad Norm: 0.0011026404099538922\n",
      "Epoch 7650, Loss: 0.0007758874562568963, Grad Norm: 0.0010966494446620345\n",
      "Epoch 7660, Loss: 0.0007717993576079607, Grad Norm: 0.0010906816460192204\n",
      "Epoch 7670, Loss: 0.0007677412359043956, Grad Norm: 0.0010847426019608974\n",
      "Epoch 7680, Loss: 0.0007636831142008305, Grad Norm: 0.0010788493091240525\n",
      "Epoch 7690, Loss: 0.0007596698123961687, Grad Norm: 0.0010729975765570998\n",
      "Epoch 7700, Loss: 0.0007556714117527008, Grad Norm: 0.0010671542258933187\n",
      "Epoch 7710, Loss: 0.0007516730111092329, Grad Norm: 0.0010613622143864632\n",
      "Epoch 7720, Loss: 0.0007477493491023779, Grad Norm: 0.0010556166525930166\n",
      "Epoch 7730, Loss: 0.0007438106695190072, Grad Norm: 0.0010498851770535111\n",
      "Epoch 7740, Loss: 0.0007398720481432974, Grad Norm: 0.0010441998019814491\n",
      "Epoch 7750, Loss: 0.0007360080489888787, Grad Norm: 0.0010385364294052124\n",
      "Epoch 7760, Loss: 0.0007321441080421209, Grad Norm: 0.0010328995995223522\n",
      "Epoch 7770, Loss: 0.0007282651495188475, Grad Norm: 0.0010272642830386758\n",
      "Epoch 7780, Loss: 0.0007244609296321869, Grad Norm: 0.0010217196540907025\n",
      "Epoch 7790, Loss: 0.0007206418085843325, Grad Norm: 0.0010161789832636714\n",
      "Epoch 7800, Loss: 0.0007168823503889143, Grad Norm: 0.0010106564732268453\n",
      "Epoch 7810, Loss: 0.0007130781887099147, Grad Norm: 0.00100518309045583\n",
      "Epoch 7820, Loss: 0.000709363492205739, Grad Norm: 0.0009997461456805468\n",
      "Epoch 7830, Loss: 0.0007056340109556913, Grad Norm: 0.0009942997712641954\n",
      "Epoch 7840, Loss: 0.0007019194308668375, Grad Norm: 0.0009889103239402175\n",
      "Epoch 7850, Loss: 0.0006982495542615652, Grad Norm: 0.0009835775708779693\n",
      "Epoch 7860, Loss: 0.0006945797358639538, Grad Norm: 0.0009782336419448256\n",
      "Epoch 7870, Loss: 0.0006909248186275363, Grad Norm: 0.0009729443700052798\n",
      "Epoch 7880, Loss: 0.0006872848607599735, Grad Norm: 0.0009677030611783266\n",
      "Epoch 7890, Loss: 0.0006836748216301203, Grad Norm: 0.0009624581434763968\n",
      "Epoch 7900, Loss: 0.0006801094859838486, Grad Norm: 0.0009572452981956303\n",
      "Epoch 7910, Loss: 0.0006765442667528987, Grad Norm: 0.00095206662081182\n",
      "Epoch 7920, Loss: 0.000672978931106627, Grad Norm: 0.0009469167562201619\n",
      "Epoch 7930, Loss: 0.0006694734329357743, Grad Norm: 0.0009418019908480346\n",
      "Epoch 7940, Loss: 0.000665952917188406, Grad Norm: 0.0009367154561914504\n",
      "Epoch 7950, Loss: 0.0006624474190175533, Grad Norm: 0.0009316452778875828\n",
      "Epoch 7960, Loss: 0.0006589717231690884, Grad Norm: 0.000926600827369839\n",
      "Epoch 7970, Loss: 0.0006555556901730597, Grad Norm: 0.0009215925820171833\n",
      "Epoch 7980, Loss: 0.0006520949536934495, Grad Norm: 0.0009166092495433986\n",
      "Epoch 7990, Loss: 0.0006486640777438879, Grad Norm: 0.0009116459405049682\n",
      "Epoch 8000, Loss: 0.0006452630623243749, Grad Norm: 0.0009067324572242796\n",
      "Epoch 8010, Loss: 0.0006418918492272496, Grad Norm: 0.0009018401033245027\n",
      "Epoch 8020, Loss: 0.0006385355954989791, Grad Norm: 0.0008969730697572231\n",
      "Epoch 8030, Loss: 0.0006352092605084181, Grad Norm: 0.0008921416010707617\n",
      "Epoch 8040, Loss: 0.0006318530649878085, Grad Norm: 0.0008873086189851165\n",
      "Epoch 8050, Loss: 0.0006285415729507804, Grad Norm: 0.000882520223967731\n",
      "Epoch 8060, Loss: 0.0006252749590203166, Grad Norm: 0.0008777651819400489\n",
      "Epoch 8070, Loss: 0.0006219784263521433, Grad Norm: 0.0008730150293558836\n",
      "Epoch 8080, Loss: 0.0006187416729517281, Grad Norm: 0.0008683212217874825\n",
      "Epoch 8090, Loss: 0.0006154900183901191, Grad Norm: 0.0008636338170617819\n",
      "Epoch 8100, Loss: 0.0006122979684732854, Grad Norm: 0.0008589797653257847\n",
      "Epoch 8110, Loss: 0.0006090612150728703, Grad Norm: 0.0008543343283236027\n",
      "Epoch 8120, Loss: 0.0006058841245248914, Grad Norm: 0.000849741161800921\n",
      "Epoch 8130, Loss: 0.0006027219933457673, Grad Norm: 0.0008451460162177682\n",
      "Epoch 8140, Loss: 0.000599574763327837, Grad Norm: 0.0008405942935496569\n",
      "Epoch 8150, Loss: 0.0005964424926787615, Grad Norm: 0.000836067134514451\n",
      "Epoch 8160, Loss: 0.0005932804197072983, Grad Norm: 0.0008315640734508634\n",
      "Epoch 8170, Loss: 0.0005902077537029982, Grad Norm: 0.0008270938415080309\n",
      "Epoch 8180, Loss: 0.000587120302952826, Grad Norm: 0.0008226376958191395\n",
      "Epoch 8190, Loss: 0.0005840626545250416, Grad Norm: 0.0008182067540474236\n",
      "Epoch 8200, Loss: 0.0005810050060972571, Grad Norm: 0.0008138103294186294\n",
      "Epoch 8210, Loss: 0.0005779474158771336, Grad Norm: 0.000809436256531626\n",
      "Epoch 8220, Loss: 0.0005749345291405916, Grad Norm: 0.000805080693680793\n",
      "Epoch 8230, Loss: 0.0005719067994505167, Grad Norm: 0.0008007372380234301\n",
      "Epoch 8240, Loss: 0.0005689685931429267, Grad Norm: 0.0007964104879647493\n",
      "Epoch 8250, Loss: 0.0005660154274664819, Grad Norm: 0.000792154052760452\n",
      "Epoch 8260, Loss: 0.0005630175583064556, Grad Norm: 0.0007878744509071112\n",
      "Epoch 8270, Loss: 0.0005600942531600595, Grad Norm: 0.0007836288423277438\n",
      "Epoch 8280, Loss: 0.0005571560468524694, Grad Norm: 0.0007794319535605609\n",
      "Epoch 8290, Loss: 0.0005542327417060733, Grad Norm: 0.0007752143428660929\n",
      "Epoch 8300, Loss: 0.0005513542564585805, Grad Norm: 0.0007710634963586926\n",
      "Epoch 8310, Loss: 0.0005485055735334754, Grad Norm: 0.0007669450715184212\n",
      "Epoch 8320, Loss: 0.0005456121871247888, Grad Norm: 0.0007628142484463751\n",
      "Epoch 8330, Loss: 0.0005427636206150055, Grad Norm: 0.0007586965220980346\n",
      "Epoch 8340, Loss: 0.0005399298388510942, Grad Norm: 0.000754631357267499\n",
      "Epoch 8350, Loss: 0.0005371110746636987, Grad Norm: 0.0007505785906687379\n",
      "Epoch 8360, Loss: 0.0005343221127986908, Grad Norm: 0.0007465400849469006\n",
      "Epoch 8370, Loss: 0.0005315481103025377, Grad Norm: 0.0007425539661198854\n",
      "Epoch 8380, Loss: 0.0005287592066451907, Grad Norm: 0.0007385615026578307\n",
      "Epoch 8390, Loss: 0.0005260150064714253, Grad Norm: 0.0007345836493186653\n",
      "Epoch 8400, Loss: 0.0005232559633441269, Grad Norm: 0.0007306532352231443\n",
      "Epoch 8410, Loss: 0.0005205267225392163, Grad Norm: 0.0007267182227224112\n",
      "Epoch 8420, Loss: 0.0005178124411031604, Grad Norm: 0.0007228159811347723\n",
      "Epoch 8430, Loss: 0.0005150981014594436, Grad Norm: 0.0007189346943050623\n",
      "Epoch 8440, Loss: 0.000512443482875824, Grad Norm: 0.0007150870515033603\n",
      "Epoch 8450, Loss: 0.0005097739631310105, Grad Norm: 0.0007112466846592724\n",
      "Epoch 8460, Loss: 0.0005071193445473909, Grad Norm: 0.0007074408349581063\n",
      "Epoch 8470, Loss: 0.0005044498248025775, Grad Norm: 0.0007036304450593889\n",
      "Epoch 8480, Loss: 0.0005018400261178613, Grad Norm: 0.0006998680764809251\n",
      "Epoch 8490, Loss: 0.0004992301692254841, Grad Norm: 0.0006961201434023678\n",
      "Epoch 8500, Loss: 0.0004966203705407679, Grad Norm: 0.0006924025947228074\n",
      "Epoch 8510, Loss: 0.0004940553335472941, Grad Norm: 0.0006886988994665444\n",
      "Epoch 8520, Loss: 0.0004914604360237718, Grad Norm: 0.0006849875790067017\n",
      "Epoch 8530, Loss: 0.0004889102419838309, Grad Norm: 0.0006813231739215553\n",
      "Epoch 8540, Loss: 0.00048637506552040577, Grad Norm: 0.0006776746595278382\n",
      "Epoch 8550, Loss: 0.0004838398308493197, Grad Norm: 0.0006740355165675282\n",
      "Epoch 8560, Loss: 0.00048131958465091884, Grad Norm: 0.0006704294355586171\n",
      "Epoch 8570, Loss: 0.0004787694779224694, Grad Norm: 0.0006668444839306176\n",
      "Epoch 8580, Loss: 0.0004763386968988925, Grad Norm: 0.0006632701843045652\n",
      "Epoch 8590, Loss: 0.0004738632123917341, Grad Norm: 0.0006597194005735219\n",
      "Epoch 8600, Loss: 0.0004713727976195514, Grad Norm: 0.0006561786867678165\n",
      "Epoch 8610, Loss: 0.0004689121851697564, Grad Norm: 0.0006526766228489578\n",
      "Epoch 8620, Loss: 0.00046648143325001, Grad Norm: 0.0006491857348009944\n",
      "Epoch 8630, Loss: 0.00046405073953792453, Grad Norm: 0.0006457061972469091\n",
      "Epoch 8640, Loss: 0.0004616498190443963, Grad Norm: 0.0006422575679607689\n",
      "Epoch 8650, Loss: 0.0004592489276546985, Grad Norm: 0.000638818833976984\n",
      "Epoch 8660, Loss: 0.00045687786769121885, Grad Norm: 0.0006354097276926041\n",
      "Epoch 8670, Loss: 0.00045450683683156967, Grad Norm: 0.0006320317625068128\n",
      "Epoch 8680, Loss: 0.00045212084660306573, Grad Norm: 0.0006286466377787292\n",
      "Epoch 8690, Loss: 0.000449749844847247, Grad Norm: 0.0006252909661270678\n",
      "Epoch 8700, Loss: 0.0004474235465750098, Grad Norm: 0.0006219465867616236\n",
      "Epoch 8710, Loss: 0.00044508237624540925, Grad Norm: 0.0006186116370372474\n",
      "Epoch 8720, Loss: 0.000442800868768245, Grad Norm: 0.0006152914720587432\n",
      "Epoch 8730, Loss: 0.0004404745704960078, Grad Norm: 0.000612017756793648\n",
      "Epoch 8740, Loss: 0.00043820799328386784, Grad Norm: 0.0006087461370043457\n",
      "Epoch 8750, Loss: 0.0004359115846455097, Grad Norm: 0.0006055047269910574\n",
      "Epoch 8760, Loss: 0.00043365987949073315, Grad Norm: 0.0006022633169777691\n",
      "Epoch 8770, Loss: 0.00043143806396983564, Grad Norm: 0.0005990398349240422\n",
      "Epoch 8780, Loss: 0.0004291863879188895, Grad Norm: 0.0005958651890978217\n",
      "Epoch 8790, Loss: 0.00042693474097177386, Grad Norm: 0.0005926614394411445\n",
      "Epoch 8800, Loss: 0.00042471292545087636, Grad Norm: 0.0005895065842196345\n",
      "Epoch 8810, Loss: 0.00042249110992997885, Grad Norm: 0.000586360867600888\n",
      "Epoch 8820, Loss: 0.0004203289281576872, Grad Norm: 0.0005832260358147323\n",
      "Epoch 8830, Loss: 0.00041813694406300783, Grad Norm: 0.0005801210645586252\n",
      "Epoch 8840, Loss: 0.0004159748204983771, Grad Norm: 0.0005770162679255009\n",
      "Epoch 8850, Loss: 0.00041381269693374634, Grad Norm: 0.0005739587941206992\n",
      "Epoch 8860, Loss: 0.0004116803756915033, Grad Norm: 0.0005709037650376558\n",
      "Epoch 8870, Loss: 0.0004095480835530907, Grad Norm: 0.0005678569432348013\n",
      "Epoch 8880, Loss: 0.00040741576231084764, Grad Norm: 0.0005648226360790431\n",
      "Epoch 8890, Loss: 0.00040531333070248365, Grad Norm: 0.0005617964779958129\n",
      "Epoch 8900, Loss: 0.00040318103856407106, Grad Norm: 0.0005587994237430394\n",
      "Epoch 8910, Loss: 0.00040110840927809477, Grad Norm: 0.0005558325792662799\n",
      "Epoch 8920, Loss: 0.00039903580909594893, Grad Norm: 0.0005528363981284201\n",
      "Epoch 8930, Loss: 0.0003969482786487788, Grad Norm: 0.000549890217371285\n",
      "Epoch 8940, Loss: 0.0003949055098928511, Grad Norm: 0.0005469765164889395\n",
      "Epoch 8950, Loss: 0.000392862712033093, Grad Norm: 0.0005440879613161087\n",
      "Epoch 8960, Loss: 0.0003908348735421896, Grad Norm: 0.0005411923048086464\n",
      "Epoch 8970, Loss: 0.00038879213389009237, Grad Norm: 0.0005383053794503212\n",
      "Epoch 8980, Loss: 0.0003867642371915281, Grad Norm: 0.0005354274762794375\n",
      "Epoch 8990, Loss: 0.00038476623012684286, Grad Norm: 0.0005325792590156198\n",
      "Epoch 9000, Loss: 0.0003827533219009638, Grad Norm: 0.0005297407042235136\n",
      "Epoch 9010, Loss: 0.0003807702159974724, Grad Norm: 0.0005269092507660389\n",
      "Epoch 9020, Loss: 0.0003788318717852235, Grad Norm: 0.0005241272156126797\n",
      "Epoch 9030, Loss: 0.0003768338938243687, Grad Norm: 0.0005213171825744212\n",
      "Epoch 9040, Loss: 0.00037489552050828934, Grad Norm: 0.0005185541231185198\n",
      "Epoch 9050, Loss: 0.00037294227513484657, Grad Norm: 0.0005157992127351463\n",
      "Epoch 9060, Loss: 0.00037100393092259765, Grad Norm: 0.0005130343488417566\n",
      "Epoch 9070, Loss: 0.00036908051697537303, Grad Norm: 0.00051032257033512\n",
      "Epoch 9080, Loss: 0.00036720186471939087, Grad Norm: 0.000507620454300195\n",
      "Epoch 9090, Loss: 0.00036530825309455395, Grad Norm: 0.0005049031460657716\n",
      "Epoch 9100, Loss: 0.0003633997985161841, Grad Norm: 0.0005022353143431246\n",
      "Epoch 9110, Loss: 0.00036150618689134717, Grad Norm: 0.0004995574126951396\n",
      "Epoch 9120, Loss: 0.00035964243579655886, Grad Norm: 0.0004968869034200907\n",
      "Epoch 9130, Loss: 0.00035777868470177054, Grad Norm: 0.0004942609230056405\n",
      "Epoch 9140, Loss: 0.00035594479413703084, Grad Norm: 0.0004916249308735132\n",
      "Epoch 9150, Loss: 0.000354081072146073, Grad Norm: 0.0004890199052169919\n",
      "Epoch 9160, Loss: 0.0003522620245348662, Grad Norm: 0.0004864232614636421\n",
      "Epoch 9170, Loss: 0.00035042816307395697, Grad Norm: 0.00048383319517597556\n",
      "Epoch 9180, Loss: 0.00034862407483160496, Grad Norm: 0.0004812512779608369\n",
      "Epoch 9190, Loss: 0.00034681998658925295, Grad Norm: 0.0004786953504662961\n",
      "Epoch 9200, Loss: 0.0003450308577157557, Grad Norm: 0.00047614972572773695\n",
      "Epoch 9210, Loss: 0.00034324167063459754, Grad Norm: 0.00047363314661197364\n",
      "Epoch 9220, Loss: 0.00034146744292229414, Grad Norm: 0.00047112430911511183\n",
      "Epoch 9230, Loss: 0.0003396932443138212, Grad Norm: 0.00046862175804562867\n",
      "Epoch 9240, Loss: 0.00033793391776271164, Grad Norm: 0.00046612758887931705\n",
      "Epoch 9250, Loss: 0.00033620442263782024, Grad Norm: 0.00046365841990336776\n",
      "Epoch 9260, Loss: 0.00033444512519054115, Grad Norm: 0.0004611813055817038\n",
      "Epoch 9270, Loss: 0.0003327007289044559, Grad Norm: 0.00045875340583734214\n",
      "Epoch 9280, Loss: 0.0003309861640445888, Grad Norm: 0.00045631654211319983\n",
      "Epoch 9290, Loss: 0.00032931630266830325, Grad Norm: 0.00045388596481643617\n",
      "Epoch 9300, Loss: 0.00032760173780843616, Grad Norm: 0.00045146181946620345\n",
      "Epoch 9310, Loss: 0.0003258872020523995, Grad Norm: 0.00044908153358846903\n",
      "Epoch 9320, Loss: 0.0003241875674575567, Grad Norm: 0.00044669388444162905\n",
      "Epoch 9330, Loss: 0.00032253266545012593, Grad Norm: 0.00044433321454562247\n",
      "Epoch 9340, Loss: 0.0003208777343388647, Grad Norm: 0.0004419791221152991\n",
      "Epoch 9350, Loss: 0.00031920793117024004, Grad Norm: 0.0004396321310196072\n",
      "Epoch 9360, Loss: 0.0003175530582666397, Grad Norm: 0.00043729174649342895\n",
      "Epoch 9370, Loss: 0.0003159279585815966, Grad Norm: 0.0004349859955254942\n",
      "Epoch 9380, Loss: 0.0003142880159430206, Grad Norm: 0.0004326910129748285\n",
      "Epoch 9390, Loss: 0.00031269274768419564, Grad Norm: 0.0004303760943002999\n",
      "Epoch 9400, Loss: 0.000311067677102983, Grad Norm: 0.0004281055589672178\n",
      "Epoch 9410, Loss: 0.00030942773446440697, Grad Norm: 0.00042584113543853164\n",
      "Epoch 9420, Loss: 0.0003078324953094125, Grad Norm: 0.0004235835513100028\n",
      "Epoch 9430, Loss: 0.00030625215731561184, Grad Norm: 0.00042133175884373486\n",
      "Epoch 9440, Loss: 0.00030468677869066596, Grad Norm: 0.00041909018182195723\n",
      "Epoch 9450, Loss: 0.00030312134185805917, Grad Norm: 0.00041688064811751246\n",
      "Epoch 9460, Loss: 0.00030152613180689514, Grad Norm: 0.00041467632399871945\n",
      "Epoch 9470, Loss: 0.00029997562523931265, Grad Norm: 0.00041247918852604926\n",
      "Epoch 9480, Loss: 0.00029844004893675447, Grad Norm: 0.00041028749546967447\n",
      "Epoch 9490, Loss: 0.00029688957147300243, Grad Norm: 0.00040810159407556057\n",
      "Epoch 9500, Loss: 0.0002953540242742747, Grad Norm: 0.0004059431084897369\n",
      "Epoch 9510, Loss: 0.00029383334913291037, Grad Norm: 0.00040379349957220256\n",
      "Epoch 9520, Loss: 0.00029232760425657034, Grad Norm: 0.00040164965321309865\n",
      "Epoch 9530, Loss: 0.0002908069873228669, Grad Norm: 0.00039954972453415394\n",
      "Epoch 9540, Loss: 0.00028927141102030873, Grad Norm: 0.00039741824730299413\n",
      "Epoch 9550, Loss: 0.0002878103987313807, Grad Norm: 0.00039533074595965445\n",
      "Epoch 9560, Loss: 0.000286319584120065, Grad Norm: 0.0003932130930479616\n",
      "Epoch 9570, Loss: 0.00028485857183113694, Grad Norm: 0.0003911610983777791\n",
      "Epoch 9580, Loss: 0.0002833975595422089, Grad Norm: 0.00038909600698389113\n",
      "Epoch 9590, Loss: 0.0002819366054609418, Grad Norm: 0.0003870369109790772\n",
      "Epoch 9600, Loss: 0.00028047559317201376, Grad Norm: 0.00038498424692079425\n",
      "Epoch 9610, Loss: 0.00027904444141313434, Grad Norm: 0.00038293717079795897\n",
      "Epoch 9620, Loss: 0.0002775834291242063, Grad Norm: 0.00038091567694209516\n",
      "Epoch 9630, Loss: 0.00027616717852652073, Grad Norm: 0.0003788890899159014\n",
      "Epoch 9640, Loss: 0.0002747360267676413, Grad Norm: 0.0003769050817936659\n",
      "Epoch 9650, Loss: 0.0002733495784923434, Grad Norm: 0.0003749079187400639\n",
      "Epoch 9660, Loss: 0.0002718886244110763, Grad Norm: 0.0003729172458406538\n",
      "Epoch 9670, Loss: 0.00027050217613577843, Grad Norm: 0.0003709502052515745\n",
      "Epoch 9680, Loss: 0.00026910085580311716, Grad Norm: 0.0003689896548166871\n",
      "Epoch 9690, Loss: 0.0002677144075278193, Grad Norm: 0.0003670377191156149\n",
      "Epoch 9700, Loss: 0.000266313087195158, Grad Norm: 0.00036509166238829494\n",
      "Epoch 9710, Loss: 0.00026498630177229643, Grad Norm: 0.00036316984915174544\n",
      "Epoch 9720, Loss: 0.0002635850105434656, Grad Norm: 0.0003612528962548822\n",
      "Epoch 9730, Loss: 0.0002622283936943859, Grad Norm: 0.0003593422006815672\n",
      "Epoch 9740, Loss: 0.00026087177684530616, Grad Norm: 0.0003574363945517689\n",
      "Epoch 9750, Loss: 0.00025954502052627504, Grad Norm: 0.0003555566072463989\n",
      "Epoch 9760, Loss: 0.00025818843278102577, Grad Norm: 0.0003536668955348432\n",
      "Epoch 9770, Loss: 0.00025686167646199465, Grad Norm: 0.0003518013108987361\n",
      "Epoch 9780, Loss: 0.00025553489103913307, Grad Norm: 0.00034994000452570617\n",
      "Epoch 9790, Loss: 0.0002542230358812958, Grad Norm: 0.0003480848390609026\n",
      "Epoch 9800, Loss: 0.00025291118072345853, Grad Norm: 0.00034625292755663395\n",
      "Epoch 9810, Loss: 0.00025161425583064556, Grad Norm: 0.00034443099866621196\n",
      "Epoch 9820, Loss: 0.00025033223209902644, Grad Norm: 0.0003426168404985219\n",
      "Epoch 9830, Loss: 0.00024906510952860117, Grad Norm: 0.0003408275661058724\n",
      "Epoch 9840, Loss: 0.0002477532543707639, Grad Norm: 0.0003390063648112118\n",
      "Epoch 9850, Loss: 0.0002464861609041691, Grad Norm: 0.0003372269857209176\n",
      "Epoch 9860, Loss: 0.00024523393949493766, Grad Norm: 0.00033545290352776647\n",
      "Epoch 9870, Loss: 0.00024396683147642761, Grad Norm: 0.00033368467120453715\n",
      "Epoch 9880, Loss: 0.00024271462461911142, Grad Norm: 0.00033192167757079005\n",
      "Epoch 9890, Loss: 0.00024144751660060138, Grad Norm: 0.00033016741508617997\n",
      "Epoch 9900, Loss: 0.0002401953242952004, Grad Norm: 0.00032843684311956167\n",
      "Epoch 9910, Loss: 0.00023897294886410236, Grad Norm: 0.0003266922431066632\n",
      "Epoch 9920, Loss: 0.00023773565771989524, Grad Norm: 0.0003249709843657911\n",
      "Epoch 9930, Loss: 0.0002365132822887972, Grad Norm: 0.0003232546732760966\n",
      "Epoch 9940, Loss: 0.00023529090685769916, Grad Norm: 0.000321561616146937\n",
      "Epoch 9950, Loss: 0.0002340834471397102, Grad Norm: 0.0003198562772013247\n",
      "Epoch 9960, Loss: 0.00023287598742172122, Grad Norm: 0.0003181583306286484\n",
      "Epoch 9970, Loss: 0.0002316834288649261, Grad Norm: 0.0003164835798088461\n",
      "Epoch 9980, Loss: 0.0002304908848600462, Grad Norm: 0.00031481386395171285\n",
      "Epoch 9990, Loss: 0.00022929834085516632, Grad Norm: 0.0003131504054181278\n"
     ]
    }
   ],
   "source": [
    "epochs = 10_000\n",
    "\n",
    "for epoch in trange(epochs):\n",
    "    loss, chao_gate, opt_state = make_step(chao_gate, X, Y, optim, opt_state)  # type: ignore\n",
    "    _, grads = compute_loss(chao_gate, X, Y)\n",
    "    grad_norm_value = grad_norm(grads)\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss}, Grad Norm: {grad_norm_value}\")\n",
    "\n",
    "    # if loss < 1e-3:\n",
    "    #     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Trained ChaoGate Parameters:\n",
      "DELTA: (Array(2.1609957, dtype=float32), Array(-2.069579, dtype=float32)), X0: 0.4518857002258301, X_THRESHOLD: -6.794766426086426\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTrained ChaoGate Parameters:\")\n",
    "print(\n",
    "    f\"DELTA: {chao_gate.DELTA_X, chao_gate.DELTA_Y}, X0: {chao_gate.X0}, X_THRESHOLD: {chao_gate.X_THRESHOLD}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(False, False, True),\n",
       " (False, True, False),\n",
       " (True, False, False),\n",
       " (True, True, True)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[\n",
    "    (\n",
    "        bool(x1.item()),\n",
    "        bool(x2.item()),\n",
    "        (\n",
    "            chao_gate.Map(\n",
    "                chao_gate.X0 + x1 * chao_gate.DELTA_X + x2 * chao_gate.DELTA_Y\n",
    "            )\n",
    "            > chao_gate.X_THRESHOLD\n",
    "        ).item(),\n",
    "    )\n",
    "    for x1, x2 in X\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final_accuracy=1.0\n"
     ]
    }
   ],
   "source": [
    "pred_ys = jax.vmap(chao_gate)(X)\n",
    "num_correct = jnp.sum((pred_ys > 0.5) == Y)\n",
    "final_accuracy = (num_correct / len(X)).item()\n",
    "print(f\"final_accuracy={final_accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
