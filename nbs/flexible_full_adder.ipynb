{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0, Loss: 2.261190176010132\n",
      "Step 100, Loss: 1.2721980810165405\n",
      "Step 200, Loss: 1.1749132871627808\n",
      "Step 300, Loss: 1.1548521518707275\n",
      "Step 400, Loss: 1.1468359231948853\n",
      "Step 500, Loss: 1.142775297164917\n",
      "Step 600, Loss: 1.1403920650482178\n",
      "Step 700, Loss: 1.1388503313064575\n",
      "Step 800, Loss: 1.1377828121185303\n",
      "Step 900, Loss: 1.1370059251785278\n",
      "Step 1000, Loss: 1.1364188194274902\n",
      "Step 1100, Loss: 1.135961651802063\n",
      "Step 1200, Loss: 1.1355972290039062\n",
      "Step 1300, Loss: 1.1353009939193726\n",
      "Step 1400, Loss: 1.1350562572479248\n",
      "Step 1500, Loss: 1.1348512172698975\n",
      "Step 1600, Loss: 1.1346774101257324\n",
      "Step 1700, Loss: 1.134528636932373\n",
      "Step 1800, Loss: 1.1344000101089478\n",
      "Step 1900, Loss: 1.1342881917953491\n",
      "Step 2000, Loss: 1.1341900825500488\n",
      "Step 2100, Loss: 1.134103536605835\n",
      "Step 2200, Loss: 1.1340267658233643\n",
      "Step 2300, Loss: 1.1339584589004517\n",
      "Step 2400, Loss: 1.1338971853256226\n",
      "Step 2500, Loss: 1.13384211063385\n",
      "Step 2600, Loss: 1.1337924003601074\n",
      "Step 2700, Loss: 1.1337474584579468\n",
      "Step 2800, Loss: 1.1337065696716309\n",
      "Step 2900, Loss: 1.133669376373291\n",
      "Step 3000, Loss: 1.1336352825164795\n",
      "Step 3100, Loss: 1.1336042881011963\n",
      "Step 3200, Loss: 1.133575677871704\n",
      "Step 3300, Loss: 1.1335493326187134\n",
      "Step 3400, Loss: 1.1335251331329346\n",
      "Step 3500, Loss: 1.133502721786499\n",
      "Step 3600, Loss: 1.1334819793701172\n",
      "Step 3700, Loss: 1.133462905883789\n",
      "Step 3800, Loss: 1.133445143699646\n",
      "Step 3900, Loss: 1.133428692817688\n",
      "Step 4000, Loss: 1.133413314819336\n",
      "Step 4100, Loss: 1.1333990097045898\n",
      "Step 4200, Loss: 1.1333856582641602\n",
      "Step 4300, Loss: 1.1333732604980469\n",
      "Step 4400, Loss: 1.133361577987671\n",
      "Step 4500, Loss: 1.1333507299423218\n",
      "Step 4600, Loss: 1.1333404779434204\n",
      "Step 4700, Loss: 1.1333309412002563\n",
      "Step 4800, Loss: 1.13332200050354\n",
      "Step 4900, Loss: 1.1333136558532715\n",
      "Step 5000, Loss: 1.1333057880401611\n",
      "Step 5100, Loss: 1.133298397064209\n",
      "Step 5200, Loss: 1.1332913637161255\n",
      "Step 5300, Loss: 1.1332849264144897\n",
      "Step 5400, Loss: 1.133278727531433\n",
      "Step 5500, Loss: 1.1332728862762451\n",
      "Step 5600, Loss: 1.1332674026489258\n",
      "Step 5700, Loss: 1.1332623958587646\n",
      "Step 5800, Loss: 1.1332573890686035\n",
      "Step 5900, Loss: 1.1332528591156006\n",
      "Step 6000, Loss: 1.1332485675811768\n",
      "Step 6100, Loss: 1.133244514465332\n",
      "Step 6200, Loss: 1.1332404613494873\n",
      "Step 6300, Loss: 1.1332368850708008\n",
      "Step 6400, Loss: 1.1332334280014038\n",
      "Step 6500, Loss: 1.133230209350586\n",
      "Step 6600, Loss: 1.1332271099090576\n",
      "Step 6700, Loss: 1.1332242488861084\n",
      "Step 6800, Loss: 1.1332213878631592\n",
      "Step 6900, Loss: 1.133218765258789\n",
      "Step 7000, Loss: 1.1332162618637085\n",
      "Step 7100, Loss: 1.133213996887207\n",
      "Step 7200, Loss: 1.1332118511199951\n",
      "Step 7300, Loss: 1.1332097053527832\n",
      "Step 7400, Loss: 1.1332077980041504\n",
      "Step 7500, Loss: 1.1332058906555176\n",
      "Step 7600, Loss: 1.1332039833068848\n",
      "Step 7700, Loss: 1.133202314376831\n",
      "Step 7800, Loss: 1.133200764656067\n",
      "Step 7900, Loss: 1.1331992149353027\n",
      "Step 8000, Loss: 1.1331977844238281\n",
      "Step 8100, Loss: 1.1331963539123535\n",
      "Step 8200, Loss: 1.1331950426101685\n",
      "Step 8300, Loss: 1.1331937313079834\n",
      "Step 8400, Loss: 1.133192539215088\n",
      "Step 8500, Loss: 1.1331915855407715\n",
      "Step 8600, Loss: 1.1331905126571655\n",
      "Step 8700, Loss: 1.1331894397735596\n",
      "Step 8800, Loss: 1.1331884860992432\n",
      "Step 8900, Loss: 1.1331875324249268\n",
      "Step 9000, Loss: 1.1331866979599\n",
      "Step 9100, Loss: 1.1331859827041626\n",
      "Step 9200, Loss: 1.1331851482391357\n",
      "Step 9300, Loss: 1.1331844329833984\n",
      "Step 9400, Loss: 1.1331837177276611\n",
      "Step 9500, Loss: 1.1331830024719238\n",
      "Step 9600, Loss: 1.1331825256347656\n",
      "Step 9700, Loss: 1.1331818103790283\n",
      "Step 9800, Loss: 1.1331813335418701\n",
      "Step 9900, Loss: 1.1331807374954224\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "import equinox as eqx\n",
    "\n",
    "\n",
    "# ChaoGate class (same as before)\n",
    "class ChaoGate(eqx.Module):\n",
    "    DELTA: jnp.ndarray\n",
    "    X0: jnp.ndarray\n",
    "    X_THRESHOLD: jnp.ndarray\n",
    "\n",
    "    def logistic_map(self, x: float, a: float = 4):\n",
    "        return a * x * (1 - x)\n",
    "\n",
    "    def __call__(self, x: jnp.ndarray):\n",
    "        x1, x2, x3 = x  # Now includes Cin as the third input\n",
    "        return jax.nn.sigmoid(\n",
    "            self.logistic_map(\n",
    "                self.X0 + x1 * self.DELTA + x2 * self.DELTA + x3 * self.DELTA\n",
    "            )\n",
    "            - self.X_THRESHOLD\n",
    "        )\n",
    "\n",
    "\n",
    "# A network of ChaoGates with fixed weights of 1\n",
    "class ChaoGateNetwork(eqx.Module):\n",
    "    gates: list  # No more weights; all gates contribute equally\n",
    "\n",
    "    def __call__(self, inputs: jnp.ndarray):\n",
    "        # Pass inputs to each gate in the array\n",
    "        gate_outputs = jnp.array([gate(inputs) for gate in self.gates])\n",
    "\n",
    "        # Fixed weights of 1: simply sum the gate outputs\n",
    "        sum_of_gates = jnp.sum(gate_outputs)\n",
    "\n",
    "        # Final output is sigmoid to map it to [0, 1]\n",
    "        final_output = jax.nn.sigmoid(sum_of_gates)\n",
    "        return final_output\n",
    "\n",
    "\n",
    "# Full-adder with a flexible array of ChaoGates\n",
    "class FlexibleFullAdder(eqx.Module):\n",
    "    sum_network: ChaoGateNetwork  # Network that will learn to compute the sum\n",
    "    carry_network: ChaoGateNetwork  # Network that will learn to compute the carry\n",
    "\n",
    "    def __call__(self, inputs: jnp.ndarray):\n",
    "        sum_output = self.sum_network(inputs)\n",
    "        carry_output = self.carry_network(inputs)\n",
    "        return sum_output, carry_output\n",
    "\n",
    "\n",
    "# Input-output pairs for a full-adder\n",
    "def full_adder_truth_table():\n",
    "    inputs = jnp.array(\n",
    "        [\n",
    "            [0, 0, 0],\n",
    "            [0, 0, 1],\n",
    "            [0, 1, 0],\n",
    "            [0, 1, 1],\n",
    "            [1, 0, 0],\n",
    "            [1, 0, 1],\n",
    "            [1, 1, 0],\n",
    "            [1, 1, 1],\n",
    "        ]\n",
    "    )\n",
    "    sum_output = jnp.array([0, 1, 1, 0, 1, 0, 0, 1])  # Full adder sum (A XOR B XOR Cin)\n",
    "    carry_output = jnp.array([0, 0, 0, 1, 0, 1, 1, 1])  # Full adder carry\n",
    "    return inputs, sum_output, carry_output\n",
    "\n",
    "\n",
    "# Loss function to train the network\n",
    "def loss_fn(model, inputs, sum_target, carry_target):\n",
    "    sum_pred, carry_pred = jax.vmap(model)(inputs)\n",
    "    sum_loss = -jnp.mean(\n",
    "        sum_target * jnp.log(sum_pred + 1e-15)\n",
    "        + (1 - sum_target) * jnp.log(1 - sum_pred + 1e-15)\n",
    "    )\n",
    "    carry_loss = -jnp.mean(\n",
    "        carry_target * jnp.log(carry_pred + 1e-15)\n",
    "        + (1 - carry_target) * jnp.log(1 - carry_pred + 1e-15)\n",
    "    )\n",
    "    return sum_loss + carry_loss\n",
    "\n",
    "\n",
    "# Training function\n",
    "def train_full_adder(model, optimizer, inputs, sum_target, carry_target, steps=10000):\n",
    "    opt_state = optimizer.init(model)\n",
    "\n",
    "    @jax.jit\n",
    "    def step(model, opt_state, inputs, sum_target, carry_target):\n",
    "        loss, grads = jax.value_and_grad(loss_fn)(\n",
    "            model, inputs, sum_target, carry_target\n",
    "        )\n",
    "        updates, opt_state = optimizer.update(grads, opt_state, model)\n",
    "        model = eqx.apply_updates(model, updates)\n",
    "        return model, opt_state, loss\n",
    "\n",
    "    for step_idx in range(steps):\n",
    "        model, opt_state, loss = step(\n",
    "            model, opt_state, inputs, sum_target, carry_target\n",
    "        )\n",
    "        if step_idx % 100 == 0:\n",
    "            print(f\"Step {step_idx}, Loss: {loss}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# Initialize a network of ChaoGates\n",
    "def initialize_chao_network(num_gates, key):\n",
    "    gates = [\n",
    "        ChaoGate(\n",
    "            DELTA=jax.random.uniform(key, ()),\n",
    "            X0=jax.random.uniform(key, ()),\n",
    "            X_THRESHOLD=jax.random.uniform(key, ()),\n",
    "        )\n",
    "        for _ in range(num_gates)\n",
    "    ]\n",
    "\n",
    "    return ChaoGateNetwork(gates=gates)\n",
    "\n",
    "\n",
    "# Main: Create and train a flexible full-adder model\n",
    "key = jax.random.PRNGKey(0)\n",
    "\n",
    "# Initialize the sum and carry networks with 4 gates each (can adjust the number of gates)\n",
    "sum_network = initialize_chao_network(4, key)\n",
    "carry_network = initialize_chao_network(4, key)\n",
    "\n",
    "# Create the flexible full-adder\n",
    "full_adder_model = FlexibleFullAdder(\n",
    "    sum_network=sum_network, carry_network=carry_network\n",
    ")\n",
    "\n",
    "# Define optimizer (using Optax)\n",
    "optimizer = optax.adam(learning_rate=0.01)\n",
    "\n",
    "# Get the truth table for the full-adder\n",
    "inputs, sum_target, carry_target = full_adder_truth_table()\n",
    "\n",
    "# Train the full-adder model\n",
    "trained_full_adder = train_full_adder(\n",
    "    full_adder_model, optimizer, inputs, sum_target, carry_target\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
