{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import equinox as eqx\n",
    "import optax\n",
    "from tqdm import trange\n",
    "from jaxtyping import Array, Bool, Float\n",
    "\n",
    "from chaogatenn.chaogate import ChaoGate\n",
    "from chaogatenn.maps import DuffingMap\n",
    "from chaogatenn.utils import grad_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data for different logic gates\n",
    "X = jnp.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=bool)  # Input combinations\n",
    "AND_Y = jnp.array([0, 0, 0, 1], dtype=bool)  # AND gate output\n",
    "OR_Y = jnp.array([0, 1, 1, 1], dtype=bool)  # OR gate output\n",
    "XOR_Y = jnp.array([0, 1, 1, 0], dtype=bool)  # XOR gate output\n",
    "NAND_Y = jnp.array([1, 1, 1, 0], dtype=bool)  # NAND gate output\n",
    "NOR_Y = jnp.array([1, 0, 0, 0], dtype=bool)  # NOR gate output\n",
    "XNOR_Y = jnp.array([1, 0, 0, 1], dtype=bool)  # XNOR gate output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of logic gates and their corresponding outputs\n",
    "logic_gates = {\n",
    "    \"AND\": AND_Y,\n",
    "    \"OR\": OR_Y,\n",
    "    \"XOR\": XOR_Y,\n",
    "    \"NAND\": NAND_Y,\n",
    "    \"NOR\": NOR_Y,\n",
    "    \"XNOR\": XNOR_Y,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@eqx.filter_value_and_grad()\n",
    "def compute_loss(\n",
    "    chao_gate: ChaoGate, x: Bool[Array, \"batch 2\"], y: Bool[Array, \"batch\"]\n",
    ") -> Float[Array, \"\"]:\n",
    "    pred = jax.vmap(chao_gate)(x)\n",
    "    # binary cross entropy\n",
    "    return -jnp.mean(y * jnp.log(pred + 1e-15) + (1 - y) * jnp.log(1 - pred + 1e-15))\n",
    "\n",
    "\n",
    "# Function to perform a single optimization step\n",
    "@eqx.filter_jit\n",
    "def make_step(\n",
    "    model: ChaoGate,\n",
    "    x: Bool[Array, \"dim 2\"],\n",
    "    y: Bool[Array, \"dim\"],\n",
    "    optim: optax.GradientTransformation,\n",
    "    opt_state: optax.OptState,\n",
    ") -> tuple[Float[Array, \"dim\"], ChaoGate, optax.OptState]:\n",
    "    loss, grads = compute_loss(model, x, y)\n",
    "    updates, opt_state = optim.update(grads, opt_state)\n",
    "    model = eqx.apply_updates(model, updates)\n",
    "    return loss, model, opt_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"../output/duffing_sweep/\"\n",
    "metrics_dict = {}\n",
    "results_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training AND gate with beta=nan:   7%|â–‹         | 67/1000 [00:14<02:44,  5.66it/s]"
     ]
    }
   ],
   "source": [
    "for gate_name, Y in logic_gates.items():\n",
    "    metrics_dict[gate_name] = []\n",
    "    results_dict[gate_name] = []\n",
    "    for beta in jnp.geomspace(1e-3, 5, num=40):  # 50 steps from 0 to 4\n",
    "        Map = DuffingMap(beta=beta)\n",
    "        chao_gate = ChaoGate(DELTA=1.0, X0=1.0, X_THRESHOLD=1.0, Map=Map)\n",
    "        optim = optax.adabelief(3e-4)\n",
    "        opt_state = optim.init(eqx.filter(chao_gate, eqx.is_inexact_array))\n",
    "\n",
    "        epochs = 1000\n",
    "        for epoch in trange(\n",
    "            epochs, desc=f\"Training {gate_name} gate with beta={beta:.2f}\"\n",
    "        ):\n",
    "            loss, chao_gate, opt_state = make_step(chao_gate, X, Y, optim, opt_state)\n",
    "            _, grads = compute_loss(chao_gate, X, Y)\n",
    "            grad_norm_value = grad_norm(grads)\n",
    "\n",
    "        pred_ys = jax.vmap(chao_gate)(X)\n",
    "        num_correct = jnp.sum((pred_ys > 0.5) == Y)\n",
    "        final_accuracy = (num_correct / len(X)).item()\n",
    "        metrics_dict[gate_name].append(\n",
    "            (beta, loss.item(), final_accuracy, grad_norm_value)\n",
    "        )\n",
    "        results_dict[gate_name].append(\n",
    "            (beta, chao_gate.DELTA, chao_gate.X0, chao_gate.X_THRESHOLD)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results for AND gate:\n",
      "a=0.00, Loss=0.029688, Accuracy=1.00, Grad Norm=0.036074\n",
      "a=0.10, Loss=0.038069, Accuracy=1.00, Grad Norm=0.059445\n",
      "a=0.21, Loss=0.080870, Accuracy=1.00, Grad Norm=0.102492\n",
      "a=0.31, Loss=0.133663, Accuracy=1.00, Grad Norm=0.147805\n",
      "a=0.41, Loss=0.200792, Accuracy=1.00, Grad Norm=0.203656\n",
      "a=0.51, Loss=0.307240, Accuracy=1.00, Grad Norm=0.318958\n",
      "a=0.62, Loss=0.479412, Accuracy=0.75, Grad Norm=0.362791\n",
      "a=0.72, Loss=0.558830, Accuracy=0.75, Grad Norm=0.163402\n",
      "a=0.82, Loss=0.577126, Accuracy=0.75, Grad Norm=0.126004\n",
      "a=0.92, Loss=0.581904, Accuracy=0.75, Grad Norm=0.146581\n",
      "a=1.03, Loss=0.583202, Accuracy=0.75, Grad Norm=0.170779\n",
      "a=1.13, Loss=0.583410, Accuracy=0.75, Grad Norm=0.192272\n",
      "a=1.23, Loss=0.583229, Accuracy=0.75, Grad Norm=0.211467\n",
      "a=1.33, Loss=0.582894, Accuracy=0.75, Grad Norm=0.229102\n",
      "a=1.44, Loss=0.582493, Accuracy=0.75, Grad Norm=0.245686\n",
      "a=1.54, Loss=0.582057, Accuracy=0.75, Grad Norm=0.261544\n",
      "a=1.64, Loss=0.581601, Accuracy=0.75, Grad Norm=0.276882\n",
      "a=1.74, Loss=0.581128, Accuracy=0.75, Grad Norm=0.291835\n",
      "a=1.85, Loss=0.580642, Accuracy=0.75, Grad Norm=0.306494\n",
      "a=1.95, Loss=0.580143, Accuracy=0.75, Grad Norm=0.320925\n",
      "a=2.05, Loss=0.579631, Accuracy=0.75, Grad Norm=0.335176\n",
      "a=2.15, Loss=0.579108, Accuracy=0.75, Grad Norm=0.349281\n",
      "a=2.26, Loss=0.578575, Accuracy=0.75, Grad Norm=0.363269\n",
      "a=2.36, Loss=0.578032, Accuracy=0.75, Grad Norm=0.377160\n",
      "a=2.46, Loss=0.577481, Accuracy=0.75, Grad Norm=0.390969\n",
      "a=2.56, Loss=0.576923, Accuracy=0.75, Grad Norm=0.404714\n",
      "a=2.67, Loss=0.576358, Accuracy=0.75, Grad Norm=0.418401\n",
      "a=2.77, Loss=0.575790, Accuracy=0.75, Grad Norm=0.432043\n",
      "a=2.87, Loss=0.575218, Accuracy=0.75, Grad Norm=0.445647\n",
      "a=2.97, Loss=0.574644, Accuracy=0.75, Grad Norm=0.459217\n",
      "a=3.08, Loss=0.574068, Accuracy=0.75, Grad Norm=0.472762\n",
      "a=3.18, Loss=0.573492, Accuracy=0.75, Grad Norm=0.486283\n",
      "a=3.28, Loss=0.572917, Accuracy=0.75, Grad Norm=0.499784\n",
      "a=3.38, Loss=0.572344, Accuracy=0.75, Grad Norm=0.513271\n",
      "a=3.49, Loss=0.571772, Accuracy=0.75, Grad Norm=0.526744\n",
      "a=3.59, Loss=0.571204, Accuracy=0.75, Grad Norm=0.540205\n",
      "a=3.69, Loss=0.570639, Accuracy=0.75, Grad Norm=0.553658\n",
      "a=3.79, Loss=0.570077, Accuracy=0.75, Grad Norm=0.567101\n",
      "a=3.90, Loss=0.569520, Accuracy=0.75, Grad Norm=0.580533\n",
      "a=4.00, Loss=0.568966, Accuracy=0.75, Grad Norm=0.593949\n",
      "\n",
      "Results for OR gate:\n",
      "a=0.00, Loss=0.084465, Accuracy=1.00, Grad Norm=0.097722\n",
      "a=0.10, Loss=0.123962, Accuracy=1.00, Grad Norm=0.127922\n",
      "a=0.21, Loss=0.171562, Accuracy=1.00, Grad Norm=0.162464\n",
      "a=0.31, Loss=0.228501, Accuracy=1.00, Grad Norm=0.228874\n",
      "a=0.41, Loss=0.329144, Accuracy=0.75, Grad Norm=0.373939\n",
      "a=0.51, Loss=0.512271, Accuracy=0.75, Grad Norm=0.406756\n",
      "a=0.62, Loss=0.599003, Accuracy=0.75, Grad Norm=0.199277\n",
      "a=0.72, Loss=0.611369, Accuracy=0.75, Grad Norm=0.180284\n",
      "a=0.82, Loss=0.611179, Accuracy=0.75, Grad Norm=0.203507\n",
      "a=0.92, Loss=0.608829, Accuracy=0.75, Grad Norm=0.225481\n",
      "a=1.03, Loss=0.606138, Accuracy=0.75, Grad Norm=0.244502\n",
      "a=1.13, Loss=0.603479, Accuracy=0.75, Grad Norm=0.261697\n",
      "a=1.23, Loss=0.600930, Accuracy=0.75, Grad Norm=0.277849\n",
      "a=1.33, Loss=0.598494, Accuracy=0.75, Grad Norm=0.293398\n",
      "a=1.44, Loss=0.596162, Accuracy=0.75, Grad Norm=0.308591\n",
      "a=1.54, Loss=0.593921, Accuracy=0.75, Grad Norm=0.323572\n",
      "a=1.64, Loss=0.591762, Accuracy=0.75, Grad Norm=0.338425\n",
      "a=1.74, Loss=0.589678, Accuracy=0.75, Grad Norm=0.353201\n",
      "a=1.85, Loss=0.587661, Accuracy=0.75, Grad Norm=0.367933\n",
      "a=1.95, Loss=0.585708, Accuracy=0.75, Grad Norm=0.382642\n",
      "a=2.05, Loss=0.583816, Accuracy=0.75, Grad Norm=0.397341\n",
      "a=2.15, Loss=0.581980, Accuracy=0.75, Grad Norm=0.412039\n",
      "a=2.26, Loss=0.580198, Accuracy=0.75, Grad Norm=0.426742\n",
      "a=2.36, Loss=0.578470, Accuracy=0.75, Grad Norm=0.441454\n",
      "a=2.46, Loss=0.576792, Accuracy=0.75, Grad Norm=0.456175\n",
      "a=2.56, Loss=0.575164, Accuracy=0.75, Grad Norm=0.470910\n",
      "a=2.67, Loss=0.573584, Accuracy=0.75, Grad Norm=0.485656\n",
      "a=2.77, Loss=0.572051, Accuracy=0.75, Grad Norm=0.500415\n",
      "a=2.87, Loss=0.570563, Accuracy=0.75, Grad Norm=0.515188\n",
      "a=2.97, Loss=0.569121, Accuracy=0.75, Grad Norm=0.529971\n",
      "a=3.08, Loss=0.567724, Accuracy=0.75, Grad Norm=0.544764\n",
      "a=3.18, Loss=0.566371, Accuracy=0.75, Grad Norm=0.559570\n",
      "a=3.28, Loss=0.565060, Accuracy=0.75, Grad Norm=0.574384\n",
      "a=3.38, Loss=0.563792, Accuracy=0.75, Grad Norm=0.589205\n",
      "a=3.49, Loss=0.562565, Accuracy=0.75, Grad Norm=0.604033\n",
      "a=3.59, Loss=0.561380, Accuracy=0.75, Grad Norm=0.618865\n",
      "a=3.69, Loss=0.560237, Accuracy=0.75, Grad Norm=0.633704\n",
      "a=3.79, Loss=0.559134, Accuracy=0.75, Grad Norm=0.648541\n",
      "a=3.90, Loss=0.558071, Accuracy=0.75, Grad Norm=0.663380\n",
      "a=4.00, Loss=0.557048, Accuracy=0.75, Grad Norm=0.678214\n",
      "\n",
      "Results for XOR gate:\n",
      "a=0.00, Loss=0.693147, Accuracy=0.50, Grad Norm=0.000000\n",
      "a=0.10, Loss=0.680083, Accuracy=0.25, Grad Norm=0.059936\n",
      "a=0.21, Loss=0.670909, Accuracy=0.25, Grad Norm=0.075785\n",
      "a=0.31, Loss=0.667827, Accuracy=0.25, Grad Norm=0.083071\n",
      "a=0.41, Loss=0.664076, Accuracy=0.25, Grad Norm=0.087612\n",
      "a=0.51, Loss=0.659706, Accuracy=0.25, Grad Norm=0.090949\n",
      "a=0.62, Loss=0.655200, Accuracy=0.25, Grad Norm=0.095203\n",
      "a=0.72, Loss=0.650839, Accuracy=0.25, Grad Norm=0.100881\n",
      "a=0.82, Loss=0.646710, Accuracy=0.25, Grad Norm=0.107685\n",
      "a=0.92, Loss=0.642814, Accuracy=0.25, Grad Norm=0.115237\n",
      "a=1.03, Loss=0.639125, Accuracy=0.75, Grad Norm=0.123259\n",
      "a=1.13, Loss=0.635615, Accuracy=0.75, Grad Norm=0.131573\n",
      "a=1.23, Loss=0.632260, Accuracy=0.75, Grad Norm=0.140068\n",
      "a=1.33, Loss=0.629041, Accuracy=0.75, Grad Norm=0.148674\n",
      "a=1.44, Loss=0.625940, Accuracy=0.75, Grad Norm=0.157348\n",
      "a=1.54, Loss=0.622947, Accuracy=0.75, Grad Norm=0.166060\n",
      "a=1.64, Loss=0.620050, Accuracy=0.75, Grad Norm=0.174793\n",
      "a=1.74, Loss=0.617243, Accuracy=0.75, Grad Norm=0.183534\n",
      "a=1.85, Loss=0.614519, Accuracy=0.75, Grad Norm=0.192275\n",
      "a=1.95, Loss=0.611873, Accuracy=0.75, Grad Norm=0.201011\n",
      "a=2.05, Loss=0.609300, Accuracy=0.75, Grad Norm=0.209737\n",
      "a=2.15, Loss=0.606797, Accuracy=0.75, Grad Norm=0.218452\n",
      "a=2.26, Loss=0.604359, Accuracy=0.75, Grad Norm=0.227153\n",
      "a=2.36, Loss=0.601985, Accuracy=0.75, Grad Norm=0.235841\n",
      "a=2.46, Loss=0.599671, Accuracy=0.75, Grad Norm=0.244514\n",
      "a=2.56, Loss=0.597415, Accuracy=0.75, Grad Norm=0.253173\n",
      "a=2.67, Loss=0.595216, Accuracy=0.75, Grad Norm=0.261817\n",
      "a=2.77, Loss=0.593070, Accuracy=0.75, Grad Norm=0.270449\n",
      "a=2.87, Loss=0.590978, Accuracy=0.75, Grad Norm=0.279066\n",
      "a=2.97, Loss=0.588936, Accuracy=0.75, Grad Norm=0.287672\n",
      "a=3.08, Loss=0.586944, Accuracy=0.75, Grad Norm=0.296265\n",
      "a=3.18, Loss=0.585001, Accuracy=0.75, Grad Norm=0.304848\n",
      "a=3.28, Loss=0.583104, Accuracy=0.75, Grad Norm=0.313421\n",
      "a=3.38, Loss=0.581254, Accuracy=0.75, Grad Norm=0.321983\n",
      "a=3.49, Loss=0.579448, Accuracy=0.75, Grad Norm=0.330538\n",
      "a=3.59, Loss=0.577687, Accuracy=0.75, Grad Norm=0.339086\n",
      "a=3.69, Loss=0.575969, Accuracy=0.75, Grad Norm=0.347626\n",
      "a=3.79, Loss=0.574294, Accuracy=0.75, Grad Norm=0.356160\n",
      "a=3.90, Loss=0.572661, Accuracy=0.75, Grad Norm=0.364690\n",
      "a=4.00, Loss=0.571070, Accuracy=0.75, Grad Norm=0.373214\n",
      "\n",
      "Results for NAND gate:\n",
      "a=0.00, Loss=nan, Accuracy=0.25, Grad Norm=nan\n",
      "a=0.10, Loss=0.382549, Accuracy=1.00, Grad Norm=0.248938\n",
      "a=0.21, Loss=0.402067, Accuracy=1.00, Grad Norm=0.251350\n",
      "a=0.31, Loss=0.418574, Accuracy=1.00, Grad Norm=0.254377\n",
      "a=0.41, Loss=0.426036, Accuracy=1.00, Grad Norm=0.255913\n",
      "a=0.51, Loss=0.428471, Accuracy=1.00, Grad Norm=0.256542\n",
      "a=0.62, Loss=0.427983, Accuracy=1.00, Grad Norm=0.256756\n",
      "a=0.72, Loss=0.425724, Accuracy=1.00, Grad Norm=0.256962\n",
      "a=0.82, Loss=0.422376, Accuracy=1.00, Grad Norm=0.257352\n",
      "a=0.92, Loss=0.418351, Accuracy=1.00, Grad Norm=0.257956\n",
      "a=1.03, Loss=0.413902, Accuracy=1.00, Grad Norm=0.258729\n",
      "a=1.13, Loss=0.409187, Accuracy=1.00, Grad Norm=0.259608\n",
      "a=1.23, Loss=0.404313, Accuracy=1.00, Grad Norm=0.260540\n",
      "a=1.33, Loss=0.399349, Accuracy=1.00, Grad Norm=0.261484\n",
      "a=1.44, Loss=0.394344, Accuracy=1.00, Grad Norm=0.262410\n",
      "a=1.54, Loss=0.389332, Accuracy=1.00, Grad Norm=0.263298\n",
      "a=1.64, Loss=0.384338, Accuracy=1.00, Grad Norm=0.264136\n",
      "a=1.74, Loss=0.379378, Accuracy=1.00, Grad Norm=0.264916\n",
      "a=1.85, Loss=0.374467, Accuracy=1.00, Grad Norm=0.265634\n",
      "a=1.95, Loss=0.369611, Accuracy=1.00, Grad Norm=0.266287\n",
      "a=2.05, Loss=0.364817, Accuracy=1.00, Grad Norm=0.266875\n",
      "a=2.15, Loss=0.360089, Accuracy=1.00, Grad Norm=0.267399\n",
      "a=2.26, Loss=0.355428, Accuracy=1.00, Grad Norm=0.267859\n",
      "a=2.36, Loss=0.350837, Accuracy=1.00, Grad Norm=0.268259\n",
      "a=2.46, Loss=0.346313, Accuracy=1.00, Grad Norm=0.268599\n",
      "a=2.56, Loss=0.341859, Accuracy=1.00, Grad Norm=0.268881\n",
      "a=2.67, Loss=0.337472, Accuracy=1.00, Grad Norm=0.269108\n",
      "a=2.77, Loss=0.333153, Accuracy=1.00, Grad Norm=0.269283\n",
      "a=2.87, Loss=0.328897, Accuracy=1.00, Grad Norm=0.269405\n",
      "a=2.97, Loss=0.324705, Accuracy=1.00, Grad Norm=0.269479\n",
      "a=3.08, Loss=0.320575, Accuracy=1.00, Grad Norm=0.269504\n",
      "a=3.18, Loss=0.316504, Accuracy=1.00, Grad Norm=0.269484\n",
      "a=3.28, Loss=0.312492, Accuracy=1.00, Grad Norm=0.269421\n",
      "a=3.38, Loss=0.308537, Accuracy=1.00, Grad Norm=0.269316\n",
      "a=3.49, Loss=0.304636, Accuracy=1.00, Grad Norm=0.269169\n",
      "a=3.59, Loss=0.300789, Accuracy=1.00, Grad Norm=0.268984\n",
      "a=3.69, Loss=0.296994, Accuracy=1.00, Grad Norm=0.268762\n",
      "a=3.79, Loss=0.293250, Accuracy=1.00, Grad Norm=0.268503\n",
      "a=3.90, Loss=0.289554, Accuracy=1.00, Grad Norm=0.268210\n",
      "a=4.00, Loss=0.285906, Accuracy=1.00, Grad Norm=0.267883\n",
      "\n",
      "Results for NOR gate:\n",
      "a=0.00, Loss=0.021811, Accuracy=1.00, Grad Norm=0.029101\n",
      "a=0.10, Loss=0.018749, Accuracy=1.00, Grad Norm=0.027862\n",
      "a=0.21, Loss=0.030648, Accuracy=1.00, Grad Norm=0.035689\n",
      "a=0.31, Loss=0.039056, Accuracy=1.00, Grad Norm=0.040045\n",
      "a=0.41, Loss=0.044441, Accuracy=1.00, Grad Norm=0.042867\n",
      "a=0.51, Loss=0.047873, Accuracy=1.00, Grad Norm=0.044963\n",
      "a=0.62, Loss=0.049785, Accuracy=1.00, Grad Norm=0.046302\n",
      "a=0.72, Loss=0.050902, Accuracy=1.00, Grad Norm=0.047155\n",
      "a=0.82, Loss=0.051268, Accuracy=1.00, Grad Norm=0.047464\n",
      "a=0.92, Loss=0.051016, Accuracy=1.00, Grad Norm=0.047291\n",
      "a=1.03, Loss=0.050235, Accuracy=1.00, Grad Norm=0.046681\n",
      "a=1.13, Loss=0.048994, Accuracy=1.00, Grad Norm=0.045677\n",
      "a=1.23, Loss=0.047355, Accuracy=1.00, Grad Norm=0.044321\n",
      "a=1.33, Loss=0.045389, Accuracy=1.00, Grad Norm=0.042667\n",
      "a=1.44, Loss=0.043183, Accuracy=1.00, Grad Norm=0.040783\n",
      "a=1.54, Loss=0.040841, Accuracy=1.00, Grad Norm=0.038758\n",
      "a=1.64, Loss=0.038484, Accuracy=1.00, Grad Norm=0.036695\n",
      "a=1.74, Loss=0.036233, Accuracy=1.00, Grad Norm=0.034706\n",
      "a=1.85, Loss=0.034194, Accuracy=1.00, Grad Norm=0.032886\n",
      "a=1.95, Loss=0.032436, Accuracy=1.00, Grad Norm=0.031307\n",
      "a=2.05, Loss=0.030987, Accuracy=1.00, Grad Norm=0.029997\n",
      "a=2.15, Loss=0.029830, Accuracy=1.00, Grad Norm=0.028949\n",
      "a=2.26, Loss=0.028925, Accuracy=1.00, Grad Norm=0.028129\n",
      "a=2.36, Loss=0.028218, Accuracy=1.00, Grad Norm=0.027491\n",
      "a=2.46, Loss=0.027657, Accuracy=1.00, Grad Norm=0.026990\n",
      "a=2.56, Loss=0.027199, Accuracy=1.00, Grad Norm=0.026587\n",
      "a=2.67, Loss=0.026812, Accuracy=1.00, Grad Norm=0.026254\n",
      "a=2.77, Loss=0.026471, Accuracy=1.00, Grad Norm=0.025970\n",
      "a=2.87, Loss=0.026159, Accuracy=1.00, Grad Norm=0.025720\n",
      "a=2.97, Loss=0.025867, Accuracy=1.00, Grad Norm=0.025496\n",
      "a=3.08, Loss=0.025587, Accuracy=1.00, Grad Norm=0.025292\n",
      "a=3.18, Loss=0.025314, Accuracy=1.00, Grad Norm=0.025104\n",
      "a=3.28, Loss=0.025047, Accuracy=1.00, Grad Norm=0.024931\n",
      "a=3.38, Loss=0.024782, Accuracy=1.00, Grad Norm=0.024773\n",
      "a=3.49, Loss=0.024521, Accuracy=1.00, Grad Norm=0.024629\n",
      "a=3.59, Loss=0.024261, Accuracy=1.00, Grad Norm=0.024499\n",
      "a=3.69, Loss=0.024003, Accuracy=1.00, Grad Norm=0.024385\n",
      "a=3.79, Loss=0.023747, Accuracy=1.00, Grad Norm=0.024286\n",
      "a=3.90, Loss=0.023493, Accuracy=1.00, Grad Norm=0.024204\n",
      "a=4.00, Loss=0.023240, Accuracy=1.00, Grad Norm=0.024139\n",
      "\n",
      "Results for XNOR gate:\n",
      "a=0.00, Loss=0.116165, Accuracy=1.00, Grad Norm=0.120008\n",
      "a=0.10, Loss=0.482147, Accuracy=0.75, Grad Norm=0.351986\n",
      "a=0.21, Loss=0.640461, Accuracy=0.75, Grad Norm=0.089047\n",
      "a=0.31, Loss=0.655748, Accuracy=0.75, Grad Norm=0.067376\n",
      "a=0.41, Loss=0.665552, Accuracy=0.75, Grad Norm=0.080457\n",
      "a=0.51, Loss=0.680817, Accuracy=0.75, Grad Norm=0.102668\n",
      "a=0.62, Loss=0.695480, Accuracy=0.50, Grad Norm=0.087442\n",
      "a=0.72, Loss=0.703494, Accuracy=0.50, Grad Norm=0.083696\n",
      "a=0.82, Loss=0.707749, Accuracy=0.75, Grad Norm=0.095450\n",
      "a=0.92, Loss=0.710410, Accuracy=0.75, Grad Norm=0.111115\n",
      "a=1.03, Loss=0.712380, Accuracy=0.75, Grad Norm=0.126770\n",
      "a=1.13, Loss=0.714031, Accuracy=0.25, Grad Norm=0.141699\n",
      "a=1.23, Loss=0.715531, Accuracy=0.25, Grad Norm=0.155904\n",
      "a=1.33, Loss=0.716957, Accuracy=0.25, Grad Norm=0.169512\n",
      "a=1.44, Loss=0.718352, Accuracy=0.25, Grad Norm=0.182642\n",
      "a=1.54, Loss=0.719738, Accuracy=0.25, Grad Norm=0.195387\n",
      "a=1.64, Loss=0.721127, Accuracy=0.25, Grad Norm=0.207818\n",
      "a=1.74, Loss=0.722527, Accuracy=0.25, Grad Norm=0.219991\n",
      "a=1.85, Loss=0.723941, Accuracy=0.25, Grad Norm=0.231945\n",
      "a=1.95, Loss=0.725374, Accuracy=0.25, Grad Norm=0.243712\n",
      "a=2.05, Loss=0.726826, Accuracy=0.25, Grad Norm=0.255317\n",
      "a=2.15, Loss=0.728298, Accuracy=0.25, Grad Norm=0.266781\n",
      "a=2.26, Loss=0.729792, Accuracy=0.25, Grad Norm=0.278120\n",
      "a=2.36, Loss=0.731307, Accuracy=0.25, Grad Norm=0.289346\n",
      "a=2.46, Loss=0.732844, Accuracy=0.25, Grad Norm=0.300472\n",
      "a=2.56, Loss=0.734403, Accuracy=0.25, Grad Norm=0.311507\n",
      "a=2.67, Loss=0.735984, Accuracy=0.25, Grad Norm=0.322457\n",
      "a=2.77, Loss=0.737587, Accuracy=0.25, Grad Norm=0.333331\n",
      "a=2.87, Loss=0.739212, Accuracy=0.25, Grad Norm=0.344135\n",
      "a=2.97, Loss=0.740860, Accuracy=0.25, Grad Norm=0.354872\n",
      "a=3.08, Loss=0.742530, Accuracy=0.25, Grad Norm=0.365547\n",
      "a=3.18, Loss=0.744222, Accuracy=0.25, Grad Norm=0.376166\n",
      "a=3.28, Loss=0.745937, Accuracy=0.25, Grad Norm=0.386730\n",
      "a=3.38, Loss=0.747675, Accuracy=0.25, Grad Norm=0.397241\n",
      "a=3.49, Loss=0.749435, Accuracy=0.25, Grad Norm=0.407705\n",
      "a=3.59, Loss=0.751217, Accuracy=0.25, Grad Norm=0.418119\n",
      "a=3.69, Loss=0.753023, Accuracy=0.25, Grad Norm=0.428490\n",
      "a=3.79, Loss=0.754851, Accuracy=0.25, Grad Norm=0.438816\n",
      "a=3.90, Loss=0.756701, Accuracy=0.25, Grad Norm=0.449099\n",
      "a=4.00, Loss=0.758574, Accuracy=0.25, Grad Norm=0.459337\n"
     ]
    }
   ],
   "source": [
    "# Print results\n",
    "for gate_name, metrics in metrics_dict.items():\n",
    "    print(f\"\\nResults for {gate_name} gate:\")\n",
    "    for a, loss, accuracy, grad_norm_value in metrics:\n",
    "        print(\n",
    "            f\"beta={beta:.2f}, Loss={loss:.6f}, Accuracy={accuracy:.2f}, Grad Norm={grad_norm_value:.6f}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform into arrays and save using numpy savetxt\n",
    "for gate_name, metrics in metrics_dict.items():\n",
    "    metrics = jnp.array(metrics)\n",
    "    np.savetxt(f\"{output_dir}{gate_name}_metrics.txt\", metrics, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for gate_name, results in results_dict.items():\n",
    "    results = jnp.array(results)\n",
    "    np.savetxt(f\"{output_dir}{gate_name}_results.txt\", results, delimiter=\",\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
