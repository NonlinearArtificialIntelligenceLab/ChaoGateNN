{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: JAX_PLATFORM_NAME=cpu\n"
     ]
    }
   ],
   "source": [
    "%env JAX_PLATFORM_NAME=cpu\n",
    "\n",
    "import jaxtyping  # noqa: F401\n",
    "\n",
    "%load_ext jaxtyping\n",
    "%jaxtyping.typechecker beartype.beartype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import equinox as eqx\n",
    "import optax\n",
    "\n",
    "from tqdm.notebook import trange\n",
    "\n",
    "from jaxtyping import Array, Bool, Float\n",
    "\n",
    "from chaogatenn.chaogate import ChaoGate\n",
    "from chaogatenn.maps import LogisticMap\n",
    "from chaogatenn.utils import grad_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Map = LogisticMap(a=4.0)\n",
    "# Map = LorenzMap(sigma=10.0, rho=28.0, beta=8/3, dt=0.01, steps=1000)\n",
    "# Map = DuffingMap(\n",
    "#     alpha=5.0, beta=5.0, delta=0.02, gamma=8.0, omega=0.5, dt=0.01, steps=1000\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n"
     ]
    }
   ],
   "source": [
    "# Training data for the Half Adder\n",
    "X = jnp.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=bool)  # Input combinations\n",
    "y_sum = jnp.array([0, 1, 1, 0], dtype=bool)  # XOR gate output for Sum\n",
    "y_carry = jnp.array([0, 0, 0, 1], dtype=bool)  # AND gate output for Carry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@eqx.filter_value_and_grad()\n",
    "def compute_sum_loss(\n",
    "    xor_gate: ChaoGate, x: Bool[Array, \"batch 2\"], y_sum: Bool[Array, \"batch\"]\n",
    ") -> Float[Array, \"\"]:\n",
    "    pred_sum = jax.vmap(xor_gate)(x)\n",
    "\n",
    "    # Binary cross-entropy loss for XOR (Sum)\n",
    "    loss_sum = -jnp.mean(\n",
    "        y_sum * jnp.log(pred_sum + 1e-15) + (1 - y_sum) * jnp.log(1 - pred_sum + 1e-15)\n",
    "    )\n",
    "\n",
    "    return loss_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@eqx.filter_value_and_grad()\n",
    "def compute_carry_loss(\n",
    "    and_gate: ChaoGate, x: Bool[Array, \"batch 2\"], y_carry: Bool[Array, \"batch\"]\n",
    ") -> Float[Array, \"\"]:\n",
    "    pred_carry = jax.vmap(and_gate)(x)\n",
    "    # Binary cross-entropy loss for AND (Carry)\n",
    "    loss_carry = -jnp.mean(\n",
    "        y_carry * jnp.log(pred_carry + 1e-15)\n",
    "        + (1 - y_carry) * jnp.log(1 - pred_carry + 1e-15)\n",
    "    )\n",
    "\n",
    "    # Total loss is the sum of both losses\n",
    "    return loss_carry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@eqx.filter_jit\n",
    "def make_step(\n",
    "    xor_gate: ChaoGate,\n",
    "    and_gate: ChaoGate,\n",
    "    X: Bool[Array, \"batch 2\"],\n",
    "    y_sum: Bool[Array, \"batch\"],\n",
    "    y_carry: Bool[Array, \"batch \"],\n",
    "    optim: optax.GradientTransformation,\n",
    "    opt_state: optax.OptState,\n",
    ") -> (Float[Array, \"dim\"], ChaoGate, optax.OptState):  # type: ignore\n",
    "    loss_sum, grads_sum = compute_sum_loss(xor_gate, X, y_sum)\n",
    "\n",
    "    loss_carry, grads_carry = compute_carry_loss(and_gate, X, y_carry)\n",
    "\n",
    "    loss = loss_sum + loss_carry\n",
    "    updates, opt_state = optim.update([grads_sum, grads_carry], opt_state)\n",
    "\n",
    "    xor_gate = eqx.apply_updates(xor_gate, updates[0])  # type: ignore\n",
    "    and_gate = eqx.apply_updates(and_gate, updates[1])  # type: ignore\n",
    "    return loss, xor_gate, and_gate, opt_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the XOR and AND gates with random values\n",
    "xor_gate = ChaoGate(DELTA=0.5, X0=1.0, X_THRESHOLD=0.4, Map=Map)\n",
    "and_gate = ChaoGate(DELTA=0.5, X0=1.0, X_THRESHOLD=0.4, Map=Map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = optax.adabelief(learning_rate=3e-4)\n",
    "opt_state = optim.init(eqx.filter([xor_gate, and_gate], eqx.is_inexact_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1114b5ded87f4a7f8382f3065dbe8477",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss: 2.115169048309326, grad norm: 6.584014415740967\n",
      "Epoch 10, loss: 1.3006647825241089, grad norm: 2.2727248668670654\n",
      "Epoch 20, loss: 1.236580729484558, grad norm: 1.4815900325775146\n",
      "Epoch 30, loss: 1.2312860488891602, grad norm: 1.6913783550262451\n",
      "Epoch 40, loss: 1.2257707118988037, grad norm: 1.0811671018600464\n",
      "Epoch 50, loss: 1.2197433710098267, grad norm: 0.8808463215827942\n",
      "Epoch 60, loss: 1.2133344411849976, grad norm: 1.1098077297210693\n",
      "Epoch 70, loss: 1.2062852382659912, grad norm: 0.9026330709457397\n",
      "Epoch 80, loss: 1.1987131834030151, grad norm: 0.9659523367881775\n",
      "Epoch 90, loss: 1.1906392574310303, grad norm: 0.9868104457855225\n",
      "Epoch 100, loss: 1.18207848072052, grad norm: 0.9743662476539612\n",
      "Epoch 110, loss: 1.1730650663375854, grad norm: 1.0136175155639648\n",
      "Epoch 120, loss: 1.1636264324188232, grad norm: 1.0104522705078125\n",
      "Epoch 130, loss: 1.1537952423095703, grad norm: 1.0302644968032837\n",
      "Epoch 140, loss: 1.143601894378662, grad norm: 1.0360119342803955\n",
      "Epoch 150, loss: 1.1330803632736206, grad norm: 1.0448931455612183\n",
      "Epoch 160, loss: 1.1222736835479736, grad norm: 1.049267053604126\n",
      "Epoch 170, loss: 1.1112216711044312, grad norm: 1.051620364189148\n",
      "Epoch 180, loss: 1.0999714136123657, grad norm: 1.0504950284957886\n",
      "Epoch 190, loss: 1.0885666608810425, grad norm: 1.0469199419021606\n",
      "Epoch 200, loss: 1.0770589113235474, grad norm: 1.0402084589004517\n",
      "Epoch 210, loss: 1.0654921531677246, grad norm: 1.0309544801712036\n",
      "Epoch 220, loss: 1.0539273023605347, grad norm: 1.0185929536819458\n",
      "Epoch 230, loss: 1.042400598526001, grad norm: 1.0038689374923706\n",
      "Epoch 240, loss: 1.0309665203094482, grad norm: 0.9868859648704529\n",
      "Epoch 250, loss: 1.0196629762649536, grad norm: 0.9673240184783936\n",
      "Epoch 260, loss: 1.0085335969924927, grad norm: 0.9463520646095276\n",
      "Epoch 270, loss: 0.99760901927948, grad norm: 0.9237504005432129\n",
      "Epoch 280, loss: 0.9869183897972107, grad norm: 0.8996335864067078\n",
      "Epoch 290, loss: 0.9764915704727173, grad norm: 0.8746433258056641\n",
      "Epoch 300, loss: 0.9663490056991577, grad norm: 0.8489462733268738\n",
      "Epoch 310, loss: 0.9565062522888184, grad norm: 0.8227253556251526\n",
      "Epoch 320, loss: 0.9469731450080872, grad norm: 0.7961699962615967\n",
      "Epoch 330, loss: 0.9377588033676147, grad norm: 0.7692322134971619\n",
      "Epoch 340, loss: 0.9288696050643921, grad norm: 0.7430345416069031\n",
      "Epoch 350, loss: 0.920303225517273, grad norm: 0.7163960933685303\n",
      "Epoch 360, loss: 0.9120607376098633, grad norm: 0.690513551235199\n",
      "Epoch 370, loss: 0.9041381478309631, grad norm: 0.6646666526794434\n",
      "Epoch 380, loss: 0.8965326547622681, grad norm: 0.6398224830627441\n",
      "Epoch 390, loss: 0.8892308473587036, grad norm: 0.6153446435928345\n",
      "Epoch 400, loss: 0.8822295665740967, grad norm: 0.5914186835289001\n",
      "Epoch 410, loss: 0.8755197525024414, grad norm: 0.5683715343475342\n",
      "Epoch 420, loss: 0.8690921068191528, grad norm: 0.5461233258247375\n",
      "Epoch 430, loss: 0.8629322052001953, grad norm: 0.524351954460144\n",
      "Epoch 440, loss: 0.8570362329483032, grad norm: 0.5035119652748108\n",
      "Epoch 450, loss: 0.8513912558555603, grad norm: 0.4832683205604553\n",
      "Epoch 460, loss: 0.8459827303886414, grad norm: 0.4638310372829437\n",
      "Epoch 470, loss: 0.8408087491989136, grad norm: 0.4454382061958313\n",
      "Epoch 480, loss: 0.8358541131019592, grad norm: 0.4273270070552826\n",
      "Epoch 490, loss: 0.8311089277267456, grad norm: 0.4101373851299286\n",
      "Epoch 500, loss: 0.8265675902366638, grad norm: 0.39361679553985596\n",
      "Epoch 510, loss: 0.8222170472145081, grad norm: 0.37777388095855713\n",
      "Epoch 520, loss: 0.8180502653121948, grad norm: 0.36263978481292725\n",
      "Epoch 530, loss: 0.8140580654144287, grad norm: 0.34802576899528503\n",
      "Epoch 540, loss: 0.8102319836616516, grad norm: 0.3339311480522156\n",
      "Epoch 550, loss: 0.8065646290779114, grad norm: 0.32044172286987305\n",
      "Epoch 560, loss: 0.8030523061752319, grad norm: 0.3075215816497803\n",
      "Epoch 570, loss: 0.7996813058853149, grad norm: 0.295167475938797\n",
      "Epoch 580, loss: 0.7964481711387634, grad norm: 0.28337109088897705\n",
      "Epoch 590, loss: 0.7933484315872192, grad norm: 0.27209657430648804\n",
      "Epoch 600, loss: 0.7903739809989929, grad norm: 0.26115384697914124\n",
      "Epoch 610, loss: 0.7875180244445801, grad norm: 0.2508035898208618\n",
      "Epoch 620, loss: 0.784774899482727, grad norm: 0.2406834363937378\n",
      "Epoch 630, loss: 0.7821418046951294, grad norm: 0.23097926378250122\n",
      "Epoch 640, loss: 0.7796123623847961, grad norm: 0.22192613780498505\n",
      "Epoch 650, loss: 0.7771811485290527, grad norm: 0.2131393998861313\n",
      "Epoch 660, loss: 0.7748452425003052, grad norm: 0.2046523243188858\n",
      "Epoch 670, loss: 0.7725977897644043, grad norm: 0.19642628729343414\n",
      "Epoch 680, loss: 0.7704375982284546, grad norm: 0.18862928450107574\n",
      "Epoch 690, loss: 0.7683587074279785, grad norm: 0.18132078647613525\n",
      "Epoch 700, loss: 0.7663570046424866, grad norm: 0.17392723262310028\n",
      "Epoch 710, loss: 0.7644308805465698, grad norm: 0.16694268584251404\n",
      "Epoch 720, loss: 0.7625769376754761, grad norm: 0.16044217348098755\n",
      "Epoch 730, loss: 0.7607908844947815, grad norm: 0.1540474146604538\n",
      "Epoch 740, loss: 0.7590692639350891, grad norm: 0.1479433923959732\n",
      "Epoch 750, loss: 0.757409930229187, grad norm: 0.1421787440776825\n",
      "Epoch 760, loss: 0.7558115720748901, grad norm: 0.1365516185760498\n",
      "Epoch 770, loss: 0.7542679309844971, grad norm: 0.13095083832740784\n",
      "Epoch 780, loss: 0.752779483795166, grad norm: 0.1257796734571457\n",
      "Epoch 790, loss: 0.7513441443443298, grad norm: 0.12079036235809326\n",
      "Epoch 800, loss: 0.7499581575393677, grad norm: 0.11599321663379669\n",
      "Epoch 810, loss: 0.7486199736595154, grad norm: 0.11135745048522949\n",
      "Epoch 820, loss: 0.7473258972167969, grad norm: 0.10696383565664291\n",
      "Epoch 830, loss: 0.7460768818855286, grad norm: 0.10244110226631165\n",
      "Epoch 840, loss: 0.744870126247406, grad norm: 0.0985686406493187\n",
      "Epoch 850, loss: 0.7437016367912292, grad norm: 0.09438873082399368\n",
      "Epoch 860, loss: 0.7425731420516968, grad norm: 0.09064500033855438\n",
      "Epoch 870, loss: 0.741480827331543, grad norm: 0.08698645234107971\n",
      "Epoch 880, loss: 0.7404241561889648, grad norm: 0.08350440859794617\n",
      "Epoch 890, loss: 0.7394005656242371, grad norm: 0.07993191480636597\n",
      "Epoch 900, loss: 0.7384098768234253, grad norm: 0.07687836140394211\n",
      "Epoch 910, loss: 0.7374515533447266, grad norm: 0.07357229292392731\n",
      "Epoch 920, loss: 0.7365211248397827, grad norm: 0.07057749480009079\n",
      "Epoch 930, loss: 0.7356196641921997, grad norm: 0.06791045516729355\n",
      "Epoch 940, loss: 0.7347468137741089, grad norm: 0.06494902819395065\n",
      "Epoch 950, loss: 0.733900249004364, grad norm: 0.06224638968706131\n",
      "Epoch 960, loss: 0.7330789566040039, grad norm: 0.05976936221122742\n",
      "Epoch 970, loss: 0.7322815656661987, grad norm: 0.0571468248963356\n",
      "Epoch 980, loss: 0.7315073013305664, grad norm: 0.054794903844594955\n",
      "Epoch 990, loss: 0.7307568192481995, grad norm: 0.05249572545289993\n",
      "Epoch 1000, loss: 0.7300264239311218, grad norm: 0.050249140709638596\n",
      "Epoch 1010, loss: 0.729319155216217, grad norm: 0.048119813203811646\n",
      "Epoch 1020, loss: 0.7286313772201538, grad norm: 0.046122681349515915\n",
      "Epoch 1030, loss: 0.7279621958732605, grad norm: 0.04409553483128548\n",
      "Epoch 1040, loss: 0.7273120880126953, grad norm: 0.04224997013807297\n",
      "Epoch 1050, loss: 0.7266798615455627, grad norm: 0.040556035935878754\n",
      "Epoch 1060, loss: 0.7260646820068359, grad norm: 0.03874032199382782\n",
      "Epoch 1070, loss: 0.7254672050476074, grad norm: 0.0370701402425766\n",
      "Epoch 1080, loss: 0.7248860001564026, grad norm: 0.035392191261053085\n",
      "Epoch 1090, loss: 0.72431880235672, grad norm: 0.033800482749938965\n",
      "Epoch 1100, loss: 0.7237678170204163, grad norm: 0.03235848993062973\n",
      "Epoch 1110, loss: 0.7232308387756348, grad norm: 0.030827557668089867\n",
      "Epoch 1120, loss: 0.7227087616920471, grad norm: 0.029520241543650627\n",
      "Epoch 1130, loss: 0.7221983671188354, grad norm: 0.0280297938734293\n",
      "Epoch 1140, loss: 0.7217034697532654, grad norm: 0.027100898325443268\n",
      "Epoch 1150, loss: 0.7212205529212952, grad norm: 0.02575397863984108\n",
      "Epoch 1160, loss: 0.7207493185997009, grad norm: 0.024418506771326065\n",
      "Epoch 1170, loss: 0.7202911972999573, grad norm: 0.02336696907877922\n",
      "Epoch 1180, loss: 0.7198439836502075, grad norm: 0.022437207400798798\n",
      "Epoch 1190, loss: 0.7194076180458069, grad norm: 0.02121030166745186\n",
      "Epoch 1200, loss: 0.7189819812774658, grad norm: 0.02021169476211071\n",
      "Epoch 1210, loss: 0.7185678482055664, grad norm: 0.01928519830107689\n",
      "Epoch 1220, loss: 0.7181627154350281, grad norm: 0.01837848126888275\n",
      "Epoch 1230, loss: 0.7177685499191284, grad norm: 0.01744351163506508\n",
      "Epoch 1240, loss: 0.7173830270767212, grad norm: 0.01669454015791416\n",
      "Epoch 1250, loss: 0.7170077562332153, grad norm: 0.015720076858997345\n",
      "Epoch 1260, loss: 0.7166403532028198, grad norm: 0.01509861834347248\n",
      "Epoch 1270, loss: 0.7162826657295227, grad norm: 0.01437200978398323\n",
      "Epoch 1280, loss: 0.7159335613250732, grad norm: 0.013688194565474987\n",
      "Epoch 1290, loss: 0.7155923247337341, grad norm: 0.0129669439047575\n",
      "Epoch 1300, loss: 0.7152597904205322, grad norm: 0.012101833708584309\n",
      "Epoch 1310, loss: 0.7149341702461243, grad norm: 0.011516228318214417\n",
      "Epoch 1320, loss: 0.7146171927452087, grad norm: 0.011011735536158085\n",
      "Epoch 1330, loss: 0.7143075466156006, grad norm: 0.010509920306503773\n",
      "Epoch 1340, loss: 0.7140040993690491, grad norm: 0.00981957744807005\n",
      "Epoch 1350, loss: 0.7137091159820557, grad norm: 0.009322619065642357\n",
      "Epoch 1360, loss: 0.7134205102920532, grad norm: 0.008879625238478184\n",
      "Epoch 1370, loss: 0.7131380438804626, grad norm: 0.008350975811481476\n",
      "Epoch 1380, loss: 0.712863564491272, grad norm: 0.007895797491073608\n",
      "Epoch 1390, loss: 0.7125940322875977, grad norm: 0.0074907890520989895\n",
      "Epoch 1400, loss: 0.7123310565948486, grad norm: 0.00700388615950942\n",
      "Epoch 1410, loss: 0.7120742201805115, grad norm: 0.006628337781876326\n",
      "Epoch 1420, loss: 0.7118251323699951, grad norm: 0.006314514204859734\n",
      "Epoch 1430, loss: 0.7115787863731384, grad norm: 0.0059002600610256195\n",
      "Epoch 1440, loss: 0.7113401293754578, grad norm: 0.005561270751059055\n",
      "Epoch 1450, loss: 0.7111063003540039, grad norm: 0.005213303491473198\n",
      "Epoch 1460, loss: 0.710878312587738, grad norm: 0.005013104062527418\n",
      "Epoch 1470, loss: 0.710655689239502, grad norm: 0.004746942315250635\n",
      "Epoch 1480, loss: 0.7104381322860718, grad norm: 0.004429808352142572\n",
      "Epoch 1490, loss: 0.7102250456809998, grad norm: 0.004212484695017338\n",
      "Epoch 1500, loss: 0.7100176215171814, grad norm: 0.003895711386576295\n",
      "Epoch 1510, loss: 0.709814727306366, grad norm: 0.003659880021587014\n",
      "Epoch 1520, loss: 0.7096160054206848, grad norm: 0.0036389564629644156\n",
      "Epoch 1530, loss: 0.7094231247901917, grad norm: 0.0031063444912433624\n",
      "Epoch 1540, loss: 0.7092334032058716, grad norm: 0.002947702072560787\n",
      "Epoch 1550, loss: 0.7090482711791992, grad norm: 0.002819543005898595\n",
      "Epoch 1560, loss: 0.7088682055473328, grad norm: 0.0025392165407538414\n",
      "Epoch 1570, loss: 0.7086924910545349, grad norm: 0.002362086670473218\n",
      "Epoch 1580, loss: 0.7085201144218445, grad norm: 0.002225305885076523\n",
      "Epoch 1590, loss: 0.7083518505096436, grad norm: 0.0020682404283434153\n",
      "Epoch 1600, loss: 0.7081881761550903, grad norm: 0.001932673156261444\n",
      "Epoch 1610, loss: 0.7080286145210266, grad norm: 0.0017722193151712418\n",
      "Epoch 1620, loss: 0.7078728675842285, grad norm: 0.0016308353515341878\n",
      "Epoch 1630, loss: 0.7077193260192871, grad norm: 0.0015682416269555688\n",
      "Epoch 1640, loss: 0.7075697779655457, grad norm: 0.00141686643473804\n",
      "Epoch 1650, loss: 0.7074257135391235, grad norm: 0.0013190268073230982\n",
      "Epoch 1660, loss: 0.7072826623916626, grad norm: 0.0012266041012480855\n",
      "Epoch 1670, loss: 0.7071452140808105, grad norm: 0.0012142769992351532\n",
      "Epoch 1680, loss: 0.7070099115371704, grad norm: 0.0011007856810465455\n",
      "Epoch 1690, loss: 0.7068784236907959, grad norm: 0.0009416826069355011\n",
      "Epoch 1700, loss: 0.7067487835884094, grad norm: 0.0009444509632885456\n",
      "Epoch 1710, loss: 0.7066234946250916, grad norm: 0.0007993082399480045\n",
      "Epoch 1720, loss: 0.7065008878707886, grad norm: 0.0007332297973334789\n",
      "Epoch 1730, loss: 0.7063819169998169, grad norm: 0.0007231969502754509\n",
      "Epoch 1740, loss: 0.7062658667564392, grad norm: 0.0006307167932391167\n",
      "Epoch 1750, loss: 0.7061508297920227, grad norm: 0.0006437917472794652\n",
      "Epoch 1760, loss: 0.70604008436203, grad norm: 0.0005713349091820419\n",
      "Epoch 1770, loss: 0.7059320211410522, grad norm: 0.0005213826079852879\n",
      "Epoch 1780, loss: 0.7058270573616028, grad norm: 0.0005451507167890668\n",
      "Epoch 1790, loss: 0.705723762512207, grad norm: 0.0004892799188382924\n",
      "Epoch 1800, loss: 0.7056238651275635, grad norm: 0.0004602818517014384\n",
      "Epoch 1810, loss: 0.7055264711380005, grad norm: 0.0006919279112480581\n",
      "Epoch 1820, loss: 0.7054312825202942, grad norm: 0.0004519345529843122\n",
      "Epoch 1830, loss: 0.7053386569023132, grad norm: 0.00044321647146716714\n",
      "Epoch 1840, loss: 0.7052483558654785, grad norm: 0.0007450596312992275\n",
      "Epoch 1850, loss: 0.705159604549408, grad norm: 0.0004332535027060658\n",
      "Epoch 1860, loss: 0.7050748467445374, grad norm: 0.0005371299339458346\n",
      "Epoch 1870, loss: 0.7049912214279175, grad norm: 0.0004962451057508588\n",
      "Epoch 1880, loss: 0.704909086227417, grad norm: 0.0004905746900476515\n",
      "Epoch 1890, loss: 0.7048295140266418, grad norm: 0.000537771440576762\n",
      "Epoch 1900, loss: 0.7047533392906189, grad norm: 0.00047252653166651726\n",
      "Epoch 1910, loss: 0.7046779990196228, grad norm: 0.0004659363185055554\n",
      "Epoch 1920, loss: 0.7046046853065491, grad norm: 0.0004723645397461951\n",
      "Epoch 1930, loss: 0.7045336961746216, grad norm: 0.00046462033060379326\n",
      "Epoch 1940, loss: 0.7044646739959717, grad norm: 0.0004671087663155049\n",
      "Epoch 1950, loss: 0.7043969631195068, grad norm: 0.00047857846948318183\n",
      "Epoch 1960, loss: 0.7043311595916748, grad norm: 0.00048541714204475284\n",
      "Epoch 1970, loss: 0.7042675018310547, grad norm: 0.0005372597370296717\n",
      "Epoch 1980, loss: 0.7042058706283569, grad norm: 0.0005206969799473882\n",
      "Epoch 1990, loss: 0.7041447162628174, grad norm: 0.0004834755090996623\n",
      "Epoch 2000, loss: 0.7040870189666748, grad norm: 0.0004999380325898528\n",
      "Epoch 2010, loss: 0.7040289640426636, grad norm: 0.0006680072983726859\n",
      "Epoch 2020, loss: 0.7039729952812195, grad norm: 0.0004907441325485706\n",
      "Epoch 2030, loss: 0.7039197087287903, grad norm: 0.0005630309460684657\n",
      "Epoch 2040, loss: 0.7038669586181641, grad norm: 0.0006173063302412629\n",
      "Epoch 2050, loss: 0.7038172483444214, grad norm: 0.0004993906477466226\n",
      "Epoch 2060, loss: 0.7037675380706787, grad norm: 0.0004918355843983591\n",
      "Epoch 2070, loss: 0.7037190794944763, grad norm: 0.0005253623821772635\n",
      "Epoch 2080, loss: 0.7036729454994202, grad norm: 0.0005246265209279954\n",
      "Epoch 2090, loss: 0.7036268711090088, grad norm: 0.00048757120384834707\n",
      "Epoch 2100, loss: 0.7035826444625854, grad norm: 0.00048370289732702076\n",
      "Epoch 2110, loss: 0.7035400867462158, grad norm: 0.0005245818756520748\n",
      "Epoch 2120, loss: 0.7034988403320312, grad norm: 0.0005234396085143089\n",
      "Epoch 2130, loss: 0.703458309173584, grad norm: 0.00047757435822859406\n",
      "Epoch 2140, loss: 0.7034199237823486, grad norm: 0.0005661668838001788\n",
      "Epoch 2150, loss: 0.7033824920654297, grad norm: 0.0005415338673628867\n",
      "Epoch 2160, loss: 0.7033454179763794, grad norm: 0.0005046791047789156\n",
      "Epoch 2170, loss: 0.7033090591430664, grad norm: 0.000551511300727725\n",
      "Epoch 2180, loss: 0.7032747864723206, grad norm: 0.0005035091890022159\n",
      "Epoch 2190, loss: 0.7032413482666016, grad norm: 0.0004782743926625699\n",
      "Epoch 2200, loss: 0.7032084465026855, grad norm: 0.0004611050826497376\n",
      "Epoch 2210, loss: 0.703177273273468, grad norm: 0.00045942020369693637\n",
      "Epoch 2220, loss: 0.7031466364860535, grad norm: 0.00045832604519091547\n",
      "Epoch 2230, loss: 0.7031166553497314, grad norm: 0.000510871410369873\n",
      "Epoch 2240, loss: 0.7030884623527527, grad norm: 0.0004468679544515908\n",
      "Epoch 2250, loss: 0.7030602097511292, grad norm: 0.0006011819932609797\n",
      "Epoch 2260, loss: 0.7030332088470459, grad norm: 0.0004328219802118838\n",
      "Epoch 2270, loss: 0.7030074596405029, grad norm: 0.0004265007155481726\n",
      "Epoch 2280, loss: 0.7029827237129211, grad norm: 0.0004657494428101927\n",
      "Epoch 2290, loss: 0.7029579877853394, grad norm: 0.0004069162532687187\n",
      "Epoch 2300, loss: 0.702934741973877, grad norm: 0.0004777678695973009\n",
      "Epoch 2310, loss: 0.7029117941856384, grad norm: 0.00039514488889835775\n",
      "Epoch 2320, loss: 0.7028900980949402, grad norm: 0.0003932564577553421\n",
      "Epoch 2330, loss: 0.7028682231903076, grad norm: 0.00038850627606734633\n",
      "Epoch 2340, loss: 0.7028489112854004, grad norm: 0.0003843355516437441\n",
      "Epoch 2350, loss: 0.7028273940086365, grad norm: 0.000537196290679276\n",
      "Epoch 2360, loss: 0.7028087973594666, grad norm: 0.0007211611955426633\n",
      "Epoch 2370, loss: 0.7027896046638489, grad norm: 0.0003782609710469842\n",
      "Epoch 2380, loss: 0.7027718424797058, grad norm: 0.0003776961239054799\n",
      "Epoch 2390, loss: 0.7027549147605896, grad norm: 0.0003824588202405721\n",
      "Epoch 2400, loss: 0.7027378082275391, grad norm: 0.0004515406908467412\n",
      "Epoch 2410, loss: 0.7027220129966736, grad norm: 0.0004826474760193378\n",
      "Epoch 2420, loss: 0.7027059197425842, grad norm: 0.0008090650662779808\n",
      "Epoch 2430, loss: 0.7026910781860352, grad norm: 0.0004081877996213734\n",
      "Epoch 2440, loss: 0.7026760578155518, grad norm: 0.0005114668747410178\n",
      "Epoch 2450, loss: 0.7026630640029907, grad norm: 0.000378765951609239\n",
      "Epoch 2460, loss: 0.702648401260376, grad norm: 0.00039968080818653107\n",
      "Epoch 2470, loss: 0.7026352286338806, grad norm: 0.0003505227214191109\n",
      "Epoch 2480, loss: 0.7026233077049255, grad norm: 0.00032057121279649436\n",
      "Epoch 2490, loss: 0.7026106119155884, grad norm: 0.00039834287599660456\n"
     ]
    }
   ],
   "source": [
    "epochs = 2500\n",
    "\n",
    "for epoch in trange(epochs):\n",
    "    loss, xor_gate, and_gate, opt_state = make_step(\n",
    "        xor_gate,  # type: ignore\n",
    "        and_gate,  # type: ignore\n",
    "        X,\n",
    "        y_sum,\n",
    "        y_carry,\n",
    "        optim,\n",
    "        opt_state,  # type: ignore\n",
    "    )\n",
    "    _, grads = compute_sum_loss(xor_gate, X, y_sum)\n",
    "    grad_norm_value = grad_norm(grads)\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, loss: {loss}, grad norm: {grad_norm_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Trained XOR Gate Parameters:\n",
      "DELTA: 0.6329566240310669, X0: 0.7230839729309082, X_THRESHOLD: -0.761564314365387\n",
      "\n",
      "Trained AND Gate Parameters:\n",
      "DELTA: 0.5060827136039734, X0: 1.0108097791671753, X_THRESHOLD: 3.158766269683838\n",
      "\n",
      "Half-Adder Evaluation:\n",
      "Input: [False False], Predicted Sum: 1, Predicted Carry: 0\n",
      "Input: [False  True], Predicted Sum: 1, Predicted Carry: 0\n",
      "Input: [ True False], Predicted Sum: 1, Predicted Carry: 0\n",
      "Input: [ True  True], Predicted Sum: 0, Predicted Carry: 1\n"
     ]
    }
   ],
   "source": [
    "# Display trained parameters\n",
    "print(\"\\nTrained XOR Gate Parameters:\")\n",
    "print(\n",
    "    f\"DELTA: {xor_gate.DELTA}, X0: {xor_gate.X0}, X_THRESHOLD: {xor_gate.X_THRESHOLD}\"\n",
    ")\n",
    "\n",
    "print(\"\\nTrained AND Gate Parameters:\")\n",
    "print(\n",
    "    f\"DELTA: {and_gate.DELTA}, X0: {and_gate.X0}, X_THRESHOLD: {and_gate.X_THRESHOLD}\"\n",
    ")\n",
    "\n",
    "# Evaluate the trained Half-Adder\n",
    "print(\"\\nHalf-Adder Evaluation:\")\n",
    "for i in range(len(X)):\n",
    "    sum_output = xor_gate(X[i])\n",
    "    carry_output = and_gate(X[i])\n",
    "    print(\n",
    "        f\"Input: {X[i]}, Predicted Sum: {sum_output:.0f}, Predicted Carry: {carry_output:.0f}\"\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
